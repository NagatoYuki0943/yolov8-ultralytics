

# 本地安装

```sh
git clone https://github.com/ultralytics/ultralytics
cd ultralytics
pip install -v -e .
# "-v" 指详细说明，或更多的输出
# "-e" 表示在可编辑模式下安装项目，因此对代码所做的任何本地修改都会生效，从而无需重新安装。
```

# [CLI](https://docs.ultralytics.com/usage/cli/)

The YOLO Command Line Interface (CLI) is the easiest way to get started training, validating, predicting and exporting YOLOv8 models.

The `yolo` command is used for all actions:

```sh
yolo TASK MODE ARGS

Where   TASK (optional) is one of [detect, segment, classify]
        MODE (required) is one of [train, val, predict, export, track]
        ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.
```

Where:

- `TASK` (optional) is one of `[detect, segment, classify]`. If it is not passed explicitly YOLOv8 will try to guess the `TASK` from the model type.
- `MODE` (required) is one of `[train, val, predict, export, track]`
- `ARGS` (optional) are any number of custom `arg=value` pairs like `imgsz=320` that override defaults. For a full list of available `ARGS` see the [Configuration](https://docs.ultralytics.com/usage/cfg/) page and `defaults.yaml` GitHub [source](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/default.yaml).

Arguments must be passed as `arg=val` pairs, split by an equals `=` sign and delimited by spaces between pairs. Do not use `--` argument prefixes or commas `,` between arguments.

- `yolo predict model=yolov8n.pt imgsz=640 conf=0.25`  ✅
- `yolo predict model yolov8n.pt imgsz 640 conf 0.25`  ❌
- `yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25`  ❌

## Overriding default arguments

Default arguments can be overridden by simply passing them as arguments in the CLI in `arg=value` pairs.

```sh
yolo detect train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01
yolo segment predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320
yolo detect val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640
```

## Overriding default config file

You can override the `default.yaml` config file entirely by passing a new file with the `cfg` arguments, i.e. `cfg=custom.yaml`.

To do this first create a copy of `default.yaml` in your current working dir with the `yolo copy-cfg` command.

This will create `default_copy.yaml`, which you can then pass as `cfg=default_copy.yaml` along with any additional args, like `imgsz=320` in this example:



```sh
yolo copy-cfg
yolo cfg=default_copy.yaml imgsz=320
```

## 默认配置文件

> `ultralytics/cfg/default.yaml`

```yaml
# Ultralytics YOLO 🚀, AGPL-3.0 license
# Default training settings and hyperparameters for medium-augmentation COCO training

task: detect  # (str) YOLO task, i.e. detect, segment, classify, pose
mode: train  # (str) YOLO mode, i.e. train, val, predict, export, track, benchmark

# Train settings -------------------------------------------------------------------------------------------------------
model:  # (str, optional) path to model file, i.e. yolov8n.pt, yolov8n.yaml
data:  # (str, optional) path to data file, i.e. coco128.yaml
epochs: 100  # (int) number of epochs to train for
time:  # (float, optional) number of hours to train for, overrides epochs if supplied
patience: 50  # (int) epochs to wait for no observable improvement for early stopping of training
batch: 16  # (int) number of images per batch (-1 for AutoBatch)
imgsz: 640  # (int | list) input images size as int for train and val modes, or list[w,h] for predict and export modes
save: True  # (bool) save train checkpoints and predict results
save_period: -1 # (int) Save checkpoint every x epochs (disabled if < 1)
cache: False  # (bool) True/ram, disk or False. Use cache for data loading
device:  # (int | str | list, optional) device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu
workers: 8  # (int) number of worker threads for data loading (per RANK if DDP)
project:  # (str, optional) project name
name:  # (str, optional) experiment name, results saved to 'project/name' directory
exist_ok: False  # (bool) whether to overwrite existing experiment
pretrained: True  # (bool | str) whether to use a pretrained model (bool) or a model to load weights from (str)
optimizer: auto  # (str) optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]
verbose: True  # (bool) whether to print verbose output
seed: 0  # (int) random seed for reproducibility
deterministic: True  # (bool) whether to enable deterministic mode
single_cls: False  # (bool) train multi-class data as single-class
rect: False  # (bool) rectangular training if mode='train' or rectangular validation if mode='val'
cos_lr: False  # (bool) use cosine learning rate scheduler
close_mosaic: 10  # (int) disable mosaic augmentation for final epochs (0 to disable)
resume: False  # (bool) resume training from last checkpoint
amp: True  # (bool) Automatic Mixed Precision (AMP) training, choices=[True, False], True runs AMP check
fraction: 1.0  # (float) dataset fraction to train on (default is 1.0, all images in train set)
profile: False  # (bool) profile ONNX and TensorRT speeds during training for loggers
freeze: None  # (int | list, optional) freeze first n layers, or freeze list of layer indices during training
multi_scale: False   # (bool) Whether to use multi-scale during training
# Segmentation
overlap_mask: True  # (bool) masks should overlap during training (segment train only)
mask_ratio: 4  # (int) mask downsample ratio (segment train only)
# Classification
dropout: 0.0  # (float) use dropout regularization (classify train only)

# Val/Test settings ----------------------------------------------------------------------------------------------------
val: True  # (bool) validate/test during training
split: val  # (str) dataset split to use for validation, i.e. 'val', 'test' or 'train'
save_json: False  # (bool) save results to JSON file
save_hybrid: False  # (bool) save hybrid version of labels (labels + additional predictions)
conf:  # (float, optional) object confidence threshold for detection (default 0.25 predict, 0.001 val)
iou: 0.7  # (float) intersection over union (IoU) threshold for NMS
max_det: 300  # (int) maximum number of detections per image
half: False  # (bool) use half precision (FP16)
dnn: False  # (bool) use OpenCV DNN for ONNX inference
plots: True  # (bool) save plots and images during train/val

# Predict settings -----------------------------------------------------------------------------------------------------
source:  # (str, optional) source directory for images or videos
vid_stride: 1  # (int) video frame-rate stride
stream_buffer: False  # (bool) buffer all streaming frames (True) or return the most recent frame (False)
visualize: False  # (bool) visualize model features
augment: False  # (bool) apply image augmentation to prediction sources
agnostic_nms: False  # (bool) class-agnostic NMS
classes:  # (int | list[int], optional) filter results by class, i.e. classes=0, or classes=[0,2,3]
retina_masks: False  # (bool) use high-resolution segmentation masks
embed:  # (list[int], optional) return feature vectors/embeddings from given layers

# Visualize settings ---------------------------------------------------------------------------------------------------
show: False  # (bool) show predicted images and videos if environment allows
save_frames: False  # (bool) save predicted individual video frames
save_txt: False  # (bool) save results as .txt file
save_conf: False  # (bool) save results with confidence scores
save_crop: False  # (bool) save cropped images with results
show_labels: True  # (bool) show prediction labels, i.e. 'person'
show_conf: True  # (bool) show prediction confidence, i.e. '0.99'
show_boxes: True  # (bool) show prediction boxes
line_width:   # (int, optional) line width of the bounding boxes. Scaled to image size if None.

# Export settings ------------------------------------------------------------------------------------------------------
format: torchscript  # (str) format to export to, choices at https://docs.ultralytics.com/modes/export/#export-formats
keras: False  # (bool) use Kera=s
optimize: False  # (bool) TorchScript: optimize for mobile
int8: False  # (bool) CoreML/TF INT8 quantization
dynamic: False  # (bool) ONNX/TF/TensorRT: dynamic axes
simplify: False  # (bool) ONNX: simplify model
opset:  # (int, optional) ONNX: opset version
workspace: 4  # (int) TensorRT: workspace size (GB)
nms: False  # (bool) CoreML: add NMS

# Hyperparameters ------------------------------------------------------------------------------------------------------
lr0: 0.01  # (float) initial learning rate (i.e. SGD=1E-2, Adam=1E-3)
lrf: 0.01  # (float) final learning rate (lr0 * lrf)
momentum: 0.937  # (float) SGD momentum/Adam beta1
weight_decay: 0.0005  # (float) optimizer weight decay 5e-4
warmup_epochs: 3.0  # (float) warmup epochs (fractions ok)
warmup_momentum: 0.8  # (float) warmup initial momentum
warmup_bias_lr: 0.1  # (float) warmup initial bias lr
box: 7.5  # (float) box loss gain
cls: 0.5  # (float) cls loss gain (scale with pixels)
dfl: 1.5  # (float) dfl loss gain
pose: 12.0  # (float) pose loss gain
kobj: 1.0  # (float) keypoint obj loss gain
label_smoothing: 0.0  # (float) label smoothing (fraction)
nbs: 64  # (int) nominal batch size
hsv_h: 0.015  # (float) image HSV-Hue augmentation (fraction)
hsv_s: 0.7  # (float) image HSV-Saturation augmentation (fraction)
hsv_v: 0.4  # (float) image HSV-Value augmentation (fraction)
degrees: 0.0  # (float) image rotation (+/- deg)
translate: 0.1  # (float) image translation (+/- fraction)
scale: 0.5  # (float) image scale (+/- gain)
shear: 0.0  # (float) image shear (+/- deg)
perspective: 0.0  # (float) image perspective (+/- fraction), range 0-0.001
flipud: 0.0  # (float) image flip up-down (probability)
fliplr: 0.5  # (float) image flip left-right (probability)
mosaic: 1.0  # (float) image mosaic (probability)
mixup: 0.0  # (float) image mixup (probability)
copy_paste: 0.0  # (float) segment copy-paste (probability)
auto_augment: randaugment  # (str) auto augmentation policy for classification (randaugment, autoaugment, augmix)
erasing: 0.4  # (float) probability of random erasing during classification training (0-1)
crop_fraction: 1.0  # (float) image crop fraction for classification evaluation/inference (0-1)

# Custom config.yaml ---------------------------------------------------------------------------------------------------
cfg:  # (str, optional) for overriding defaults.yaml

# Tracker settings ------------------------------------------------------------------------------------------------------
tracker: botsort.yaml  # (str) tracker type, choices=[botsort.yaml, bytetrack.yaml]
```

## cli 文件

> `ultralytics/cfg/__init__.py`

# [Configuration](https://docs.ultralytics.com/usage/cfg/)

YOLO settings and hyperparameters play a critical role in the model's performance, speed, and accuracy. These settings and hyperparameters can affect the model's behavior at various stages of the model development process, including training, validation, and prediction.

YOLOv8 'yolo' CLI commands use the following syntax:

```sh
yolo TASK MODE ARGS
yolo task=detect    mode=train    model=yolov8n.pt        args...
          classify       predict        yolov8n-cls.yaml  args...
          segment        val            yolov8n-seg.yaml  args...
                         export         yolov8n.pt        format=onnx  args...

# example    后面必须使用 =
yolo predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'
yolo val detect data=coco.yaml device=0
yolo val detect data=coco128.yaml batch=1 device=0|cpu
```

Where:

- `TASK` (optional) is one of `[detect, segment, classify, pose]`. If it is not passed explicitly YOLOv8 will try to guess the `TASK` from the model type.
- `MODE` (required) is one of `[train, val, predict, export, track, benchmark]`
- `ARGS` (optional) are any number of custom `arg=value` pairs like `imgsz=320` that override defaults. For a full list of available `ARGS` see the [Configuration](https://docs.ultralytics.com/usage/cfg/) page and `defaults.yaml` GitHub [source](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/default.yaml).

YOLO models can be used for a variety of tasks, including detection, segmentation, classification and pose. These tasks differ in the type of output they produce and the specific problem they are designed to solve.

- **Detect**: For identifying and localizing objects or regions of interest in an image or video.
- **Segment**: For dividing an image or video into regions or pixels that correspond to different objects or classes.
- **Classify**: For predicting the class label of an input image.
- **Pose**: For identifying objects and estimating their keypoints in an image or video.

| Key    | Value      | Description                                     |
| :----- | :--------- | :---------------------------------------------- |
| `task` | `'detect'` | YOLO task, i.e. detect, segment, classify, pose |

## Modes

YOLO models can be used in different modes depending on the specific problem you are trying to solve. These modes include:

- **Train**: For training a YOLOv8 model on a custom dataset.

- **Val**: For validating a YOLOv8 model after it has been trained.

- **Predict**: For making predictions using a trained YOLOv8 model on new images or videos.

- **Export**: For exporting a YOLOv8 model to a format that can be used for deployment.

- **Track**: For tracking objects in real-time using a YOLOv8 model.

- **Benchmark**: For benchmarking YOLOv8 exports (ONNX, TensorRT, etc.) speed and accuracy.

| Key    | Value     | Description                                                  |
| :----- | :-------- | :----------------------------------------------------------- |
| `mode` | `'train'` | YOLO mode, i.e. train, val, predict, export, track, benchmark |

# 数据集

> 先要把数据集放入dataset中，修改data/目录下的yaml，调整为自己的数据集，需要调整路径，分类数，标签名

> yolo数据集格式(yolov5/v8的coco128和霹雳吧啦Wz的yolo3为例)
>
> txt内容，每一行都是 `3 0.933536 0.486124 0.030408 0.154487`
>
> 是 label 中心横坐标与图像宽度比值 中心纵坐标与图像高度比值 bbox宽度与图像宽度比值 bbox高度与图像宽高比值

```sh
#-------------------------------------------#
# 	yolov5 v8的格式
#-------------------------------------------#
yaml:
    path: ../datasets/coco128   # dataset root dir
    train: images/train         # train images (relative to 'path') 128 images
    val: images/val             # val images (relative to 'path') 128 images
    test: images/test           # test images (optional)

dir:
    datasets
    ├── coco128
        ├── images
        │   ├── train   # 训练图片
        │   ├── val     # 验证图片
        │   └── test    # 测试图片
        └── labels
            ├── train   # 训练标签txt
            ├── val     # 验证标签txt
            └── test    # 测试标签txt

#-------------------------------------------#
# 	yolov5 v8另的一种图片目录格式
#-------------------------------------------#
yaml:
    path: ../datasets/coco128   # dataset root dir
    train: train/images         # train images (relative to 'path')
    val: val/images             # val images (relative to 'path')
    test: test/images           # test images (optional)
dir:
    datasets
    ├── coco128
        ├── train
        │   ├── images  # 训练图片
        │   └── labels  # 训练标签txt
        ├── val
        │   ├── images  # 验证图片
        │   └── labels  # 验证标签txt
        └── test
            ├── images  # 测试图片
            └── labels  # 测试标签txt
```

> `ultralytics/datasets/class20.yaml`

```yaml
# YOLOv5 🚀 by Ultralytics, GPL-3.0 license
# COCO128 dataset https://www.kaggle.com/ultralytics/coco128 (first 128 images from COCO train2017) by Ultralytics
# Example usage: python train.py --data coco128.yaml
# parent
# ├── ultralytics
# |   └── ultralytics
# └── datasets
#     └── yourname
#         └── images/
#             └── train2017/  存放训练图片
#             └── val2017/    存放验证图片
#         └── labels/
#             └── train2017/  存放训练标签  class x_center y_center width height
#             └── val2017/    存放验证标签


# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ../datasets/classes20  # dataset root dir
train: images/train2017  # train images (relative to 'path') 128 images
val: images/val2017  # val images (relative to 'path') 128 images
test:  # test images (optional)

# Classes
names:
  0: aeroplane
  1: bicycle
  2: bird
  3: boat
  4: bottle
  5: bus
  6: car
  7: cat
  8: chair
  9: cow
  10: diningtable
  11: dog
  12: horse
  13: motorbike
  14: person
  15: pottedplant
  16: sheep
  17: sofa
  18: train
  19: tvmonitor
```

# 下载权重

> 将下载好的权重放到`weights/`文件下下

## 模型

所有的 YOLOv8 预训练模型都可以在此找到。检测、分割和姿态模型在 [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/coco.yaml) 数据集上进行预训练，而分类模型在 [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/ImageNet.yaml) 数据集上进行预训练。

在首次使用时，[模型](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models) 会自动从最新的 Ultralytics [发布版本](https://github.com/ultralytics/assets/releases)中下载。

| 模型                                                         | 尺寸 （像素） | mAPval 50-95 | 推理速度 CPU ONNX (ms) | 推理速度 A100 TensorRT (ms) | 参数量 (M) | FLOPs (B) |
| ------------------------------------------------------------ | ------------- | ------------ | ---------------------- | --------------------------- | ---------- | --------- |
| [YOLOv8n](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt) | 640           | 37.3         | 80.4                   | 0.99                        | 3.2        | 8.7       |
| [yolov8n](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt) | 640           | 44.9         | 128.4                  | 1.20                        | 11.2       | 28.6      |
| [YOLOv8m](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt) | 640           | 50.2         | 234.7                  | 1.83                        | 25.9       | 78.9      |
| [YOLOv8l](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt) | 640           | 52.9         | 375.2                  | 2.39                        | 43.7       | 165.2     |
| [YOLOv8x](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt) | 640           | 53.9         | 479.1                  | 3.53                        | 68.2       | 257.8     |

- **mAPval** 结果都在 [COCO val2017](http://cocodataset.org/) 数据集上，使用单模型单尺度测试得到。
  复现命令 `yolo val detect data=coco.yaml device=0`
- **推理速度**使用 COCO 验证集图片推理时间进行平均得到，测试环境使用 [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) 实例。
  复现命令 `yolo val detect data=coco128.yaml batch=1 device=0|cpu`

# [训练](https://docs.ultralytics.com/zh/modes/train/)

## 训练模式的关键特性

以下是YOLOv8训练模式的一些显著特点：

- **自动数据集下载:** 标准数据集如COCO、VOC和ImageNet将在首次使用时自动下载。
- **多GPU支持:** 无缝地跨多个GPU扩展您的训练工作，以加快过程。
- **超参数配置:** 通过YAML配置文件或CLI参数修改超参数的选项。
- **可视化和监控:** 实时跟踪训练指标并可视化学习过程，以获得更好的洞察力。

## 使用示例

### **单GPU和CPU训练示例**

设备将自动确定。如果有可用的GPU，那么将使用它，否则将在CPU上开始训练。

> python

```python
from ultralytics import YOLO

# 加载一个模型
model = YOLO('yolov8n.yaml')  # 从YAML建立一个新模型
model = YOLO('yolov8n.pt')  # 加载预训练模型（推荐用于训练）
model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # 从YAML建立并转移权重

# 训练模型
results = model.train(data='coco128.yaml', epochs=100, imgsz=640)
```

```python
from ultralytics import YOLO

# yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10
# fraction=1.0 cos_lr=True device=0 model=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml

# 加载一个模型
model = YOLO('weights/yolov8n.pt')  # 加载预训练模型（推荐用于训练）

# 训练模型
results = model.train(
    imgsz=640,
    batch=-1,
    workers=8,
    epochs=300,
    patience=0,
    close_mosaic=10,
    fraction=1.0,
    cos_lr=True,
    device=0,
    data='ultralytics/cfg/datasets/coco128.yaml',
)

print(results)
```

> cli

```sh
# 从YAML构建新模型，从头开始训练
yolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640

# 从预训练*.pt模型开始训练
yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640

# 从YAML构建一个新模型，转移预训练权重，然后开始训练
yolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640
```

### 多GPU训练

多GPU训练通过在多个GPU上分布训练负载，实现对可用硬件资源的更有效利用。无论是通过Python API还是命令行界面，都可以使用此功能。 若要启用多GPU训练，请指定您希望使用的GPU设备ID。

> python

```python
from ultralytics import YOLO

# 加载模型
model = YOLO('yolov8n.pt')  # 加载预训练模型（推荐用于训练）

# 使用2个GPU训练模型
results = model.train(data='coco128.yaml', epochs=100, imgsz=640, device=[0, 1])
```

> cli

```sh
# 使用GPU 0和1从预训练*.pt模型开始训练
yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1
```

### 恢复中断的训练

在处理深度学习模型时，从之前保存的状态恢复训练是一个关键特性。在各种情况下，这可能很方便，比如当训练过程意外中断，或者当您希望用新数据或更多时期继续训练模型时。

恢复训练时，Ultralytics YOLO将加载最后保存的模型的权重，并恢复优化器状态、学习率调度器和时期编号。这允许您无缝地从离开的地方继续训练过程。

在Ultralytics YOLO中，您可以通过在调用`train`方法时将`resume`参数设置为`True`并指定包含部分训练模型权重的`.pt`文件路径来轻松恢复训练。

下面是使用Python和命令行恢复中断训练的示例：

> python

```python
from ultralytics import YOLO

# 加载模型
model = YOLO('path/to/last.pt')  # 加载部分训练的模型

# 恢复训练
results = model.train(resume=True)
```

> cli

```sh
# 恢复中断的训练
yolo train resume model=path/to/last.pt
```

通过设置`resume=True`，`train`函数将从'path/to/last.pt'文件中存储的状态继续训练。如果省略`resume`参数或将其设置为`False`，`train`函数将启动新的训练会话。

请记住，默认情况下，检查点会在每个时期结束时保存，或者使用`save_period`参数以固定间隔保存，因此您必须至少完成1个时期才能恢复训练运行。

## 参数

YOLO模型的训练设置是指用于对数据集进行模型训练的各种超参数和配置。这些设置会影响模型的性能、速度和准确性。一些常见的YOLO训练设置包括批大小、学习率、动量和权重衰减。其他可能影响训练过程的因素包括优化器的选择、损失函数的选择以及训练数据集的大小和组成。仔细调整和实验这些设置以实现给定任务的最佳性能是非常重要的。

| 参数              | 默认值   | 说明                                                         |
| :---------------- | :------- | :----------------------------------------------------------- |
| `model`           | `None`   | 指定用于训练的模型文件。接受指向 `.pt` 预训练模型或 `.yaml` 配置文件。对于定义模型结构或初始化权重至关重要。 |
| `data`            | `None`   | 数据集配置文件的路径（例如 `coco128.yaml`).该文件包含特定于数据集的参数，包括训练数据和验证数据的路径、类名和类数。 |
| `epochs`          | `100`    | 训练epoch总数。每个epoch代表对整个数据集进行一次完整的训练。调整该值会影响训练时间和模型性能。 |
| `time`            | `None`   | 最长训练时间（小时）。如果设置了该值，则会覆盖 `epochs` 参数，允许训练在指定的持续时间后自动停止。对于时间有限的训练场景非常有用。 |
| `patience`        | `100`    | 在验证指标没有改善的情况下，提前停止训练所需的epoch数。当性能趋于平稳时停止训练，有助于防止过度拟合。 |
| `batch`           | `16`     | 训练的批量大小，表示在更新模型内部参数之前要处理多少张图像。自动批处理 (`batch=-1`)会根据 GPU 内存可用性动态调整批处理大小。 |
| `imgsz`           | `640`    | 用于训练的目标图像尺寸。所有图像在输入模型前都会被调整到这一尺寸。影响模型精度和计算复杂度。 |
| `save`            | `True`   | 可保存训练检查点和最终模型权重。这对恢复训练或模型部署非常有用。 |
| `save_period`     | `-1`     | 保存模型检查点的频率，以 epochs 为单位。值为-1 时将禁用此功能。该功能适用于在长时间训练过程中保存临时模型。 |
| `cache`           | `False`  | 在内存中缓存数据集图像 (`True`/`ram`）、磁盘 (`disk`），或禁用它 (`False`).通过减少磁盘 I/O 提高训练速度，但代价是增加内存使用量。 |
| `device`          | `None`   | 指定用于训练的计算设备：单个 GPU (`device=0`）、多个 GPU (`device=0,1`)、CPU (`device=cpu`)，或苹果芯片的 MPS (`device=mps`). |
| `workers`         | `8`      | 加载数据的工作线程数（每 `RANK` 多 GPU 训练）。影响数据预处理和输入模型的速度，尤其适用于多 GPU 设置。 |
| `project`         | `None`   | 保存训练结果的项目目录名称。允许有组织地存储不同的实验。     |
| `name`            | `None`   | 训练运行的名称。用于在项目文件夹内创建一个子目录，用于存储训练日志和输出结果。 |
| `exist_ok`        | `False`  | 如果为 True，则允许覆盖现有的项目/名称目录。这对迭代实验非常有用，无需手动清除之前的输出。 |
| `pretrained`      | `True`   | 决定是否从预处理模型开始训练。可以是布尔值，也可以是加载权重的特定模型的字符串路径。提高训练效率和模型性能。 |
| `optimizer`       | `'auto'` | 为训练选择优化器。选项包括 `SGD`, `Adam`, `AdamW`, `NAdam`, `RAdam`, `RMSProp` 等，或 `auto` 用于根据模型配置进行自动选择。影响收敛速度和稳定性。 |
| `verbose`         | `False`  | 在训练过程中启用冗长输出，提供详细日志和进度更新。有助于调试和密切监控训练过程。 |
| `seed`            | `0`      | 为训练设置随机种子，确保在相同配置下运行的结果具有可重复性。 |
| `deterministic`   | `True`   | 强制使用确定性算法，确保可重复性，但由于对非确定性算法的限制，可能会影响性能和速度。 |
| `single_cls`      | `False`  | 在训练过程中将多类数据集中的所有类别视为单一类别。适用于二元分类任务，或侧重于对象的存在而非分类。 |
| `rect`            | `False`  | 可进行矩形训练，优化批次组成以减少填充。这可以提高效率和速度，但可能会影响模型的准确性。 |
| `cos_lr`          | `False`  | 利用余弦学习率调度器，根据历时的余弦曲线调整学习率。这有助于管理学习率，实现更好的收敛。 |
| `close_mosaic`    | `10`     | 在训练完成前禁用最后 N 个epoch的马赛克数据增强以稳定训练。设置为 0 则禁用此功能。 |
| `resume`          | `False`  | 从上次保存的检查点恢复训练。自动加载模型权重、优化器状态和历时计数，无缝继续训练。 |
| `amp`             | `True`   | 启用自动混合精度 (AMP) 训练，可减少内存使用量并加快训练速度，同时将对精度的影响降至最低。 |
| `fraction`        | `1.0`    | 指定用于训练的数据集的部分。允许在完整数据集的子集上进行训练，这对实验或资源有限的情况非常有用。 |
| `profile`         | `False`  | 在训练过程中，可对ONNX 和TensorRT 速度进行剖析，有助于优化模型部署。 |
| `freeze`          | `None`   | 冻结模型的前 N 层或按索引指定的层，从而减少可训练参数的数量。这对微调或迁移学习非常有用。 |
| `lr0`             | `0.01`   | 初始学习率（即 `SGD=1E-2`, `Adam=1E-3`) .调整这个值对优化过程至关重要，会影响模型权重的更新速度。 |
| `lrf`             | `0.01`   | 最终学习率占初始学习率的百分比 = (`lr0 * lrf`)，与调度程序结合使用，随着时间的推移调整学习率。 |
| `momentum`        | `0.937`  | 用于 SGD 的动量因子，或用于 Adam 优化器的 beta1，用于将过去的梯度纳入当前更新。 |
| `weight_decay`    | `0.0005` | L2 正则化项，对大权重进行惩罚，以防止过度拟合。              |
| `warmup_epochs`   | `3.0`    | 学习率预热的epoch数，学习率从低值逐渐增加到初始学习率，以在早期稳定训练。 |
| `warmup_momentum` | `0.8`    | 热身阶段的初始动量，在热身期间逐渐调整到设定动量。           |
| `warmup_bias_lr`  | `0.1`    | 热身阶段的偏置参数学习率，有助于稳定初始epoch的模型训练。    |
| `box`             | `7.5`    | 损失函数中边框损失部分的权重，影响对准确预测边框坐标的重视程度。 |
| `cls`             | `0.5`    | 分类损失在总损失函数中的权重，影响正确分类预测相对于其他部分的重要性。 |
| `dfl`             | `1.5`    | 分布焦点损失权重，在某些YOLO 版本中用于精细分类。            |
| `pose`            | `12.0`   | 姿态损失在姿态估计模型中的权重，影响准确预测姿态关键点的重点。 |
| `kobj`            | `2.0`    | 姿态估计模型中关键点对象性损失的权重，在检测可信度和姿态精度之间取得平衡。 |
| `label_smoothing` | `0.0`    | 应用标签平滑，将硬标签软化为目标标签和标签均匀分布的混合标签，可以提高泛化效果。 |
| `nbs`             | `64`     | 用于损耗正常化的标称批量大小。                               |
| `overlap_mask`    | `True`   | 决定在训练过程中分割掩码是否应该重叠，适用于实例分割任务。   |
| `mask_ratio`      | `4`      | 分割掩码的下采样率，影响训练时使用的掩码分辨率。             |
| `dropout`         | `0.0`    | 分类任务中正则化的放弃率，通过在训练过程中随机省略单元来防止过拟合。 |
| `val`             | `True`   | 可在训练过程中进行验证，以便在单独的数据集上对模型性能进行定期评估。 |
| `plots`           | `False`  | 生成并保存训练和验证指标图以及预测示例图，以便直观地了解模型性能和学习进度。 |

>`rect = True` 使用长方形训练
>
>Setting "rect"=True allows you to train using rectangular images, not necessarily square ones. This allows for more efficient use of GPU memory as there's less need for padding spatial dimensions.
>
>[Custom input size: letterbox vs resizing · Issue #11350 ](https://github.com/ultralytics/yolov5/issues/11350)
>
>[About the rectangle training · Issue #4819](https://github.com/ultralytics/ultralytics/issues/4819)

## example



```sh
# Build a new model from YAML and start training from scratch
yolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640

# Start training from a pretrained *.pt model
yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640

# Build a new model from YAML, transfer pretrained weights to it and start training
yolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640
```

> `Multi-GPU Training`

```sh
# Start training from a pretrained *.pt model using GPUs 0 and 1
yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1
```

> `auto optimizer`

```sh
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=ultralytics/cfg/models/v8/yolov8n.yaml pretrained=weights/yolov8n.pt data=ultralytics/datasets/coco128.yaml

#                                                                                                                                model可以直接设置为pt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml

#                                                        rtdetr 训练轮数更少
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=100 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=ultralytics/cfg/models/rt-detr/rtdetr-x.yaml pretrained=weights/rtdetr-x.pt data=ultralytics/cfg/datasets/coco128.yaml

#                                                        rtdetr 训练轮数更少                                                       model可以直接设置为pt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=100 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=weights/rtdetr-x.pt data=ultralytics/cfg/datasets/coco128.yaml
```

> `SGD`

```sh
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 optimizer=SGD lr0=0.01 cos_lr=True device=0 model=ultralytics/cfg/models/v8/yolov8n.yaml pretrained=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml

#                                                                                                                                                       model可以直接设置为pt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 optimizer=SGD lr0=0.01 cos_lr=True device=0 model=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml
```

> `Adam`

```sh
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 optimizer=AdamW lr0=0.001 cos_lr=True device=0 model=ultralytics/cfg/models/v8/yolov8n.yaml pretrained=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml

#                                                                                                                                                          model可以直接设置为pt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 optimizer=AdamW lr0=0.001 cos_lr=True device=0 model=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml
```

> `resume`

```sh
#                                                                                                                                model=最后的pt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=weights/last.pt data=ultralytics/cfg/datasets/coco128.yaml resume=True exist_ok=True
```

## **不需要在模型配置中显示更改类别数**

> 会自动将nc调整为数据集的类别数量

```sh
> yolo task=detect mode=train imgsz=640 batch=-1 epochs=300 optimizer=SGD lr0=0.01 cos_lr=True device=0 pretrained=weights/yolov8n.pt model=ultralytics/models/v8/yolov8n.yaml data=ultralytics/datasets/classes20.yaml

                   from  n    params  module                                       arguments
  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]
  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]
  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]
  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]
  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]
  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]
  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]
  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]
  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]
  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]
 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]
 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]
 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]
 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]
 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]
 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]
 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]
 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]
 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]
 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]
 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.Detect                [80, [64, 128, 256]]
yolov8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs

Transferred 355/355 items from pretrained weights
Ultralytics YOLOv8.0.58  Python-3.10.9 torch-2.0.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11264MiB)
yolo\engine\trainer: task=detect, mode=train, model=ultralytics/models/v8/yolov8n.yaml, data=ultralytics/datasets/classes20.yaml, epochs=300, patience=50, batch=-1, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=None, exist_ok=False, pretrained=weights/yolov8n.pt, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=d:\code\ultralytics\runs\detect\train2
Overriding model.yaml nc=80 with nc=20		# 这里自动覆盖了旧的类别数

                   from  n    params  module                                       arguments
  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]
  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]
  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]
  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]
  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]
  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]
  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]
  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]
  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]
  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]
 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]
 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]
 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]
 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]
 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]
 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]
 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]
 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]
 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]
 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]
 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.Detect                [20, [64, 128, 256]]
yolov8n summary: 225 layers, 3014748 parameters, 3014732 gradients, 8.2 GFLOPs

Transferred 319/355 items from pretrained weights
TensorBoard: Start with 'tensorboard --logdir d:\code\ultralytics\runs\detect\train', view at http://localhost:6006/
AMP: running Automatic Mixed Precision (AMP) checks with yolov8n...
AMP: checks passed
AutoBatch: Computing optimal batch size for imgsz=640
AutoBatch: CUDA:0 (NVIDIA GeForce GTX 1080 Ti) 11.00G total, 0.10G reserved, 0.07G allocated, 10.83G free
      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output
     3014748       8.215         0.210         28.59         17.95        (1, 3, 640, 640)                    list
     3014748       16.43         0.296         13.96         21.27        (2, 3, 640, 640)                    list
     3014748       32.86         0.581         12.96         20.99        (4, 3, 640, 640)                    list
     3014748       65.72         1.065         20.27          28.6        (8, 3, 640, 640)                    list
     3014748       131.4         2.334         34.56         48.56       (16, 3, 640, 640)                    list
AutoBatch: Using batch-size 50 for CUDA:0 7.30G/11.00G (66%)
optimizer: SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.000390625), 63 bias
train: Scanning D:\code\datasets\classes20\labels\train.cache... 5266 images, 0 backgrounds, 0 corrupt: 100%|██████████
val: Scanning D:\code\datasets\classes20\labels\val.cache... 586 images, 0 backgrounds, 0 corrupt: 100%|██████████| 586
Plotting labels to d:\code\ultralytics\runs\detect\train\labels.jpg...
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to d:\code\ultralytics\runs\detect\train
Starting training for 300 epochs...
```

> 自动调整 `nc` 的代码在 `ultralytics/nn/task.py`

```python
        ch = self.yaml['ch'] = self.yaml.get('ch', ch)  # input channels
        if nc and nc != self.yaml['nc']:    # 使用data config中的names长度覆盖模型配置文件中的类别
            LOGGER.info(f"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}")
            self.yaml['nc'] = nc  # override yaml value
```

## 训练时出现的问题

### 训练 `obj_loss` 增大 | reduce FPs | 解决特殊场景模型拍摄日常目标的FP数量过多

> [how to use Background images in training? · Issue #2844 · ultralytics/yolov5 (github.com)](https://github.com/ultralytics/yolov5/issues/2844)
>
> 在图片训练文件夹 `images/train` 中添加背景图片文件，比如coco或者voc数据集的一些照片
>
> 不需要添加空白label txt文件，添加了也不会出错
>
> `(if no objects in image, no `*.txt` file is required).`
>
> [目标检测（降低误检测率及小目标检测系列笔记）](https://blog.csdn.net/weixin_44836143/article/details/105952819)

```sh
train: Scanning D:\code\datasets\classes20\labels\train... 5266 images, 1000 backgrounds, 0 corrupt: 100%|██████████|
train: New cache created: D:\code\datasets\classes20\labels\train.cache
val: Scanning D:\code\datasets\classes20\labels\val... 586 images, 0 backgrounds, 0 corrupt: 100%|██████████|
val: New cache created: D:\code\datasets\classes20\labels\val.cache
```



# [验证](https://docs.ultralytics.com/zh/modes/val/)

## Val 模式的主要特点

以下是 YOLOv8 的 Val 模式提供的显著功能：

- **自动化设置：** 模型记住其训练配置，以便直接进行验证。
- **多指标支持：** 根据一系列准确度指标评估您的模型。
- **CLI 和 Python API：** 根据您的验证偏好选择命令行界面或 Python API。
- **数据兼容性：** 与训练阶段使用的数据集以及自定义数据集无缝协作。

## 使用示例

在 COCO128 数据集上验证训练过的 YOLOv8n 模型的准确性。由于 `model` 保留了其训练的 `data` 和参数作为模型属性，因此无需传递任何参数。有关完整的导出参数列表，请参阅下面的参数部分。

> python

```python
from ultralytics import YOLO

# 加载模型
model = YOLO('yolov8n.pt')  # 加载官方模型
model = YOLO('path/to/best.pt')  # 加载自定义模型

# 验证模型
metrics = model.val()  # 无需参数，数据集和设置记忆
metrics.box.map    # map50-95
metrics.box.map50  # map50
metrics.box.map75  # map75
metrics.box.maps   # 包含每个类别的map50-95列表
```

> cli

```sh
yolo detect val model=yolov8n.pt  # 验证官方模型
yolo detect val model=path/to/best.pt  # 验证自定义模型
```

## 参数

YOLO 模型的验证设置是指用于评估模型在验证数据集上性能的各种超参数和配置。这些设置会影响模型的性能、速度和准确性。一些常见的 YOLO 验证设置包括批处理大小、在训练期间验证频率以及用于评估模型性能的指标。其他可能影响验证过程的因素包括验证数据集的大小和组成以及模型用于特定任务的特性。仔细调整和实验这些设置很重要，以确保模型在验证数据集上表现良好并且检测和预防过拟合。

| 参数          | 类型    | 默认值  | 说明                                                         |
| :------------ | :------ | :------ | :----------------------------------------------------------- |
| `data`        | `str`   | `None`  | 指定数据集配置文件的路径（如 `coco128.yaml`).该文件包括验证数据的路径、类名和类数。 |
| `imgsz`       | `int`   | `640`   | 定义输入图像的尺寸。所有图像在处理前都会调整到这一尺寸。     |
| `batch`       | `int`   | `16`    | 设置每批图像的数量。使用 `-1` 的自动批处理功能，可根据 GPU 内存可用性自动调整。 |
| `save_json`   | `bool`  | `False` | 如果 `True`此外，还可将结果保存到 JSON 文件中，以便进一步分析或与其他工具集成。 |
| `save_hybrid` | `bool`  | `False` | 如果 `True`，保存混合版本的标签，将原始注释与额外的模型预测相结合。 |
| `conf`        | `float` | `0.001` | 设置检测的最小置信度阈值。置信度低于此阈值的检测将被丢弃。   |
| `iou`         | `float` | `0.6`   | 设置非最大抑制 (NMS) 的交叉重叠 (IoU) 阈值。有助于减少重复检测。 |
| `max_det`     | `int`   | `300`   | 限制每幅图像的最大检测次数。在密度较高的场景中非常有用，可以防止检测次数过多。 |
| `half`        | `bool`  | `True`  | 可进行半精度（FP16）计算，减少内存使用量，在提高速度的同时，将对精度的影响降至最低。 |
| `device`      | `str`   | `None`  | 指定验证设备 (`cpu`, `cuda:0`等）。可灵活利用 CPU 或 GPU 资源。 |
| `dnn`         | `bool`  | `False` | 如果 `True`它使用 OpenCV DNN 模块进行ONNX 模型推断，为PyTorch 推断方法提供了一种替代方法。 |
| `plots`       | `bool`  | `False` | 当设置为 `True`此外，它还能生成并保存预测结果与地面实况的对比图，以便对模型的性能进行可视化评估。 |
| `rect`        | `bool`  | `False` | 如果 `True`该软件使用矩形推理进行批处理，减少了填充，可能会提高速度和效率。 |
| `split`       | `str`   | `val`   | 确定用于验证的数据集分割 (`val`, `test`或 `train`).可灵活选择数据段进行性能评估。 |

> example

```sh
yolo detect val model=yolov8n.pt  # val official model
yolo detect val model=path/to/best.pt  # val custom model
```

### default confidence threshold = 0.001

> [mAP bug at higher --conf · Issue #1466 · ultralytics/yolov5](https://github.com/ultralytics/yolov5/issues/1466)
>
> [Why does the confidence threshold of 0.001 in val.py result in good results? · Issue #11745 · ultralytics/yolov5](https://github.com/ultralytics/yolov5/issues/11745)

### 验证模型在自定义数据集上的效果 精度0.995

> https://www.jianshu.com/p/cfb01add61bd#1684051613808
>
> https://github.com/ultralytics/yolov5/issues/5508
>
> https://github.com/ultralytics/yolov5/issues/1563
>
> https://github.com/ultralytics/yolov5/pull/1646
>
> `savehybrid` 会合并已知的labels，导致得分很高

## torch

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.pt device=0
```

## torchscript

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.torchscript device=0
```

## onnx

> 注意:
>
> `onnxruntime` 和 `onnxruntime-gpu` 不要同时安装，否则使用 `gpu` 推理时速度会很慢，如果同时安装了2个包，要全部卸载，再安装 `onnxruntime-gpu` 才能使用gpu推理，否则gpu速度会很慢

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.onnx device=0
```

## openvino

> 注意：openvino没法使用cuda，但是使用 --device 0 会提高推理速度

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n_openvnio_model device=cpu
```

## tensorrt

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.onnx device=0 half=True
```

# [预测](https://docs.ultralytics.com/zh/modes/predict/)

## 预测模式的关键特性

YOLOv8 的预测模式被设计为强大且多功能，包括以下特性：

- **兼容多个数据来源：** 无论您的数据是单独图片，图片集合，视频文件，还是实时视频流，预测模式都能胜任。
- **流式模式：** 使用流式功能生成一个内存高效的 `Results` 对象生成器。在调用预测器时，通过设置 `stream=True` 来启用此功能。
- **批处理：** 能够在单个批次中处理多个图片或视频帧，进一步加快推理时间。
- **易于集成：** 由于其灵活的 API，易于与现有数据管道和其他软件组件集成。

## 使用示例

Ultralytics YOLO 模型在进行推理时返回一个 Python `Results` 对象列表，或者当传入 `stream=True` 时，返回一个内存高效的 Python `Results` 对象生成器：

> 使用 `stream=False` 返回列表

```python
from ultralytics import YOLO

# 加载模型
model = YOLO('yolov8n.pt')  # 预训练的 YOLOv8n 模型

# 在图片列表上运行批量推理
results = model(['im1.jpg', 'im2.jpg'])  # 返回 Results 对象列表

# 处理结果列表
for result in results:
    boxes = result.boxes  # 边界框输出的 Boxes 对象
    masks = result.masks  # 分割掩码输出的 Masks 对象
    keypoints = result.keypoints  # 姿态输出的 Keypoints 对象
    probs = result.probs  # 分类输出的 Probs 对象
```

> 使用 `stream=True` 返回生成器

```python
from ultralytics import YOLO

# 加载模型
model = YOLO('yolov8n.pt')  # 预训练的 YOLOv8n 模型

# 在图片列表上运行批量推理
results = model(['im1.jpg', 'im2.jpg'], stream=True)  # 返回 Results 对象生成器

# 处理结果生成器
for result in results:
    boxes = result.boxes  # 边界框输出的 Boxes 对象
    masks = result.masks  # 分割掩码输出的 Masks 对象
    keypoints = result.keypoints  # 姿态输出的 Keypoints 对象
    probs = result.probs  # 分类输出的 Probs 对象
```

## 推理来源

YOLOv8 可以处理推理输入的不同类型，如下表所示。来源包括静态图像、视频流和各种数据格式。表格还表示了每种来源是否可以在流式模式下使用，使用参数 `stream=True` ✅。流式模式对于处理视频或实时流非常有利，因为它创建了结果的生成器，而不是将所有帧加载到内存。

> 使用 `stream=True` 处理长视频或大型数据集来高效地管理内存。当 `stream=False` 时，所有帧或数据点的结果都将存储在内存中，这可能很快导致内存不足错误。相对地，`stream=True` 使用生成器，只保留当前帧或数据点的结果在内存中，显著减少了内存消耗，防止内存不足问题。

| 来源      | 默认值                                     | 类型            | 说明                                                         |
| :-------- | :----------------------------------------- | :-------------- | :----------------------------------------------------------- |
| 图像      | `'image.jpg'`                              | `str` 或 `Path` | 单个图像文件。                                               |
| 网址      | `'https://ultralytics.com/images/bus.jpg'` | `str`           | 图片的 URL。                                                 |
| 截图      | `'screen'`                                 | `str`           | 截图                                                         |
| PIL       | `Image.open('im.jpg')`                     | `PIL.Image`     | 具有 RGB 通道的 HWC 格式。                                   |
| OpenCV    | `cv2.imread('im.jpg')`                     | `np.ndarray`    | 带有 BGR 频道的 HWC 格式 `uint8 (0-255)`.                    |
| numpy     | `np.zeros((640,1280,3))`                   | `np.ndarray`    | 带有 BGR 频道的 HWC 格式 `uint8 (0-255)`.                    |
| torch     | `torch.zeros(16,3,320,640)`                | `torch.Tensor`  | 带 RGB 通道的 BCHW 格式 `float32 (0.0-1.0)`.                 |
| CSV       | `'sources.csv'`                            | `str` 或 `Path` | 包含图像、视频或目录路径的 CSV 文件。                        |
| 视频 ✅    | `'video.mp4'`                              | `str` 或 `Path` | MP4 和 AVI 等格式的视频文件                                  |
| 目录 ✅    | `'path/'`                                  | `str` 或 `Path` | 包含图像或视频的目录路径。                                   |
| 球体 ✅    | `'path/*.jpg'`                             | `str`           | 全局模式来匹配多个文件。使用 `*` 字符作为通配符。            |
| YouTube ✅ | `'https://youtu.be/LNwODJXcvt4'`           | `str`           | YouTube 视频的 URL。                                         |
| 流 ✅      | `'rtsp://example.com/media.mp4'`           | `str`           | 流媒体协议（如 RTSP、RTMP、TCP）的 URL 或 IP 地址。          |
| 多流 ✅    | `'list.streams'`                           | `str` 或 `Path` | `*.streams` 文本文件，每行一个流 URL，即 8 个流将以 8 的批处理大小运行。 |

## 参数

`model.predict()` 在推理时接受多个参数，可以用来覆盖默认值：

```python
from ultralytics import YOLO

# 加载预训练的YOLOv8n模型
model = YOLO('yolov8n.pt')

# 在'bus.jpg'上运行推理，并附加参数
model.predict('bus.jpg', save=True, imgsz=320, conf=0.5)
```

| 参数            | 类型           | 默认值                 | 说明                                                         |
| :-------------- | :------------- | :--------------------- | :----------------------------------------------------------- |
| `source`        | `str`          | `'ultralytics/assets'` | 指定推理的数据源。可以是图像路径、视频文件、目录、URL 或用于实时馈送的设备 ID。支持多种格式和来源，可灵活应用于不同类型的输入。 |
| `conf`          | `float`        | `0.25`                 | 设置检测的最小置信度阈值。如果检测到的对象置信度低于此阈值，则将不予考虑。调整该值有助于减少误报。 |
| `iou`           | `float`        | `0.7`                  | 非最大抑制 (NMS) 的交叉重叠 (IoU) 阈值。数值越高，消除重叠框的检测次数越少，这对减少重复检测非常有用。 |
| `imgsz`         | `int or tuple` | `640`                  | 定义用于推理的图像大小。可以是一个整数 `640` 或一个（高、宽）元组。适当调整大小可以提高检测精度和处理速度。 |
| `half`          | `bool`         | `False`                | 启用半精度（FP16）推理，可加快支持的 GPU 上的模型推理速度，同时将对精度的影响降至最低。 |
| `device`        | `str`          | `None`                 | 指定用于推理的设备（例如：......）、 `cpu`, `cuda:0` 或 `0`).允许用户选择 CPU、特定 GPU 或其他计算设备来执行模型。 |
| `max_det`       | `int`          | `300`                  | 每幅图像允许的最大检测次数。限制模型在单次推理中可检测到的物体总数，防止在密集场景中产生过多输出。 |
| `vid_stride`    | `int`          | `1`                    | 视频输入的帧间距。允许跳过视频中的帧，以加快处理速度，但会牺牲时间分辨率。值为 1 时处理每一帧，值越大跳帧越多。 |
| `stream_buffer` | `bool`         | `False`                | 确定在处理视频流时是否对所有帧进行缓冲 (`True`)，或者模型是否应该返回最近的帧 (`False`).适用于实时应用。 |
| `visualize`     | `bool`         | `False`                | 在推理过程中激活模型特征的可视化，从而深入了解模型 "看到 "了什么。这对调试和模型解释非常有用。 |
| `augment`       | `bool`         | `False`                | 可对预测进行测试时间增强（TTA），从而在牺牲推理速度的情况下提高检测的鲁棒性。 |
| `agnostic_nms`  | `bool`         | `False`                | 启用与类别无关的非最大抑制 (NMS)，可合并不同类别的重叠方框。这在多类检测场景中非常有用，因为在这种场景中，类的重叠很常见。 |
| `classes`       | `list[int]`    | `None`                 | 根据一组类别 ID 过滤预测结果。只有属于指定类别的检测结果才会返回。在多类检测任务中，该功能有助于集中检测相关对象。 |
| `retina_masks`  | `bool`         | `False`                | 如果模型中存在高分辨率的分割掩膜，则使用高分辨率的分割掩膜。这可以提高分割任务的掩膜质量，提供更精细的细节。 |
| `embed`         | `list[int]`    | `None`                 | 指定从中提取特征向量或嵌入的层。这对聚类或相似性搜索等下游任务非常有用。 |

可视化参数：

| 论据          | 类型          | 默认值  | 说明                                                         |
| :------------ | :------------ | :------ | :----------------------------------------------------------- |
| `show`        | `bool`        | `False` | 如果 `True`在一个窗口中显示注释的图像或视频。有助于在开发或测试过程中提供即时视觉反馈。 |
| `save`        | `bool`        | `False` | 可将注释的图像或视频保存到文件中。这有助于记录、进一步分析或共享结果。 |
| `save_frames` | `bool`        | `False` | 处理视频时，将单个帧保存为图像。这对提取特定帧或逐帧进行详细分析非常有用。 |
| `save_txt`    | `bool`        | `False` | 将检测结果保存在文本文件中，格式如下 `[class] [x_center] [y_center] [width] [height] [confidence]`.有助于与其他分析工具集成。 |
| `save_conf`   | `bool`        | `False` | 在保存的文本文件中包含置信度分数。增强了后期处理和分析的细节。 |
| `save_crop`   | `bool`        | `False` | 保存经过裁剪的检测图像。可用于数据集扩充、分析或为特定物体创建重点数据集。 |
| `show_labels` | `bool`        | `True`  | 在可视输出中显示每次检测的标签。让用户立即了解检测到的物体。 |
| `show_conf`   | `bool`        | `True`  | 在标签旁显示每次检测的置信度得分。让人了解模型对每次检测的确定性。 |
| `show_boxes`  | `bool`        | `True`  | 在检测到的物体周围绘制边界框。对于图像或视频帧中物体的视觉识别和定位至关重要。 |
| `line_width`  | `None or int` | `None`  | 指定边界框的线宽。如果 `None`根据图像大小自动调整线宽。提供可视化定制，使图像更加清晰。 |

## torch

```sh
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.pt source=ultralytics/assets/bus.jpg device=0

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.pt source=../datasets/coco128/images/train2017 device=0
```

## torchscript

```sh
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.torchscript source=ultralytics/assets/bus.jpg device=0

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.torchscript source=../datasets/coco128/images/train2017 device=0
```

## onnx

> 注意:
>
> `onnxruntime` 和 `onnxruntime-gpu` 不要同时安装，否则使用 `gpu` 推理时速度会很慢，如果同时安装了2个包，要全部卸载，再安装 `onnxruntime-gpu` 才能使用gpu推理，否则gpu速度会很慢

```sh
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.onnx source=ultralytics/assets/bus.jpg device=0
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.onnx source=../datasets/coco128/images/train2017 device=0

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp16.onnx half=True source=ultralytics/assets/bus.jpg device=0           # fp16模型需要 half=True
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp16.onnx half=True source=../datasets/coco128/images/train2017 device=0

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.cpu.dynamic.onnx source=ultralytics/assets/bus.jpg device=0              # 使用cpu导出的dynamic模型可以用gpu推理
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.cpu.dynamic.onnx source=../datasets/coco128/images/train2017 device=0
```

## openvino

> 注意：openvino没法使用cuda，但是使用 `device=0` 会提高推理速度

```sh
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n_openvino_model source=ultralytics/assets/bus.jpg device=cpu

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n_openvino_model source=../datasets/coco128/images/train2017 device=cpu
```

## tensorrt

```sh
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.engine half=True source=ultralytics/assets/bus.jpg device=0                          # fp32模型也能用 --half 推理
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.engine half=True source=../datasets/coco128/images/train2017 device=0

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp16.engine half=True source=ultralytics/assets/bus.jpg device=0
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp16.engine half=True source=../datasets/coco128/images/train2017 device=0

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp32.dynamic.engine half=True source=ultralytics/assets/bus.jpg device=0             # fp32模型也能用 --half 推理
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp32.dynamic.engine half=True source=../datasets/coco128/images/train2017 device=0
```



# [导出](https://docs.ultralytics.com/zh/modes/export/)

## 导出模式的关键特性

以下是一些突出的功能：

- **一键导出：** 用于导出到不同格式的简单命令。
- **批量导出：** 支持批推理能力的模型导出。
- **优化推理：** 导出的模型针对更快的推理时间进行优化。
- **教学视频：** 提供深入指导和教学，确保流畅的导出体验。

## 使用示例

将 YOLOv8n 模型导出为 ONNX 或 TensorRT 等不同格式。查看下面的参数部分，了解完整的导出参数列表。

> python

```python
from ultralytics import YOLO

# 加载模型
model = YOLO('yolov8n.pt')  # 加载官方模型
model = YOLO('path/to/best.pt')  # 加载自定义训练的模型

# 导出模型
model.export(format='onnx')
```

> cli

```sh
yolo export model=yolov8n.pt format=onnx  # 导出官方模型
yolo export model=path/to/best.pt format=onnx  # 导出自定义训练的模型
```

## 参数

YOLO 模型的导出设置是指用于在其他环境或平台中使用模型时保存或导出模型的各种配置和选项。这些设置会影响模型的性能、大小和与不同系统的兼容性。一些常见的 YOLO 导出设置包括导出的模型文件格式（例如 ONNX、TensorFlow SavedModel）、模型将在哪个设备上运行（例如 CPU、GPU）以及是否包含附加功能，如遮罩或每个框多个标签。其他可能影响导出过程的因素包括模型用途的具体细节以及目标环境或平台的要求或限制。重要的是要仔细考虑和配置这些设置，以确保导出的模型针对预期用例经过优化，并且可以在目标环境中有效使用。

| 参数        | 类型             | 默认值          | 说明                                                         |
| :---------- | :--------------- | :-------------- | :----------------------------------------------------------- |
| `format`    | `str`            | `'torchscript'` | 导出模型的目标格式，例如 `'onnx'`, `'torchscript'`, `'tensorflow'`或其他，定义与各种部署环境的兼容性。 |
| `imgsz`     | `int` 或 `tuple` | `640`           | 模型输入所需的图像尺寸。对于正方形图像，可以是一个整数，或者是一个元组 `(height, width)` 了解具体尺寸。 |
| `keras`     | `bool`           | `False`         | 启用导出为 Keras 格式的TensorFlow SavedModel ，提供与TensorFlow serving 和 API 的兼容性。 |
| `optimize`  | `bool`           | `False`         | 在导出到TorchScript 时，应用针对移动设备的优化，可能会减小模型大小并提高性能。 |
| `half`      | `bool`           | `False`         | 启用 FP16（半精度）量化，在支持的硬件上减小模型大小并可能加快推理速度。 |
| `int8`      | `bool`           | `False`         | 激活 INT8 量化，进一步压缩模型并加快推理速度，同时将精度损失降至最低，主要用于边缘设备。 |
| `dynamic`   | `bool`           | `False`         | 允许ONNX 和TensorRT 导出动态输入尺寸，提高了处理不同图像尺寸的灵活性。 |
| `simplify`  | `bool`           | `False`         | 简化了ONNX 导出的模型图，可能会提高性能和兼容性。            |
| `opset`     | `int`            | `None`          | 指定ONNX opset 版本，以便与不同的ONNX 解析器和运行时兼容。如果未设置，则使用最新的支持版本。 |
| `workspace` | `float`          | `4.0`           | 以 GB 为单位设置最大工作区大小，用于TensorRT 优化，平衡内存使用和性能。 |
| `nms`       | `bool`           | `False`         | 在CoreML 导出中添加非最大值抑制 (NMS)，这对精确高效的检测后处理至关重要。 |

## 导出格式

YOLOv8 可用的导出格式如下表所示。您可以使用 `format` 参数，即 `format='onnx'` 或 `format='engine'`.

| 格式                                                         | `format` 参数 | 模型                      | 元数据 | 参数                                                |
| :----------------------------------------------------------- | :------------ | :------------------------ | :----- | :-------------------------------------------------- |
| [PyTorch](https://pytorch.org/)                              | -             | `yolov8n.pt`              | ✅      | -                                                   |
| [TorchScript](https://pytorch.org/docs/stable/jit.html)      | `torchscript` | `yolov8n.torchscript`     | ✅      | `imgsz`, `optimize`                                 |
| [ONNX](https://onnx.ai/)                                     | `onnx`        | `yolov8n.onnx`            | ✅      | `imgsz`, `half`, `dynamic`, `simplify`, `opset`     |
| [OpenVINO](https://docs.ultralytics.com/zh/integrations/openvino/) | `openvino`    | `yolov8n_openvino_model/` | ✅      | `imgsz`, `half`, `int8`                             |
| [TensorRT](https://developer.nvidia.com/tensorrt)            | `engine`      | `yolov8n.engine`          | ✅      | `imgsz`, `half`, `dynamic`, `simplify`, `workspace` |
| [CoreML](https://github.com/apple/coremltools)               | `coreml`      | `yolov8n.mlpackage`       | ✅      | `imgsz`, `half`, `int8`, `nms`                      |
| [TF SavedModel](https://www.tensorflow.org/guide/saved_model) | `saved_model` | `yolov8n_saved_model/`    | ✅      | `imgsz`, `keras`, `int8`                            |
| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`          | `yolov8n.pb`              | ❌      | `imgsz`                                             |
| [TF 轻型](https://www.tensorflow.org/lite)                   | `tflite`      | `yolov8n.tflite`          | ✅      | `imgsz`, `half`, `int8`                             |
| [TF 边缘TPU](https://coral.ai/docs/edgetpu/models-intro/)    | `edgetpu`     | `yolov8n_edgetpu.tflite`  | ✅      | `imgsz`                                             |
| [TF.js](https://www.tensorflow.org/js)                       | `tfjs`        | `yolov8n_web_model/`      | ✅      | `imgsz`, `half`, `int8`                             |
| [PaddlePaddle](https://github.com/PaddlePaddle)              | `paddle`      | `yolov8n_paddle_model/`   | ✅      | `imgsz`                                             |
| [NCNN](https://github.com/Tencent/ncnn)                      | `ncnn`        | `yolov8n_ncnn_model/`     | ✅      | `imgsz`, `half`                                     |

> example

```sh
yolo export model=yolov8n.pt format=onnx  # export official model
yolo export model=path/to/best.pt format=onnx  # export custom trained model
```

## torchscript

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=torchscript device=0
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=torchscript device=cpu optimize=True # optimize not compatible with cuda devices, i.e. use device=cpu
```

## onnx

> 注意:
>
> `onnxruntime` 和 `onnxruntime-gpu` 不要同时安装，否则使用 `gpu` 推理时速度会很慢，如果同时安装了2个包，要全部卸载，再安装`onnxruntime-gpu` 才能使用gpu推理，否则gpu速度会很慢

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=0

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=0 half=True                # half=True only compatible with GPU export, i.e. use device=0

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=cpu dynamic=True

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=cpu half=True dynamic=True  # 导出失败 half=True not compatible with dynamic=True, i.e. use only one.
```

### opencv使用的onnx

> https://github.com/ultralytics/ultralytics/tree/main/examples/YOLOv8-OpenCV-ONNX-Python

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=0 opset=12             # opset必须为12

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=0 half=True opset=12   # opset必须为12

# opencv不支持dynamic
```

## openvino

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=openvino device=cpu

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=openvino device=cpu half=True

yolo task =detect mode=export imgsz=640 model=weights/yolov8n.pt format=openvino device=cpu int8=True data=ultralytics/cfg/datasets/coco128.yaml # INT8 export requires a data argument for calibration
```

### 通过openvino的`ovc`命令将onnx转换为openvino格式(支持**fp16**)

> https://docs.openvino.ai/archive/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html

```sh
ovc "onnx_path" --output_model "output_path" --compress_to_fp16

ovc "onnx_path" --output_model "output_path" --compress_to_fp16
```

## tensorrt

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=engine simplify=True device=0 # 可以用simplify的onnx

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=engine simplify=True device=0 half=True
```

## ncnn

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=ncnn simplify=True device=0 # 可以用simplify的onnx

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=ncnn simplify=True device=0 half=True
```

## onnx openvino tensorrt

> 目前不支持同时导出多种格式，每种格式都要单独导出

# [跟踪](https://docs.ultralytics.com/zh/modes/track/)

## 一瞥特点

Ultralytics YOLO扩展了其物体检测功能，以提供强大且多功能的物体追踪：

- **实时追踪：** 在高帧率视频中无缝追踪物体。
- **支持多个追踪器：** 从多种成熟的追踪算法中选择。
- **自定义追踪器配置：** 通过调整各种参数来定制追踪算法，以满足特定需求。

## 可用的追踪器

Ultralytics YOLO支持以下追踪算法。可以通过传递相关的YAML配置文件如`tracker=tracker_type.yaml`来启用：

- [BoT-SORT](https://github.com/NirAharon/BoT-SORT) - 使用 `botsort.yaml` 启用此追踪器。
- [ByteTrack](https://github.com/ifzhang/ByteTrack) - 使用 `bytetrack.yaml` 启用此追踪器。

默认追踪器是BoT-SORT。

## 追踪

要在视频流中运行追踪器，请使用已训练的检测、分割或姿态模型，例如YOLOv8n、YOLOv8n-seg和YOLOv8n-pose。



> python

```python
from ultralytics import YOLO

# 加载官方或自定义模型
model = YOLO('yolov8n.pt')  # 加载一个官方的检测模型
model = YOLO('yolov8n-seg.pt')  # 加载一个官方的分割模型
model = YOLO('yolov8n-pose.pt')  # 加载一个官方的姿态模型
model = YOLO('path/to/best.pt')  # 加载一个自定义训练的模型

# 使用模型进行追踪
results = model.track(source="https://youtu.be/LNwODJXcvt4", show=True)  # 使用默认追踪器进行追踪
results = model.track(source="https://youtu.be/LNwODJXcvt4", show=True, tracker="bytetrack.yaml")  # 使用ByteTrack追踪器进行追踪
```

> cli

```sh
# 使用命令行界面进行各种模型的追踪
yolo track model=yolov8n.pt source="https://youtu.be/LNwODJXcvt4"  # 官方检测模型
yolo track model=yolov8n-seg.pt source="https://youtu.be/LNwODJXcvt4"  # 官方分割模型
yolo track model=yolov8n-pose.pt source="https://youtu.be/LNwODJXcvt4"  # 官方姿态模型
yolo track model=path/to/best.pt source="https://youtu.be/LNwODJXcvt4"  # 自定义训练模型

# 使用ByteTrack追踪器进行追踪
yolo track model=path/to/best.pt tracker="bytetrack.yaml"
```

## 配置

### 追踪参数

追踪配置与预测模式共享一些属性，如`conf`、`iou`和`show`。有关进一步配置，请参见[预测](https://docs.ultralytics.com/zh/modes/predict)模型页面。

> python

```sh
from ultralytics import YOLO

# 配置追踪参数并运行追踪器
model = YOLO('yolov8n.pt')
results = model.track(source="https://youtu.be/LNwODJXcvt4", conf=0.3, iou=0.5, show=True)
```

> cli

```sh
# 使用命令行界面配置追踪参数并运行追踪器
yolo track model=yolov8n.pt source="https://youtu.be/LNwODJXcvt4" conf=0.3, iou=0.5 show
```

### 选择追踪器

Ultralytics还允许您使用修改后的追踪器配置文件。要执行此操作，只需从[ultralytics/cfg/trackers](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/cfg/trackers)中复制一个追踪器配置文件（例如，`custom_tracker.yaml`）并根据您的需求修改任何配置（除了`tracker_type`）。



> python

```python
from ultralytics import YOLO

# 加载模型并使用自定义配置文件运行追踪器
model = YOLO('yolov8n.pt')
results = model.track(source="https://youtu.be/LNwODJXcvt4", tracker='custom_tracker.yaml')
```

> cli

```sh
# 使用命令行界面加载模型并使用自定义配置文件运行追踪器
yolo track model=yolov8n.pt source="https://youtu.be/LNwODJXcvt4" tracker='custom_tracker.yaml'
```

# [Tune](https://docs.ultralytics.com/integrations/ray-tune/)

## Usage

```python
from ultralytics import YOLO

# Load a YOLOv8n model
model = YOLO('yolov8n.pt')

# Start tuning hyperparameters for YOLOv8n training on the COCO8 dataset
result_grid = model.tune(data='coco8.yaml', use_ray=True, iterations=10)
```

## `tune()` Method Parameters

The `tune()` method in YOLOv8 provides an easy-to-use interface for hyperparameter tuning with Ray Tune. It accepts several arguments that allow you to customize the tuning process. Below is a detailed explanation of each parameter:

| Parameter       | Type             | Description                                                  | Default Value |
| :-------------- | :--------------- | :----------------------------------------------------------- | :------------ |
| `data`          | `str`            | The dataset configuration file (in YAML format) to run the tuner on. This file should specify the training and validation data paths, as well as other dataset-specific settings. |               |
| `space`         | `dict, optional` | A dictionary defining the hyperparameter search space for Ray Tune. Each key corresponds to a hyperparameter name, and the value specifies the range of values to explore during tuning. If not provided, YOLOv8 uses a default search space with various hyperparameters. |               |
| `grace_period`  | `int, optional`  | The grace period in epochs for the [ASHA scheduler](https://docs.ray.io/en/latest/tune/api/schedulers.html) in Ray Tune. The scheduler will not terminate any trial before this number of epochs, allowing the model to have some minimum training before making a decision on early stopping. | 10            |
| `gpu_per_trial` | `int, optional`  | The number of GPUs to allocate per trial during tuning. This helps manage GPU usage, particularly in multi-GPU environments. If not provided, the tuner will use all available GPUs. | None          |
| `iterations`    | `int, optional`  | The maximum number of trials to run during tuning. This parameter helps control the total number of hyperparameter combinations tested, ensuring the tuning process does not run indefinitely. | 10            |
| `**train_args`  | `dict, optional` | Additional arguments to pass to the `train()` method during tuning. These arguments can include settings like the number of training epochs, batch size, and other training-specific configurations. | {}            |

By customizing these parameters, you can fine-tune the hyperparameter optimization process to suit your specific needs and available computational resources.



## Default Search Space Description

The following table lists the default search space parameters for hyperparameter tuning in YOLOv8 with Ray Tune. Each parameter has a specific value range defined by `tune.uniform()`.

| Parameter         | Value Range                | Description                              |
| :---------------- | :------------------------- | :--------------------------------------- |
| `lr0`             | `tune.uniform(1e-5, 1e-1)` | Initial learning rate                    |
| `lrf`             | `tune.uniform(0.01, 1.0)`  | Final learning rate factor               |
| `momentum`        | `tune.uniform(0.6, 0.98)`  | Momentum                                 |
| `weight_decay`    | `tune.uniform(0.0, 0.001)` | Weight decay                             |
| `warmup_epochs`   | `tune.uniform(0.0, 5.0)`   | Warmup epochs                            |
| `warmup_momentum` | `tune.uniform(0.0, 0.95)`  | Warmup momentum                          |
| `box`             | `tune.uniform(0.02, 0.2)`  | Box loss weight                          |
| `cls`             | `tune.uniform(0.2, 4.0)`   | Class loss weight                        |
| `hsv_h`           | `tune.uniform(0.0, 0.1)`   | Hue augmentation range                   |
| `hsv_s`           | `tune.uniform(0.0, 0.9)`   | Saturation augmentation range            |
| `hsv_v`           | `tune.uniform(0.0, 0.9)`   | Value (brightness) augmentation range    |
| `degrees`         | `tune.uniform(0.0, 45.0)`  | Rotation augmentation range (degrees)    |
| `translate`       | `tune.uniform(0.0, 0.9)`   | Translation augmentation range           |
| `scale`           | `tune.uniform(0.0, 0.9)`   | Scaling augmentation range               |
| `shear`           | `tune.uniform(0.0, 10.0)`  | Shear augmentation range (degrees)       |
| `perspective`     | `tune.uniform(0.0, 0.001)` | Perspective augmentation range           |
| `flipud`          | `tune.uniform(0.0, 1.0)`   | Vertical flip augmentation probability   |
| `fliplr`          | `tune.uniform(0.0, 1.0)`   | Horizontal flip augmentation probability |
| `mosaic`          | `tune.uniform(0.0, 1.0)`   | Mosaic augmentation probability          |
| `mixup`           | `tune.uniform(0.0, 1.0)`   | Mixup augmentation probability           |
| `copy_paste`      | `tune.uniform(0.0, 1.0)`   | Copy-paste augmentation probability      |

## Custom Search Space Example

In this example, we demonstrate how to use a custom search space for hyperparameter tuning with Ray Tune and YOLOv8. By providing a custom search space, you can focus the tuning process on specific hyperparameters of interest.

```python
from ultralytics import YOLO

# Define a YOLO model
model = YOLO("yolov8n.pt")

# Run Ray Tune on the model
result_grid = model.tune(data="coco128.yaml",
                         space={"lr0": tune.uniform(1e-5, 1e-1)},
                         epochs=50,
                         use_ray=True)
```

In the code snippet above, we create a YOLO model with the "yolov8n.pt" pretrained weights. Then, we call the `tune()` method, specifying the dataset configuration with "coco128.yaml". We provide a custom search space for the initial learning rate `lr0` using a dictionary with the key "lr0" and the value `tune.uniform(1e-5, 1e-1)`. Finally, we pass additional training arguments, such as the number of epochs directly to the tune method as `epochs=50`.

## Processing Ray Tune Results

After running a hyperparameter tuning experiment with Ray Tune, you might want to perform various analyses on the obtained results. This guide will take you through common workflows for processing and analyzing these results.

### Loading Tune Experiment Results from a Directory

After running the tuning experiment with `tuner.fit()`, you can load the results from a directory. This is useful, especially if you're performing the analysis after the initial training script has exited.

```python
experiment_path = f"{storage_path}/{exp_name}"
print(f"Loading results from {experiment_path}...")

restored_tuner = tune.Tuner.restore(experiment_path, trainable=train_mnist)
result_grid = restored_tuner.get_results()
```

### Basic Experiment-Level Analysis

Get an overview of how trials performed. You can quickly check if there were any errors during the trials.

```python
if result_grid.errors:
    print("One or more trials failed!")
else:
    print("No errors!")
```

### Basic Trial-Level Analysis

Access individual trial hyperparameter configurations and the last reported metrics.

```python
for i, result in enumerate(result_grid):
    print(f"Trial #{i}: Configuration: {result.config}, Last Reported Metrics: {result.metrics}")
```

### Plotting the Entire History of Reported Metrics for a Trial

You can plot the history of reported metrics for each trial to see how the metrics evolved over time.



```python
import matplotlib.pyplot as plt

for result in result_grid:
    plt.plot(result.metrics_dataframe["training_iteration"], result.metrics_dataframe["mean_accuracy"], label=f"Trial {i}")

plt.xlabel('Training Iterations')
plt.ylabel('Mean Accuracy')
plt.legend()
plt.show()
```

# yolo special commands

## yolo help

```sh
> yolo help

    Arguments received: ['yolo', 'help']. Ultralytics 'yolo' commands use the following syntax:

        yolo TASK MODE ARGS

        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose')
                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')
                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.
                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'

    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01
        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01

    2. Predict a YouTube video using a pretrained segmentation model at image size 320:
        yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320

    3. Val a pretrained detection model at batch-size 1 and image size 640:
        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640

    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)
        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128

    5. Run special commands:
        yolo help
        yolo checks
        yolo version
        yolo settings
        yolo copy-cfg
        yolo cfg

    Docs: https://docs.ultralytics.com
    Community: https://community.ultralytics.com
    GitHub: https://github.com/ultralytics/ultralytics
```

## yolo checks

```sh
> yolo checks
Ultralytics YOLOv8.0.195  Python-3.11.4 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11264MiB)
Setup complete  (16 CPUs, 31.9 GB RAM, 152.1/200.0 GB disk)

OS                  Windows-10-10.0.19044-SP0
Environment         Windows
Python              3.11.4
Install             git
RAM                 31.91 GB
CPU                 Intel Core(TM) i7-10700 2.90GHz
CUDA                12.1

matplotlib           3.8.0>=3.3.0
numpy                1.26.0>=1.22.2
opencv-python        4.8.1.78>=4.6.0
pillow               10.0.1>=7.1.2
pyyaml               6.0.1>=5.3.1
requests             2.31.0>=2.23.0
scipy                1.10.1>=1.4.1
torch                2.1.0+cu121>=1.8.0
torchvision          0.16.0+cu121>=0.9.0
tqdm                 4.66.1>=4.64.0
pandas               2.1.1>=1.1.4
seaborn              0.13.0>=0.11.0
psutil               5.9.5
py-cpuinfo           9.0.0
thop                 0.1.1-2209072238>=0.1.1
```

## yolo version

```sh
> yolo version
8.0.195
```

## yolo settings

```sh
> yolo settings
 Learn about settings at https://docs.ultralytics.com/quickstart/#ultralytics-settings
Printing 'C:\Users\Administrator\AppData\Roaming\Ultralytics\settings.yaml'

settings_version: 0.0.4
datasets_dir: D:\ml\code\datasets
weights_dir: d:\ml\code\yolov8-ultralytics\weights
runs_dir: d:\ml\code\yolov8-ultralytics\runs
uuid: 062fa24c9a04873db7e870e2df7f4297a2745f5a740d9e7bd868b5884cf0b91a
sync: true
api_key: ''
clearml: true
comet: true
dvc: true
hub: true
mlflow: true
neptune: true
raytune: true
tensorboard: true
wandb: true
```

## yolo copy-cfg

```sh
> yolo copy-cfg
D:\ml\code\yolov8-ultralytics\ultralytics\cfg\default.yaml copied to D:\ml\code\yolov8-ultralytics\default_copy.yaml
Example YOLO command with this new custom cfg:
    yolo cfg='D:\ml\code\yolov8-ultralytics\default_copy.yaml' imgsz=320 batch=8
```

## yolo cfg

```sh
> yolo cfg
Printing 'D:\ml\code\yolov8-ultralytics\ultralytics\cfg\default.yaml'

task: detect
mode: train
model: null
data: null
epochs: 100
patience: 50
batch: 16
imgsz: 640
save: true
save_period: -1
cache: false
device: null
workers: 8
project: null
name: null
exist_ok: false
pretrained: true
optimizer: auto
verbose: true
seed: 0
deterministic: true
single_cls: false
rect: false
cos_lr: false
close_mosaic: 10
resume: false
amp: true
fraction: 1.0
profile: false
freeze: None
overlap_mask: true
mask_ratio: 4
dropout: 0.0
val: true
split: val
save_json: false
save_hybrid: false
conf: null
iou: 0.7
max_det: 300
half: false
dnn: false
plots: true
source: null
show: false
save_txt: false
save_conf: false
save_crop: false
show_labels: true
show_conf: true
vid_stride: 1
stream_buffer: false
line_width: null
visualize: false
augment: false
agnostic_nms: false
classes: null
retina_masks: false
boxes: true
format: torchscript
keras: false
optimize: false
int8: false
dynamic: false
simplify: false
opset: null
workspace: 4
nms: false
lr0: 0.01
lrf: 0.01
momentum: 0.937
weight_decay: 0.0005
warmup_epochs: 3.0
warmup_momentum: 0.8
warmup_bias_lr: 0.1
box: 7.5
cls: 0.5
dfl: 1.5
pose: 12.0
kobj: 1.0
label_smoothing: 0.0
nbs: 64
hsv_h: 0.015
hsv_s: 0.7
hsv_v: 0.4
degrees: 0.0
translate: 0.1
scale: 0.5
shear: 0.0
perspective: 0.0
flipud: 0.0
fliplr: 0.5
mosaic: 1.0
mixup: 0.0
copy_paste: 0.0
cfg: null
tracker: botsort.yaml
```

