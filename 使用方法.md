

# 本地安装

```sh
git clone https://github.com/ultralytics/ultralytics
cd ultralytics
pip install -v -e .
# "-v" 指详细说明，或更多的输出
# "-e" 表示在可编辑模式下安装项目，因此对代码所做的任何本地修改都会生效，从而无需重新安装。
```

# [CLI](https://docs.ultralytics.com/usage/cli/)

The YOLO Command Line Interface (CLI) is the easiest way to get started training, validating, predicting and exporting YOLOv8 models.

The `yolo` command is used for all actions:

```sh
yolo TASK MODE ARGS

Where   TASK (optional) is one of [detect, segment, classify]
        MODE (required) is one of [train, val, predict, export, track]
        ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.
```

Where:

- `TASK` (optional) is one of `[detect, segment, classify]`. If it is not passed explicitly YOLOv8 will try to guess the `TASK` from the model type.
- `MODE` (required) is one of `[train, val, predict, export, track]`
- `ARGS` (optional) are any number of custom `arg=value` pairs like `imgsz=320` that override defaults. For a full list of available `ARGS` see the [Configuration](https://docs.ultralytics.com/usage/cfg/) page and `defaults.yaml` GitHub [source](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/default.yaml).

Arguments must be passed as `arg=val` pairs, split by an equals `=` sign and delimited by spaces between pairs. Do not use `--` argument prefixes or commas `,` between arguments.

- `yolo predict model=yolov8n.pt imgsz=640 conf=0.25`  ✅
- `yolo predict model yolov8n.pt imgsz 640 conf 0.25`  ❌
- `yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25`  ❌

## Overriding default arguments

Default arguments can be overridden by simply passing them as arguments in the CLI in `arg=value` pairs.

```sh
yolo detect train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01
yolo segment predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320
yolo detect val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640
```

## Overriding default config file

You can override the `default.yaml` config file entirely by passing a new file with the `cfg` arguments, i.e. `cfg=custom.yaml`.

To do this first create a copy of `default.yaml` in your current working dir with the `yolo copy-cfg` command.

This will create `default_copy.yaml`, which you can then pass as `cfg=default_copy.yaml` along with any additional args, like `imgsz=320` in this example:



```sh
yolo copy-cfg
yolo cfg=default_copy.yaml imgsz=320
```

## 默认配置文件

> `ultralytics/cfg/default.yaml`

```yaml
# Ultralytics YOLO 🚀, AGPL-3.0 license
# Default training settings and hyperparameters for medium-augmentation COCO training

task: detect  # (str) YOLO task, i.e. detect, segment, classify, pose
mode: train  # (str) YOLO mode, i.e. train, val, predict, export, track, benchmark

# Train settings -------------------------------------------------------------------------------------------------------
model:  # (str, optional) path to model file, i.e. yolov8n.pt, yolov8n.yaml
data:  # (str, optional) path to data file, i.e. coco128.yaml
epochs: 100  # (int) number of epochs to train for
time:  # (float, optional) number of hours to train for, overrides epochs if supplied
patience: 50  # (int) epochs to wait for no observable improvement for early stopping of training
batch: 16  # (int) number of images per batch (-1 for AutoBatch)
imgsz: 640  # (int | list) input images size as int for train and val modes, or list[w,h] for predict and export modes
save: True  # (bool) save train checkpoints and predict results
save_period: -1 # (int) Save checkpoint every x epochs (disabled if < 1)
cache: False  # (bool) True/ram, disk or False. Use cache for data loading
device:  # (int | str | list, optional) device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu
workers: 8  # (int) number of worker threads for data loading (per RANK if DDP)
project:  # (str, optional) project name
name:  # (str, optional) experiment name, results saved to 'project/name' directory
exist_ok: False  # (bool) whether to overwrite existing experiment
pretrained: True  # (bool | str) whether to use a pretrained model (bool) or a model to load weights from (str)
optimizer: auto  # (str) optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]
verbose: True  # (bool) whether to print verbose output
seed: 0  # (int) random seed for reproducibility
deterministic: True  # (bool) whether to enable deterministic mode
single_cls: False  # (bool) train multi-class data as single-class
rect: False  # (bool) rectangular training if mode='train' or rectangular validation if mode='val'
cos_lr: False  # (bool) use cosine learning rate scheduler
close_mosaic: 10  # (int) disable mosaic augmentation for final epochs (0 to disable)
resume: False  # (bool) resume training from last checkpoint
amp: True  # (bool) Automatic Mixed Precision (AMP) training, choices=[True, False], True runs AMP check
fraction: 1.0  # (float) dataset fraction to train on (default is 1.0, all images in train set)
profile: False  # (bool) profile ONNX and TensorRT speeds during training for loggers
freeze: None  # (int | list, optional) freeze first n layers, or freeze list of layer indices during training
multi_scale: False   # (bool) Whether to use multi-scale during training
# Segmentation
overlap_mask: True  # (bool) masks should overlap during training (segment train only)
mask_ratio: 4  # (int) mask downsample ratio (segment train only)
# Classification
dropout: 0.0  # (float) use dropout regularization (classify train only)

# Val/Test settings ----------------------------------------------------------------------------------------------------
val: True  # (bool) validate/test during training
split: val  # (str) dataset split to use for validation, i.e. 'val', 'test' or 'train'
save_json: False  # (bool) save results to JSON file
save_hybrid: False  # (bool) save hybrid version of labels (labels + additional predictions)
conf:  # (float, optional) object confidence threshold for detection (default 0.25 predict, 0.001 val)
iou: 0.7  # (float) intersection over union (IoU) threshold for NMS
max_det: 300  # (int) maximum number of detections per image
half: False  # (bool) use half precision (FP16)
dnn: False  # (bool) use OpenCV DNN for ONNX inference
plots: True  # (bool) save plots and images during train/val

# Predict settings -----------------------------------------------------------------------------------------------------
source:  # (str, optional) source directory for images or videos
vid_stride: 1  # (int) video frame-rate stride
stream_buffer: False  # (bool) buffer all streaming frames (True) or return the most recent frame (False)
visualize: False  # (bool) visualize model features
augment: False  # (bool) apply image augmentation to prediction sources
agnostic_nms: False  # (bool) class-agnostic NMS
classes:  # (int | list[int], optional) filter results by class, i.e. classes=0, or classes=[0,2,3]
retina_masks: False  # (bool) use high-resolution segmentation masks
embed:  # (list[int], optional) return feature vectors/embeddings from given layers

# Visualize settings ---------------------------------------------------------------------------------------------------
show: False  # (bool) show predicted images and videos if environment allows
save_frames: False  # (bool) save predicted individual video frames
save_txt: False  # (bool) save results as .txt file
save_conf: False  # (bool) save results with confidence scores
save_crop: False  # (bool) save cropped images with results
show_labels: True  # (bool) show prediction labels, i.e. 'person'
show_conf: True  # (bool) show prediction confidence, i.e. '0.99'
show_boxes: True  # (bool) show prediction boxes
line_width:   # (int, optional) line width of the bounding boxes. Scaled to image size if None.

# Export settings ------------------------------------------------------------------------------------------------------
format: torchscript  # (str) format to export to, choices at https://docs.ultralytics.com/modes/export/#export-formats
keras: False  # (bool) use Kera=s
optimize: False  # (bool) TorchScript: optimize for mobile
int8: False  # (bool) CoreML/TF INT8 quantization
dynamic: False  # (bool) ONNX/TF/TensorRT: dynamic axes
simplify: False  # (bool) ONNX: simplify model
opset:  # (int, optional) ONNX: opset version
workspace: 4  # (int) TensorRT: workspace size (GB)
nms: False  # (bool) CoreML: add NMS

# Hyperparameters ------------------------------------------------------------------------------------------------------
lr0: 0.01  # (float) initial learning rate (i.e. SGD=1E-2, Adam=1E-3)
lrf: 0.01  # (float) final learning rate (lr0 * lrf)
momentum: 0.937  # (float) SGD momentum/Adam beta1
weight_decay: 0.0005  # (float) optimizer weight decay 5e-4
warmup_epochs: 3.0  # (float) warmup epochs (fractions ok)
warmup_momentum: 0.8  # (float) warmup initial momentum
warmup_bias_lr: 0.1  # (float) warmup initial bias lr
box: 7.5  # (float) box loss gain
cls: 0.5  # (float) cls loss gain (scale with pixels)
dfl: 1.5  # (float) dfl loss gain
pose: 12.0  # (float) pose loss gain
kobj: 1.0  # (float) keypoint obj loss gain
label_smoothing: 0.0  # (float) label smoothing (fraction)
nbs: 64  # (int) nominal batch size
hsv_h: 0.015  # (float) image HSV-Hue augmentation (fraction)
hsv_s: 0.7  # (float) image HSV-Saturation augmentation (fraction)
hsv_v: 0.4  # (float) image HSV-Value augmentation (fraction)
degrees: 0.0  # (float) image rotation (+/- deg)
translate: 0.1  # (float) image translation (+/- fraction)
scale: 0.5  # (float) image scale (+/- gain)
shear: 0.0  # (float) image shear (+/- deg)
perspective: 0.0  # (float) image perspective (+/- fraction), range 0-0.001
flipud: 0.0  # (float) image flip up-down (probability)
fliplr: 0.5  # (float) image flip left-right (probability)
mosaic: 1.0  # (float) image mosaic (probability)
mixup: 0.0  # (float) image mixup (probability)
copy_paste: 0.0  # (float) segment copy-paste (probability)
auto_augment: randaugment  # (str) auto augmentation policy for classification (randaugment, autoaugment, augmix)
erasing: 0.4  # (float) probability of random erasing during classification training (0-1)
crop_fraction: 1.0  # (float) image crop fraction for classification evaluation/inference (0-1)

# Custom config.yaml ---------------------------------------------------------------------------------------------------
cfg:  # (str, optional) for overriding defaults.yaml

# Tracker settings ------------------------------------------------------------------------------------------------------
tracker: botsort.yaml  # (str) tracker type, choices=[botsort.yaml, bytetrack.yaml]
```

## cli 文件

> `ultralytics/cfg/__init__.py`

# [Configuration](https://docs.ultralytics.com/usage/cfg/)

YOLO settings and hyperparameters play a critical role in the model's performance, speed, and accuracy. These settings and hyperparameters can affect the model's behavior at various stages of the model development process, including training, validation, and prediction.

YOLOv8 'yolo' CLI commands use the following syntax:

```sh
yolo TASK MODE ARGS
yolo task=detect    mode=train    model=yolov8n.pt        args...
          classify       predict        yolov8n-cls.yaml  args...
          segment        val            yolov8n-seg.yaml  args...
                         export         yolov8n.pt        format=onnx  args...

# example    后面必须使用 =
yolo predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'
yolo val detect data=coco.yaml device=0
yolo val detect data=coco128.yaml batch=1 device=0|cpu
```

Where:

- `TASK` (optional) is one of `[detect, segment, classify, pose]`. If it is not passed explicitly YOLOv8 will try to guess the `TASK` from the model type.
- `MODE` (required) is one of `[train, val, predict, export, track, benchmark]`
- `ARGS` (optional) are any number of custom `arg=value` pairs like `imgsz=320` that override defaults. For a full list of available `ARGS` see the [Configuration](https://docs.ultralytics.com/usage/cfg/) page and `defaults.yaml` GitHub [source](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/default.yaml).

YOLO models can be used for a variety of tasks, including detection, segmentation, classification and pose. These tasks differ in the type of output they produce and the specific problem they are designed to solve.

- **Detect**: For identifying and localizing objects or regions of interest in an image or video.
- **Segment**: For dividing an image or video into regions or pixels that correspond to different objects or classes.
- **Classify**: For predicting the class label of an input image.
- **Pose**: For identifying objects and estimating their keypoints in an image or video.

| Key    | Value      | Description                                     |
| :----- | :--------- | :---------------------------------------------- |
| `task` | `'detect'` | YOLO task, i.e. detect, segment, classify, pose |

## Modes

YOLO models can be used in different modes depending on the specific problem you are trying to solve. These modes include:

- **Train**: For training a YOLOv8 model on a custom dataset.

- **Val**: For validating a YOLOv8 model after it has been trained.

- **Predict**: For making predictions using a trained YOLOv8 model on new images or videos.

- **Export**: For exporting a YOLOv8 model to a format that can be used for deployment.

- **Track**: For tracking objects in real-time using a YOLOv8 model.

- **Benchmark**: For benchmarking YOLOv8 exports (ONNX, TensorRT, etc.) speed and accuracy.

| Key    | Value     | Description                                                  |
| :----- | :-------- | :----------------------------------------------------------- |
| `mode` | `'train'` | YOLO mode, i.e. train, val, predict, export, track, benchmark |

# 数据集

> 先要把数据集放入dataset中，修改data/目录下的yaml，调整为自己的数据集，需要调整路径，分类数，标签名

> yolo数据集格式(yolov5/v8的coco128和霹雳吧啦Wz的yolo3为例)
>
> txt内容，每一行都是 `3 0.933536 0.486124 0.030408 0.154487`
>
> 是 label 中心横坐标与图像宽度比值 中心纵坐标与图像高度比值 bbox宽度与图像宽度比值 bbox高度与图像宽高比值

```sh
#-------------------------------------------#
# 	yolov5 v8的格式
#-------------------------------------------#
yaml:
    path: ../datasets/coco128   # dataset root dir
    train: images/train         # train images (relative to 'path') 128 images
    val: images/val             # val images (relative to 'path') 128 images
    test: images/test           # test images (optional)

dir:
    datasets
    ├── coco128
        ├── images
        │   ├── train   # 训练图片
        │   ├── val     # 验证图片
        │   └── test    # 测试图片
        └── labels
            ├── train   # 训练标签txt
            ├── val     # 验证标签txt
            └── test    # 测试标签txt

#-------------------------------------------#
# 	yolov5 v8另的一种图片目录格式
#-------------------------------------------#
yaml:
    path: ../datasets/coco128   # dataset root dir
    train: train/images         # train images (relative to 'path')
    val: val/images             # val images (relative to 'path')
    test: test/images           # test images (optional)
dir:
    datasets
    ├── coco128
        ├── train
        │   ├── images  # 训练图片
        │   └── labels  # 训练标签txt
        ├── val
        │   ├── images  # 验证图片
        │   └── labels  # 验证标签txt
        └── test
            ├── images  # 测试图片
            └── labels  # 测试标签txt
```

> `ultralytics/datasets/class20.yaml`

```yaml
# YOLOv5 🚀 by Ultralytics, GPL-3.0 license
# COCO128 dataset https://www.kaggle.com/ultralytics/coco128 (first 128 images from COCO train2017) by Ultralytics
# Example usage: python train.py --data coco128.yaml
# parent
# ├── ultralytics
# |   └── ultralytics
# └── datasets
#     └── yourname
#         └── images/
#             └── train2017/  存放训练图片
#             └── val2017/    存放验证图片
#         └── labels/
#             └── train2017/  存放训练标签  class x_center y_center width height
#             └── val2017/    存放验证标签


# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ../datasets/classes20  # dataset root dir
train: images/train2017  # train images (relative to 'path') 128 images
val: images/val2017  # val images (relative to 'path') 128 images
test:  # test images (optional)

# Classes
names:
  0: aeroplane
  1: bicycle
  2: bird
  3: boat
  4: bottle
  5: bus
  6: car
  7: cat
  8: chair
  9: cow
  10: diningtable
  11: dog
  12: horse
  13: motorbike
  14: person
  15: pottedplant
  16: sheep
  17: sofa
  18: train
  19: tvmonitor
```

# 下载权重

> 将下载好的权重放到`weights/`文件下下

## 模型

所有的 YOLOv8 预训练模型都可以在此找到。检测、分割和姿态模型在 [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/coco.yaml) 数据集上进行预训练，而分类模型在 [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/ImageNet.yaml) 数据集上进行预训练。

在首次使用时，[模型](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models) 会自动从最新的 Ultralytics [发布版本](https://github.com/ultralytics/assets/releases)中下载。

| 模型                                                         | 尺寸 （像素） | mAPval 50-95 | 推理速度 CPU ONNX (ms) | 推理速度 A100 TensorRT (ms) | 参数量 (M) | FLOPs (B) |
| ------------------------------------------------------------ | ------------- | ------------ | ---------------------- | --------------------------- | ---------- | --------- |
| [YOLOv8n](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt) | 640           | 37.3         | 80.4                   | 0.99                        | 3.2        | 8.7       |
| [yolov8n](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt) | 640           | 44.9         | 128.4                  | 1.20                        | 11.2       | 28.6      |
| [YOLOv8m](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt) | 640           | 50.2         | 234.7                  | 1.83                        | 25.9       | 78.9      |
| [YOLOv8l](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt) | 640           | 52.9         | 375.2                  | 2.39                        | 43.7       | 165.2     |
| [YOLOv8x](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt) | 640           | 53.9         | 479.1                  | 3.53                        | 68.2       | 257.8     |

- **mAPval** 结果都在 [COCO val2017](http://cocodataset.org/) 数据集上，使用单模型单尺度测试得到。
  复现命令 `yolo val detect data=coco.yaml device=0`
- **推理速度**使用 COCO 验证集图片推理时间进行平均得到，测试环境使用 [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) 实例。
  复现命令 `yolo val detect data=coco128.yaml batch=1 device=0|cpu`

# [训练](https://docs.ultralytics.com/zh/modes/train/)

## 训练模式的关键特性

以下是YOLOv8训练模式的一些显著特点：

- **自动数据集下载:** 标准数据集如COCO、VOC和ImageNet将在首次使用时自动下载。
- **多GPU支持:** 无缝地跨多个GPU扩展您的训练工作，以加快过程。
- **超参数配置:** 通过YAML配置文件或CLI参数修改超参数的选项。
- **可视化和监控:** 实时跟踪训练指标并可视化学习过程，以获得更好的洞察力。

## 使用示例

### **单GPU和CPU训练示例**

设备将自动确定。如果有可用的GPU，那么将使用它，否则将在CPU上开始训练。

> python

```python
from ultralytics import YOLO

# 加载一个模型
model = YOLO('yolov8n.yaml')  # 从YAML建立一个新模型
model = YOLO('yolov8n.pt')  # 加载预训练模型（推荐用于训练）
model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # 从YAML建立并转移权重

# 训练模型
results = model.train(data='coco128.yaml', epochs=100, imgsz=640)
```

```python
from ultralytics import YOLO

# yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10
# fraction=1.0 cos_lr=True device=0 model=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml

# 加载一个模型
model = YOLO('weights/yolov8n.pt')  # 加载预训练模型（推荐用于训练）

# 训练模型
results = model.train(
    imgsz=640,
    batch=-1,
    workers=8,
    epochs=300,
    patience=0,
    close_mosaic=10,
    fraction=1.0,
    cos_lr=True,
    device=0,
    data='ultralytics/cfg/datasets/coco128.yaml',
)

print(results)
```

> cli

```sh
# 从YAML构建新模型，从头开始训练
yolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640

# 从预训练*.pt模型开始训练
yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640

# 从YAML构建一个新模型，转移预训练权重，然后开始训练
yolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640
```

### 多GPU训练

多GPU训练通过在多个GPU上分布训练负载，实现对可用硬件资源的更有效利用。无论是通过Python API还是命令行界面，都可以使用此功能。 若要启用多GPU训练，请指定您希望使用的GPU设备ID。

> python

```python
from ultralytics import YOLO

# 加载模型
model = YOLO('yolov8n.pt')  # 加载预训练模型（推荐用于训练）

# 使用2个GPU训练模型
results = model.train(data='coco128.yaml', epochs=100, imgsz=640, device=[0, 1])
```

> cli

```sh
# 使用GPU 0和1从预训练*.pt模型开始训练
yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1
```

### 恢复中断的训练

在处理深度学习模型时，从之前保存的状态恢复训练是一个关键特性。在各种情况下，这可能很方便，比如当训练过程意外中断，或者当您希望用新数据或更多时期继续训练模型时。

恢复训练时，Ultralytics YOLO将加载最后保存的模型的权重，并恢复优化器状态、学习率调度器和时期编号。这允许您无缝地从离开的地方继续训练过程。

在Ultralytics YOLO中，您可以通过在调用`train`方法时将`resume`参数设置为`True`并指定包含部分训练模型权重的`.pt`文件路径来轻松恢复训练。

下面是使用Python和命令行恢复中断训练的示例：

> python

```python
from ultralytics import YOLO

# 加载模型
model = YOLO('path/to/last.pt')  # 加载部分训练的模型

# 恢复训练
results = model.train(resume=True)
```

> cli

```sh
# 恢复中断的训练
yolo train resume model=path/to/last.pt
```

通过设置`resume=True`，`train`函数将从'path/to/last.pt'文件中存储的状态继续训练。如果省略`resume`参数或将其设置为`False`，`train`函数将启动新的训练会话。

请记住，默认情况下，检查点会在每个时期结束时保存，或者使用`save_period`参数以固定间隔保存，因此您必须至少完成1个时期才能恢复训练运行。

## 参数

YOLO模型的训练设置是指用于对数据集进行模型训练的各种超参数和配置。这些设置会影响模型的性能、速度和准确性。一些常见的YOLO训练设置包括批大小、学习率、动量和权重衰减。其他可能影响训练过程的因素包括优化器的选择、损失函数的选择以及训练数据集的大小和组成。仔细调整和实验这些设置以实现给定任务的最佳性能是非常重要的。

| 键                | 值       | 描述                                                         |
| :---------------- | :------- | :----------------------------------------------------------- |
| `model`           | `None`   | 模型文件路径，例如 yolov8n.pt, yolov8n.yaml                  |
| `data`            | `None`   | 数据文件路径，例如 coco128.yaml                              |
| `epochs`          | `100`    | 训练的轮次数量                                               |
| `time`            | `None`   | 训练的时长（浮点数），如果提供就会覆盖epochs参数             |
| `patience`        | `50`     | 早停训练的等待轮次                                           |
| `batch`           | `16`     | 每批图像数量（-1为自动批大小）                               |
| `imgsz`           | `640`    | 输入图像的大小，以整数表示                                   |
| `save`            | `True`   | 保存训练检查点和预测结果                                     |
| `save_period`     | `-1`     | 每x轮次保存检查点（如果<1则禁用）                            |
| `cache`           | `False`  | True/ram, disk 或 False。使用缓存加载数据                    |
| `device`          | `None`   | 运行设备，例如 cuda device=0 或 device=0,1,2,3 或 device=cpu |
| `workers`         | `8`      | 数据加载的工作线程数（如果DDP则为每个RANK）                  |
| `project`         | `None`   | 项目名称                                                     |
| `name`            | `None`   | 实验名称                                                     |
| `exist_ok`        | `False`  | 是否覆盖现有实验                                             |
| `pretrained`      | `True`   | (bool 或 str) 是否使用预训练模型（bool）或从中加载权重的模型（str） |
| `optimizer`       | `'auto'` | 使用的优化器，选择范围=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto] |
| `verbose`         | `False`  | 是否打印详细输出                                             |
| `seed`            | `0`      | 随机种子，用于可重复性                                       |
| `deterministic`   | `True`   | 是否启用确定性模式                                           |
| `single_cls`      | `False`  | 将多类数据作为单类训练                                       |
| `rect`            | `False`  | 矩形训练，每批为最小填充整合                                 |
| `cos_lr`          | `False`  | 使用余弦学习率调度器                                         |
| `close_mosaic`    | `10`     | (int) 最后轮次禁用马赛克增强（0为禁用）                      |
| `resume`          | `False`  | 从最后检查点恢复训练                                         |
| `amp`             | `True`   | 自动混合精度（AMP）训练，选择范围=[True, False]              |
| `fraction`        | `1.0`    | 训练的数据集比例（默认为1.0，即训练集中的所有图像）          |
| `profile`         | `False`  | 在训练期间为记录器分析ONNX和TensorRT速度                     |
| `freeze`          | `None`   | (int 或 list, 可选) 在训练期间冻结前n层，或冻结层索引列表    |
| `lr0`             | `0.01`   | 初始学习率（例如 SGD=1E-2, Adam=1E-3）                       |
| `lrf`             | `0.01`   | 最终学习率 (lr0 * lrf)                                       |
| `momentum`        | `0.937`  | SGD动量/Adam beta1                                           |
| `weight_decay`    | `0.0005` | 优化器权重衰减5e-4                                           |
| `warmup_epochs`   | `3.0`    | 热身轮次（小数ok）                                           |
| `warmup_momentum` | `0.8`    | 热身初始动量                                                 |
| `warmup_bias_lr`  | `0.1`    | 热身初始偏差lr                                               |
| `box`             | `7.5`    | 框损失增益                                                   |
| `cls`             | `0.5`    | cls损失增益（根据像素缩放）                                  |
| `dfl`             | `1.5`    | dfl损失增益                                                  |
| `pose`            | `12.0`   | 姿态损失增益（仅限姿态）                                     |
| `kobj`            | `2.0`    | 关键点obj损失增益（仅限姿态）                                |
| `label_smoothing` | `0.0`    | 标签平滑（小数）                                             |
| `nbs`             | `64`     | 标称批大小                                                   |
| `overlap_mask`    | `True`   | 训练期间掩码应重叠（仅限分割训练）                           |
| `mask_ratio`      | `4`      | 掩码降采样比率（仅限分割训练）                               |
| `dropout`         | `0.0`    | 使用dropout正则化（仅限分类训练）                            |
| `val`             | `True`   | 训练期间验证/测试                                            |

>`rect = True` 使用长方形训练
>
>Setting "rect"=True allows you to train using rectangular images, not necessarily square ones. This allows for more efficient use of GPU memory as there's less need for padding spatial dimensions.
>
>[Custom input size: letterbox vs resizing · Issue #11350 ](https://github.com/ultralytics/yolov5/issues/11350)
>
>[About the rectangle training · Issue #4819](https://github.com/ultralytics/ultralytics/issues/4819)

## example



```sh
# Build a new model from YAML and start training from scratch
yolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640

# Start training from a pretrained *.pt model
yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640

# Build a new model from YAML, transfer pretrained weights to it and start training
yolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640
```

> `Multi-GPU Training`

```sh
# Start training from a pretrained *.pt model using GPUs 0 and 1
yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1
```

> `auto optimizer`

```sh
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=ultralytics/cfg/models/v8/yolov8n.yaml pretrained=weights/yolov8n.pt data=ultralytics/datasets/coco128.yaml

#                                                                                                                                model可以直接设置为pt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml

#                                                        rtdetr 训练轮数更少
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=100 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=ultralytics/cfg/models/rt-detr/rtdetr-x.yaml pretrained=weights/rtdetr-x.pt data=ultralytics/cfg/datasets/coco128.yaml

#                                                        rtdetr 训练轮数更少                                                       model可以直接设置为pt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=100 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=weights/rtdetr-x.pt data=ultralytics/cfg/datasets/coco128.yaml
```

> `SGD`

```sh
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 optimizer=SGD lr0=0.01 cos_lr=True device=0 model=ultralytics/cfg/models/v8/yolov8n.yaml pretrained=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml

#                                                                                                                                                       model可以直接设置为pt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 optimizer=SGD lr0=0.01 cos_lr=True device=0 model=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml
```

> `Adam`

```sh
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 optimizer=AdamW lr0=0.001 cos_lr=True device=0 model=ultralytics/cfg/models/v8/yolov8n.yaml pretrained=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml

#                                                                                                                                                          model可以直接设置为pt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 optimizer=AdamW lr0=0.001 cos_lr=True device=0 model=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml
```

> `resume`

```sh
#                                                                                                                                model=最后的pt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=weights/last.pt data=ultralytics/cfg/datasets/coco128.yaml resume=True exist_ok=True
```

## **不需要在模型配置中显示更改类别数**

> 会自动将nc调整为数据集的类别数量

```sh
> yolo task=detect mode=train imgsz=640 batch=-1 epochs=300 optimizer=SGD lr0=0.01 cos_lr=True device=0 pretrained=weights/yolov8n.pt model=ultralytics/models/v8/yolov8n.yaml data=ultralytics/datasets/classes20.yaml

                   from  n    params  module                                       arguments
  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]
  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]
  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]
  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]
  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]
  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]
  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]
  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]
  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]
  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]
 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]
 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]
 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]
 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]
 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]
 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]
 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]
 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]
 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]
 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]
 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.Detect                [80, [64, 128, 256]]
yolov8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs

Transferred 355/355 items from pretrained weights
Ultralytics YOLOv8.0.58  Python-3.10.9 torch-2.0.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11264MiB)
yolo\engine\trainer: task=detect, mode=train, model=ultralytics/models/v8/yolov8n.yaml, data=ultralytics/datasets/classes20.yaml, epochs=300, patience=50, batch=-1, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=None, exist_ok=False, pretrained=weights/yolov8n.pt, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=d:\code\ultralytics\runs\detect\train2
Overriding model.yaml nc=80 with nc=20		# 这里自动覆盖了旧的类别数

                   from  n    params  module                                       arguments
  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]
  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]
  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]
  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]
  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]
  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]
  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]
  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]
  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]
  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]
 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]
 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]
 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]
 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]
 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]
 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]
 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]
 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]
 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]
 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]
 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.Detect                [20, [64, 128, 256]]
yolov8n summary: 225 layers, 3014748 parameters, 3014732 gradients, 8.2 GFLOPs

Transferred 319/355 items from pretrained weights
TensorBoard: Start with 'tensorboard --logdir d:\code\ultralytics\runs\detect\train', view at http://localhost:6006/
AMP: running Automatic Mixed Precision (AMP) checks with yolov8n...
AMP: checks passed
AutoBatch: Computing optimal batch size for imgsz=640
AutoBatch: CUDA:0 (NVIDIA GeForce GTX 1080 Ti) 11.00G total, 0.10G reserved, 0.07G allocated, 10.83G free
      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output
     3014748       8.215         0.210         28.59         17.95        (1, 3, 640, 640)                    list
     3014748       16.43         0.296         13.96         21.27        (2, 3, 640, 640)                    list
     3014748       32.86         0.581         12.96         20.99        (4, 3, 640, 640)                    list
     3014748       65.72         1.065         20.27          28.6        (8, 3, 640, 640)                    list
     3014748       131.4         2.334         34.56         48.56       (16, 3, 640, 640)                    list
AutoBatch: Using batch-size 50 for CUDA:0 7.30G/11.00G (66%)
optimizer: SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.000390625), 63 bias
train: Scanning D:\code\datasets\classes20\labels\train.cache... 5266 images, 0 backgrounds, 0 corrupt: 100%|██████████
val: Scanning D:\code\datasets\classes20\labels\val.cache... 586 images, 0 backgrounds, 0 corrupt: 100%|██████████| 586
Plotting labels to d:\code\ultralytics\runs\detect\train\labels.jpg...
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to d:\code\ultralytics\runs\detect\train
Starting training for 300 epochs...
```

> 自动调整 `nc` 的代码在 `ultralytics/nn/task.py`

```python
        ch = self.yaml['ch'] = self.yaml.get('ch', ch)  # input channels
        if nc and nc != self.yaml['nc']:    # 使用data config中的names长度覆盖模型配置文件中的类别
            LOGGER.info(f"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}")
            self.yaml['nc'] = nc  # override yaml value
```

## 训练时出现的问题

### 训练 `obj_loss` 增大 | reduce FPs | 解决特殊场景模型拍摄日常目标的FP数量过多

> [how to use Background images in training? · Issue #2844 · ultralytics/yolov5 (github.com)](https://github.com/ultralytics/yolov5/issues/2844)
>
> 在图片训练文件夹 `images/train` 中添加背景图片文件，比如coco或者voc数据集的一些照片
>
> 不需要添加空白label txt文件，添加了也不会出错
>
> `(if no objects in image, no `*.txt` file is required).`
>
> [目标检测（降低误检测率及小目标检测系列笔记）](https://blog.csdn.net/weixin_44836143/article/details/105952819)

```sh
train: Scanning D:\code\datasets\classes20\labels\train... 5266 images, 1000 backgrounds, 0 corrupt: 100%|██████████|
train: New cache created: D:\code\datasets\classes20\labels\train.cache
val: Scanning D:\code\datasets\classes20\labels\val... 586 images, 0 backgrounds, 0 corrupt: 100%|██████████|
val: New cache created: D:\code\datasets\classes20\labels\val.cache
```



# [验证](https://docs.ultralytics.com/zh/modes/val/)

## Val 模式的主要特点

以下是 YOLOv8 的 Val 模式提供的显著功能：

- **自动化设置：** 模型记住其训练配置，以便直接进行验证。
- **多指标支持：** 根据一系列准确度指标评估您的模型。
- **CLI 和 Python API：** 根据您的验证偏好选择命令行界面或 Python API。
- **数据兼容性：** 与训练阶段使用的数据集以及自定义数据集无缝协作。

## 使用示例

在 COCO128 数据集上验证训练过的 YOLOv8n 模型的准确性。由于 `model` 保留了其训练的 `data` 和参数作为模型属性，因此无需传递任何参数。有关完整的导出参数列表，请参阅下面的参数部分。

> python

```python
from ultralytics import YOLO

# 加载模型
model = YOLO('yolov8n.pt')  # 加载官方模型
model = YOLO('path/to/best.pt')  # 加载自定义模型

# 验证模型
metrics = model.val()  # 无需参数，数据集和设置记忆
metrics.box.map    # map50-95
metrics.box.map50  # map50
metrics.box.map75  # map75
metrics.box.maps   # 包含每个类别的map50-95列表
```

> cli

```sh
yolo detect val model=yolov8n.pt  # 验证官方模型
yolo detect val model=path/to/best.pt  # 验证自定义模型
```

## 参数

YOLO 模型的验证设置是指用于评估模型在验证数据集上性能的各种超参数和配置。这些设置会影响模型的性能、速度和准确性。一些常见的 YOLO 验证设置包括批处理大小、在训练期间验证频率以及用于评估模型性能的指标。其他可能影响验证过程的因素包括验证数据集的大小和组成以及模型用于特定任务的特性。仔细调整和实验这些设置很重要，以确保模型在验证数据集上表现良好并且检测和预防过拟合。

| 键            | 值      | 描述                                                   |
| :------------ | :------ | :----------------------------------------------------- |
| `data`        | `None`  | 数据文件的路径，例如 coco128.yaml                      |
| `imgsz`       | `640`   | 输入图像的大小，以整数表示                             |
| `batch`       | `16`    | 每批图像的数量（AutoBatch 为 -1）                      |
| `save_json`   | `False` | 将结果保存至 JSON 文件                                 |
| `save_hybrid` | `False` | 保存混合版本的标签（标签 + 额外预测）                  |
| `conf`        | `0.001` | 用于检测的对象置信度阈值                               |
| `iou`         | `0.6`   | NMS（非极大抑制）用的交并比（IoU）阈值                 |
| `max_det`     | `300`   | 每张图像的最大检测数量                                 |
| `half`        | `True`  | 使用半精度（FP16）                                     |
| `device`      | `None`  | 运行所用的设备，例如 cuda device=0/1/2/3 或 device=cpu |
| `dnn`         | `False` | 使用 OpenCV DNN 进行 ONNX 推理                         |
| `plots`       | `False` | 在训练期间显示图表                                     |
| `rect`        | `False` | 矩形验证，每批图像为了最小填充整齐排列                 |
| `split`       | `val`   | 用于验证的数据集分割，例如 'val'、'test' 或 'train'    |

> example

```sh
yolo detect val model=yolov8n.pt  # val official model
yolo detect val model=path/to/best.pt  # val custom model
```

### default confidence threshold = 0.001

> [mAP bug at higher --conf · Issue #1466 · ultralytics/yolov5](https://github.com/ultralytics/yolov5/issues/1466)
>
> [Why does the confidence threshold of 0.001 in val.py result in good results? · Issue #11745 · ultralytics/yolov5](https://github.com/ultralytics/yolov5/issues/11745)

### 验证模型在自定义数据集上的效果 精度0.995

> https://www.jianshu.com/p/cfb01add61bd#1684051613808
>
> https://github.com/ultralytics/yolov5/issues/5508
>
> https://github.com/ultralytics/yolov5/issues/1563
>
> https://github.com/ultralytics/yolov5/pull/1646
>
> `savehybrid` 会合并已知的labels，导致得分很高

## torch

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.pt device=0
```

## torchscript

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.torchscript device=0
```

## onnx

> 注意:
>
> `onnxruntime` 和 `onnxruntime-gpu` 不要同时安装，否则使用 `gpu` 推理时速度会很慢，如果同时安装了2个包，要全部卸载，再安装 `onnxruntime-gpu` 才能使用gpu推理，否则gpu速度会很慢

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.onnx device=0
```

## openvino

> 注意：openvino没法使用cuda，但是使用 --device 0 会提高推理速度

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n_openvnio_model device=cpu
```

## tensorrt

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.onnx device=0 half=True
```

# [预测](https://docs.ultralytics.com/zh/modes/predict/)

## 预测模式的关键特性

YOLOv8 的预测模式被设计为强大且多功能，包括以下特性：

- **兼容多个数据来源：** 无论您的数据是单独图片，图片集合，视频文件，还是实时视频流，预测模式都能胜任。
- **流式模式：** 使用流式功能生成一个内存高效的 `Results` 对象生成器。在调用预测器时，通过设置 `stream=True` 来启用此功能。
- **批处理：** 能够在单个批次中处理多个图片或视频帧，进一步加快推理时间。
- **易于集成：** 由于其灵活的 API，易于与现有数据管道和其他软件组件集成。

## 使用示例

Ultralytics YOLO 模型在进行推理时返回一个 Python `Results` 对象列表，或者当传入 `stream=True` 时，返回一个内存高效的 Python `Results` 对象生成器：

> 使用 `stream=False` 返回列表

```python
from ultralytics import YOLO

# 加载模型
model = YOLO('yolov8n.pt')  # 预训练的 YOLOv8n 模型

# 在图片列表上运行批量推理
results = model(['im1.jpg', 'im2.jpg'])  # 返回 Results 对象列表

# 处理结果列表
for result in results:
    boxes = result.boxes  # 边界框输出的 Boxes 对象
    masks = result.masks  # 分割掩码输出的 Masks 对象
    keypoints = result.keypoints  # 姿态输出的 Keypoints 对象
    probs = result.probs  # 分类输出的 Probs 对象
```

> 使用 `stream=True` 返回生成器

```python
from ultralytics import YOLO

# 加载模型
model = YOLO('yolov8n.pt')  # 预训练的 YOLOv8n 模型

# 在图片列表上运行批量推理
results = model(['im1.jpg', 'im2.jpg'], stream=True)  # 返回 Results 对象生成器

# 处理结果生成器
for result in results:
    boxes = result.boxes  # 边界框输出的 Boxes 对象
    masks = result.masks  # 分割掩码输出的 Masks 对象
    keypoints = result.keypoints  # 姿态输出的 Keypoints 对象
    probs = result.probs  # 分类输出的 Probs 对象
```

## 推理来源

YOLOv8 可以处理推理输入的不同类型，如下表所示。来源包括静态图像、视频流和各种数据格式。表格还表示了每种来源是否可以在流式模式下使用，使用参数 `stream=True` ✅。流式模式对于处理视频或实时流非常有利，因为它创建了结果的生成器，而不是将所有帧加载到内存。

> 使用 `stream=True` 处理长视频或大型数据集来高效地管理内存。当 `stream=False` 时，所有帧或数据点的结果都将存储在内存中，这可能很快导致内存不足错误。相对地，`stream=True` 使用生成器，只保留当前帧或数据点的结果在内存中，显著减少了内存消耗，防止内存不足问题。

| 来源       | 参数                                       | 类型            | 备注                                                         |
| :--------- | :----------------------------------------- | :-------------- | :----------------------------------------------------------- |
| 图像       | `'image.jpg'`                              | `str` 或 `Path` | 单个图像文件。                                               |
| URL        | `'https://ultralytics.com/images/bus.jpg'` | `str`           | 图像的 URL 地址。                                            |
| 截屏       | `'screen'`                                 | `str`           | 截取屏幕图像。                                               |
| PIL        | `Image.open('im.jpg')`                     | `PIL.Image`     | RGB 通道的 HWC 格式图像。                                    |
| OpenCV     | `cv2.imread('im.jpg')`                     | `np.ndarray`    | BGR 通道的 HWC 格式图像 `uint8 (0-255)`。                    |
| numpy      | `np.zeros((640,1280,3))`                   | `np.ndarray`    | BGR 通道的 HWC 格式图像 `uint8 (0-255)`。                    |
| torch      | `torch.zeros(16,3,320,640)`                | `torch.Tensor`  | RGB 通道的 BCHW 格式图像 `float32 (0.0-1.0)`。               |
| CSV        | `'sources.csv'`                            | `str` 或 `Path` | 包含图像、视频或目录路径的 CSV 文件。                        |
| 视频 ✅     | `'video.mp4'`                              | `str` 或 `Path` | 如 MP4, AVI 等格式的视频文件。                               |
| 目录 ✅     | `'path/'`                                  | `str` 或 `Path` | 包含图像或视频文件的目录路径。                               |
| 通配符 ✅   | `'path/*.jpg'`                             | `str`           | 匹配多个文件的通配符模式。使用 `*` 字符作为通配符。          |
| YouTube ✅  | `'https://youtu.be/LNwODJXcvt4'`           | `str`           | YouTube 视频的 URL 地址。                                    |
| 流媒体 ✅   | `'rtsp://example.com/media.mp4'`           | `str`           | RTSP, RTMP, TCP 或 IP 地址等流协议的 URL 地址。              |
| 多流媒体 ✅ | `'list.streams'`                           | `str` 或 `Path` | 一个流 URL 每行的 `*.streams` 文本文件，例如 8 个流将以 8 的批处理大小运行。 |

## 推理参数

`model.predict()` 在推理时接受多个参数，可以用来覆盖默认值：

```python
from ultralytics import YOLO

# 加载预训练的YOLOv8n模型
model = YOLO('yolov8n.pt')

# 在'bus.jpg'上运行推理，并附加参数
model.predict('bus.jpg', save=True, imgsz=320, conf=0.5)
```

支持的所有参数：

| 名称            | 类型           | 默认值                 | 描述                                                 |
| :-------------- | :------------- | :--------------------- | :--------------------------------------------------- |
| `source`        | `str`          | `'ultralytics/assets'` | 图像或视频的源目录                                   |
| `conf`          | `float`        | `0.25`                 | 检测对象的置信度阈值                                 |
| `iou`           | `float`        | `0.7`                  | 用于NMS的交并比（IoU）阈值                           |
| `imgsz`         | `int or tuple` | `640`                  | 图像大小，可以是标量或（h, w）列表，例如（640, 480） |
| `half`          | `bool`         | `False`                | 使用半精度（FP16）                                   |
| `device`        | `None or str`  | `None`                 | 运行设备，例如 cuda device=0/1/2/3 或 device=cpu     |
| `show`          | `bool`         | `False`                | 如果可能，显示结果                                   |
| `save`          | `bool`         | `False`                | 保存带有结果的图像                                   |
| `save_txt`      | `bool`         | `False`                | 将结果保存为.txt文件                                 |
| `save_conf`     | `bool`         | `False`                | 保存带有置信度分数的结果                             |
| `save_crop`     | `bool`         | `False`                | 保存带有结果的裁剪图像                               |
| `show_labels`   | `bool`         | `True`                 | 隐藏标签                                             |
| `show_conf`     | `bool`         | `True`                 | 隐藏置信度分数                                       |
| `max_det`       | `int`          | `300`                  | 每张图像的最大检测数量                               |
| `vid_stride`    | `bool`         | `False`                | 视频帧速率跳跃                                       |
| `stream_buffer` | `bool`         | `False`                | 缓冲所有流媒体帧（True）或返回最新帧（False）        |
| `line_width`    | `None or int`  | `None`                 | 边框线宽度。如果为None，则按图像大小缩放。           |
| `visualize`     | `bool`         | `False`                | 可视化模型特征                                       |
| `augment`       | `bool`         | `False`                | 应用图像增强到预测源                                 |
| `agnostic_nms`  | `bool`         | `False`                | 类别不敏感的NMS                                      |
| `retina_masks`  | `bool`         | `False`                | 使用高分辨率分割掩码                                 |
| `classes`       | `None or list` | `None`                 | 按类别过滤结果，例如 classes=0，或 classes=[0,2,3]   |
| `boxes`         | `bool`         | `True`                 | 在分割预测中显示框                                   |

## torch

```sh
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.pt source=ultralytics/assets/bus.jpg device=0

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.pt source=../datasets/coco128/images/train2017 device=0
```

## torchscript

```sh
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.torchscript source=ultralytics/assets/bus.jpg device=0

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.torchscript source=../datasets/coco128/images/train2017 device=0
```

## onnx

> 注意:
>
> `onnxruntime` 和 `onnxruntime-gpu` 不要同时安装，否则使用 `gpu` 推理时速度会很慢，如果同时安装了2个包，要全部卸载，再安装 `onnxruntime-gpu` 才能使用gpu推理，否则gpu速度会很慢

```sh
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.onnx source=ultralytics/assets/bus.jpg device=0
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.onnx source=../datasets/coco128/images/train2017 device=0

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp16.onnx half=True source=ultralytics/assets/bus.jpg device=0           # fp16模型需要 half=True
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp16.onnx half=True source=../datasets/coco128/images/train2017 device=0

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.cpu.dynamic.onnx source=ultralytics/assets/bus.jpg device=0              # 使用cpu导出的dynamic模型可以用gpu推理
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.cpu.dynamic.onnx source=../datasets/coco128/images/train2017 device=0
```

## openvino

> 注意：openvino没法使用cuda，但是使用 `device=0` 会提高推理速度

```sh
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n_openvino_model source=ultralytics/assets/bus.jpg device=cpu

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n_openvino_model source=../datasets/coco128/images/train2017 device=cpu
```

## tensorrt

```sh
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.engine half=True source=ultralytics/assets/bus.jpg device=0                          # fp32模型也能用 --half 推理
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.engine half=True source=../datasets/coco128/images/train2017 device=0

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp16.engine half=True source=ultralytics/assets/bus.jpg device=0
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp16.engine half=True source=../datasets/coco128/images/train2017 device=0

yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp32.dynamic.engine half=True source=ultralytics/assets/bus.jpg device=0             # fp32模型也能用 --half 推理
yolo task=detect mode=predict imgsz=640 save=True save_txt=True save_conf=True save_crop=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp32.dynamic.engine half=True source=../datasets/coco128/images/train2017 device=0
```



# [导出](https://docs.ultralytics.com/zh/modes/export/)

## 导出模式的关键特性

以下是一些突出的功能：

- **一键导出：** 用于导出到不同格式的简单命令。
- **批量导出：** 支持批推理能力的模型导出。
- **优化推理：** 导出的模型针对更快的推理时间进行优化。
- **教学视频：** 提供深入指导和教学，确保流畅的导出体验。

## 使用示例

将 YOLOv8n 模型导出为 ONNX 或 TensorRT 等不同格式。查看下面的参数部分，了解完整的导出参数列表。

> python

```python
from ultralytics import YOLO

# 加载模型
model = YOLO('yolov8n.pt')  # 加载官方模型
model = YOLO('path/to/best.pt')  # 加载自定义训练的模型

# 导出模型
model.export(format='onnx')
```

> cli

```sh
yolo export model=yolov8n.pt format=onnx  # 导出官方模型
yolo export model=path/to/best.pt format=onnx  # 导出自定义训练的模型
```

## 参数

YOLO 模型的导出设置是指用于在其他环境或平台中使用模型时保存或导出模型的各种配置和选项。这些设置会影响模型的性能、大小和与不同系统的兼容性。一些常见的 YOLO 导出设置包括导出的模型文件格式（例如 ONNX、TensorFlow SavedModel）、模型将在哪个设备上运行（例如 CPU、GPU）以及是否包含附加功能，如遮罩或每个框多个标签。其他可能影响导出过程的因素包括模型用途的具体细节以及目标环境或平台的要求或限制。重要的是要仔细考虑和配置这些设置，以确保导出的模型针对预期用例经过优化，并且可以在目标环境中有效使用。

| 键          | 值              | 描述                                                |
| :---------- | :-------------- | :-------------------------------------------------- |
| `format`    | `'torchscript'` | 导出的格式                                          |
| `imgsz`     | `640`           | 图像尺寸，可以是标量或 (h, w) 列表，比如 (640, 480) |
| `keras`     | `False`         | 使用 Keras 导出 TF SavedModel                       |
| `optimize`  | `False`         | TorchScript：为移动设备优化                         |
| `half`      | `False`         | FP16 量化                                           |
| `int8`      | `False`         | INT8 量化                                           |
| `dynamic`   | `False`         | ONNX/TensorRT：动态轴                               |
| `simplify`  | `False`         | ONNX/TensorRT：简化模型                             |
| `opset`     | `None`          | ONNX：opset 版本（可选，默认为最新版本）            |
| `workspace` | `4`             | TensorRT：工作区大小（GB）                          |
| `nms`       | `False`         | CoreML：添加 NMS                                    |

## 导出格式

下表中提供了可用的 YOLOv8 导出格式。您可以使用 `format` 参数导出任何格式的模型，比如 `format='onnx'` 或 `format='engine'`。

| 格式                                                         | `format` 参数 | 模型                      | 元数据 | 参数                                                |
| :----------------------------------------------------------- | :------------ | :------------------------ | :----- | :-------------------------------------------------- |
| [PyTorch](https://pytorch.org/)                              | -             | `yolov8n.pt`              | ✅      | -                                                   |
| [TorchScript](https://pytorch.org/docs/stable/jit.html)      | `torchscript` | `yolov8n.torchscript`     | ✅      | `imgsz`, `optimize`                                 |
| [ONNX](https://onnx.ai/)                                     | `onnx`        | `yolov8n.onnx`            | ✅      | `imgsz`, `half`, `dynamic`, `simplify`, `opset`     |
| [OpenVINO](https://docs.openvino.ai/latest/index.html)       | `openvino`    | `yolov8n_openvino_model/` | ✅      | `imgsz`, `half`, `int8`                             |
| [TensorRT](https://developer.nvidia.com/tensorrt)            | `engine`      | `yolov8n.engine`          | ✅      | `imgsz`, `half`, `dynamic`, `simplify`, `workspace` |
| [CoreML](https://github.com/apple/coremltools)               | `coreml`      | `yolov8n.mlpackage`       | ✅      | `imgsz`, `half`, `int8`, `nms`                      |
| [TF SavedModel](https://www.tensorflow.org/guide/saved_model) | `saved_model` | `yolov8n_saved_model/`    | ✅      | `imgsz`, `keras`, `int8`                            |
| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`          | `yolov8n.pb`              | ❌      | `imgsz`                                             |
| [TF Lite](https://www.tensorflow.org/lite)                   | `tflite`      | `yolov8n.tflite`          | ✅      | `imgsz`, `half`, `int8`                             |
| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)   | `edgetpu`     | `yolov8n_edgetpu.tflite`  | ✅      | `imgsz`                                             |
| [TF.js](https://www.tensorflow.org/js)                       | `tfjs`        | `yolov8n_web_model/`      | ✅      | `imgsz`, `half`, `int8`                             |
| [PaddlePaddle](https://github.com/PaddlePaddle)              | `paddle`      | `yolov8n_paddle_model/`   | ✅      | `imgsz`                                             |
| [ncnn](https://github.com/Tencent/ncnn)                      | `ncnn`        | `yolov8n_ncnn_model/`     | ✅      | `imgsz`, `half`                                     |

> example

```sh
yolo export model=yolov8n.pt format=onnx  # export official model
yolo export model=path/to/best.pt format=onnx  # export custom trained model
```

## torchscript

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=torchscript device=0
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=torchscript device=cpu optimize=True # optimize not compatible with cuda devices, i.e. use device=cpu
```

## onnx

> 注意:
>
> `onnxruntime` 和 `onnxruntime-gpu` 不要同时安装，否则使用 `gpu` 推理时速度会很慢，如果同时安装了2个包，要全部卸载，再安装`onnxruntime-gpu` 才能使用gpu推理，否则gpu速度会很慢

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=0

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=0 half=True                # half=True only compatible with GPU export, i.e. use device=0

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=cpu dynamic=True

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=cpu half=True dynamic=True  # 导出失败 half=True not compatible with dynamic=True, i.e. use only one.
```

### opencv使用的onnx

> https://github.com/ultralytics/ultralytics/tree/main/examples/YOLOv8-OpenCV-ONNX-Python

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=0 opset=12             # opset必须为12

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=0 half=True opset=12   # opset必须为12

# opencv不支持dynamic
```

## openvino

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=openvino device=cpu

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=openvino device=cpu half=True

yolo task =detect mode=export imgsz=640 model=weights/yolov8n.pt format=openvino device=cpu int8=True data=ultralytics/cfg/datasets/coco128.yaml # INT8 export requires a data argument for calibration
```

### 通过openvino的`mo`命令将onnx转换为openvino格式(支持**fp16**)

> https://docs.openvino.ai/latest/notebooks/102-pytorch-onnx-to-openvino-with-output.html

```sh
mo --input_model "onnx_path" --output_dir "output_path" --compress_to_fp16

mo --input_model "onnx_path" --output_dir "output_path" --compress_to_fp16
```

#### 代码方式

```python
from openvino.tools import mo
from openvino.runtime import serialize

onnx_path = "onnx_path"

# fp32 IR model
fp32_path = "fp32_path"
output_path = fp32_path + ".xml"
print(f"Export ONNX to OpenVINO FP32 IR to: {output_path}")
model = mo.convert_model(onnx_path)
serialize(model, output_path)

# fp16 IR model
fp16_path = "fp16_path"
output_path = fp16_path + ".xml"

print(f"Export ONNX to OpenVINO FP16 IR to: {output_path}")
model = mo.convert_model(onnx_path, compress_to_fp16=True)
serialize(model, output_path)
```

### export failure  0.9s: DLL load failed while importing ie_api

> https://blog.csdn.net/qq_26815239/article/details/123047840
>
> 如果你使用的是 Python 3.8 或更高版本，并且是在Windows系统下通过pip安装的openvino，那么该错误的解决方案如下：

1. 进入目录 `your\env\site-packages\openvino\inference_engine`
2. 打开文件 `__init__.py`
3. 26行下添加一行

```python
        if os.path.isdir(lib_path):
            # On Windows, with Python >= 3.8, DLLs are no longer imported from the PATH.
            if (3, 8) <= sys.version_info:
                os.add_dll_directory(os.path.abspath(lib_path))
                os.environ['PATH'] = os.path.abspath(lib_path) + ';' + os.environ['PATH']	# 添加这一行
```

## tensorrt

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=engine simplify=True device=0 # 可以用simplify的onnx

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=engine simplify=True device=0 half=True
```

## ncnn

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=ncnn simplify=True device=0 # 可以用simplify的onnx

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=ncnn simplify=True device=0 half=True
```

## onnx openvino tensorrt

> 目前不支持同时导出多种格式，每种格式都要单独导出

# [跟踪](https://docs.ultralytics.com/zh/modes/track/)

## 一瞥特点

Ultralytics YOLO扩展了其物体检测功能，以提供强大且多功能的物体追踪：

- **实时追踪：** 在高帧率视频中无缝追踪物体。
- **支持多个追踪器：** 从多种成熟的追踪算法中选择。
- **自定义追踪器配置：** 通过调整各种参数来定制追踪算法，以满足特定需求。

## 可用的追踪器

Ultralytics YOLO支持以下追踪算法。可以通过传递相关的YAML配置文件如`tracker=tracker_type.yaml`来启用：

- [BoT-SORT](https://github.com/NirAharon/BoT-SORT) - 使用 `botsort.yaml` 启用此追踪器。
- [ByteTrack](https://github.com/ifzhang/ByteTrack) - 使用 `bytetrack.yaml` 启用此追踪器。

默认追踪器是BoT-SORT。



## 追踪

要在视频流中运行追踪器，请使用已训练的检测、分割或姿态模型，例如YOLOv8n、YOLOv8n-seg和YOLOv8n-pose。



> python

```python
from ultralytics import YOLO

# 加载官方或自定义模型
model = YOLO('yolov8n.pt')  # 加载一个官方的检测模型
model = YOLO('yolov8n-seg.pt')  # 加载一个官方的分割模型
model = YOLO('yolov8n-pose.pt')  # 加载一个官方的姿态模型
model = YOLO('path/to/best.pt')  # 加载一个自定义训练的模型

# 使用模型进行追踪
results = model.track(source="https://youtu.be/LNwODJXcvt4", show=True)  # 使用默认追踪器进行追踪
results = model.track(source="https://youtu.be/LNwODJXcvt4", show=True, tracker="bytetrack.yaml")  # 使用ByteTrack追踪器进行追踪
```

> cli

```sh
# 使用命令行界面进行各种模型的追踪
yolo track model=yolov8n.pt source="https://youtu.be/LNwODJXcvt4"  # 官方检测模型
yolo track model=yolov8n-seg.pt source="https://youtu.be/LNwODJXcvt4"  # 官方分割模型
yolo track model=yolov8n-pose.pt source="https://youtu.be/LNwODJXcvt4"  # 官方姿态模型
yolo track model=path/to/best.pt source="https://youtu.be/LNwODJXcvt4"  # 自定义训练模型

# 使用ByteTrack追踪器进行追踪
yolo track model=path/to/best.pt tracker="bytetrack.yaml"
```

## 配置

### 追踪参数

追踪配置与预测模式共享一些属性，如`conf`、`iou`和`show`。有关进一步配置，请参见[预测](https://docs.ultralytics.com/zh/modes/predict)模型页面。

> python

```sh
from ultralytics import YOLO

# 配置追踪参数并运行追踪器
model = YOLO('yolov8n.pt')
results = model.track(source="https://youtu.be/LNwODJXcvt4", conf=0.3, iou=0.5, show=True)
```

> cli

```sh
# 使用命令行界面配置追踪参数并运行追踪器
yolo track model=yolov8n.pt source="https://youtu.be/LNwODJXcvt4" conf=0.3, iou=0.5 show
```

### 选择追踪器

Ultralytics还允许您使用修改后的追踪器配置文件。要执行此操作，只需从[ultralytics/cfg/trackers](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/cfg/trackers)中复制一个追踪器配置文件（例如，`custom_tracker.yaml`）并根据您的需求修改任何配置（除了`tracker_type`）。



> python

```python
from ultralytics import YOLO

# 加载模型并使用自定义配置文件运行追踪器
model = YOLO('yolov8n.pt')
results = model.track(source="https://youtu.be/LNwODJXcvt4", tracker='custom_tracker.yaml')
```

> cli

```sh
# 使用命令行界面加载模型并使用自定义配置文件运行追踪器
yolo track model=yolov8n.pt source="https://youtu.be/LNwODJXcvt4" tracker='custom_tracker.yaml'
```

# [Tune](https://docs.ultralytics.com/integrations/ray-tune/)

## Usage

```python
from ultralytics import YOLO

# Load a YOLOv8n model
model = YOLO('yolov8n.pt')

# Start tuning hyperparameters for YOLOv8n training on the COCO8 dataset
result_grid = model.tune(data='coco8.yaml', use_ray=True, iterations=10)
```

## `tune()` Method Parameters

The `tune()` method in YOLOv8 provides an easy-to-use interface for hyperparameter tuning with Ray Tune. It accepts several arguments that allow you to customize the tuning process. Below is a detailed explanation of each parameter:

| Parameter       | Type             | Description                                                  | Default Value |
| :-------------- | :--------------- | :----------------------------------------------------------- | :------------ |
| `data`          | `str`            | The dataset configuration file (in YAML format) to run the tuner on. This file should specify the training and validation data paths, as well as other dataset-specific settings. |               |
| `space`         | `dict, optional` | A dictionary defining the hyperparameter search space for Ray Tune. Each key corresponds to a hyperparameter name, and the value specifies the range of values to explore during tuning. If not provided, YOLOv8 uses a default search space with various hyperparameters. |               |
| `grace_period`  | `int, optional`  | The grace period in epochs for the [ASHA scheduler](https://docs.ray.io/en/latest/tune/api/schedulers.html) in Ray Tune. The scheduler will not terminate any trial before this number of epochs, allowing the model to have some minimum training before making a decision on early stopping. | 10            |
| `gpu_per_trial` | `int, optional`  | The number of GPUs to allocate per trial during tuning. This helps manage GPU usage, particularly in multi-GPU environments. If not provided, the tuner will use all available GPUs. | None          |
| `iterations`    | `int, optional`  | The maximum number of trials to run during tuning. This parameter helps control the total number of hyperparameter combinations tested, ensuring the tuning process does not run indefinitely. | 10            |
| `**train_args`  | `dict, optional` | Additional arguments to pass to the `train()` method during tuning. These arguments can include settings like the number of training epochs, batch size, and other training-specific configurations. | {}            |

By customizing these parameters, you can fine-tune the hyperparameter optimization process to suit your specific needs and available computational resources.



## Default Search Space Description

The following table lists the default search space parameters for hyperparameter tuning in YOLOv8 with Ray Tune. Each parameter has a specific value range defined by `tune.uniform()`.

| Parameter         | Value Range                | Description                              |
| :---------------- | :------------------------- | :--------------------------------------- |
| `lr0`             | `tune.uniform(1e-5, 1e-1)` | Initial learning rate                    |
| `lrf`             | `tune.uniform(0.01, 1.0)`  | Final learning rate factor               |
| `momentum`        | `tune.uniform(0.6, 0.98)`  | Momentum                                 |
| `weight_decay`    | `tune.uniform(0.0, 0.001)` | Weight decay                             |
| `warmup_epochs`   | `tune.uniform(0.0, 5.0)`   | Warmup epochs                            |
| `warmup_momentum` | `tune.uniform(0.0, 0.95)`  | Warmup momentum                          |
| `box`             | `tune.uniform(0.02, 0.2)`  | Box loss weight                          |
| `cls`             | `tune.uniform(0.2, 4.0)`   | Class loss weight                        |
| `hsv_h`           | `tune.uniform(0.0, 0.1)`   | Hue augmentation range                   |
| `hsv_s`           | `tune.uniform(0.0, 0.9)`   | Saturation augmentation range            |
| `hsv_v`           | `tune.uniform(0.0, 0.9)`   | Value (brightness) augmentation range    |
| `degrees`         | `tune.uniform(0.0, 45.0)`  | Rotation augmentation range (degrees)    |
| `translate`       | `tune.uniform(0.0, 0.9)`   | Translation augmentation range           |
| `scale`           | `tune.uniform(0.0, 0.9)`   | Scaling augmentation range               |
| `shear`           | `tune.uniform(0.0, 10.0)`  | Shear augmentation range (degrees)       |
| `perspective`     | `tune.uniform(0.0, 0.001)` | Perspective augmentation range           |
| `flipud`          | `tune.uniform(0.0, 1.0)`   | Vertical flip augmentation probability   |
| `fliplr`          | `tune.uniform(0.0, 1.0)`   | Horizontal flip augmentation probability |
| `mosaic`          | `tune.uniform(0.0, 1.0)`   | Mosaic augmentation probability          |
| `mixup`           | `tune.uniform(0.0, 1.0)`   | Mixup augmentation probability           |
| `copy_paste`      | `tune.uniform(0.0, 1.0)`   | Copy-paste augmentation probability      |

## Custom Search Space Example

In this example, we demonstrate how to use a custom search space for hyperparameter tuning with Ray Tune and YOLOv8. By providing a custom search space, you can focus the tuning process on specific hyperparameters of interest.

```python
from ultralytics import YOLO

# Define a YOLO model
model = YOLO("yolov8n.pt")

# Run Ray Tune on the model
result_grid = model.tune(data="coco128.yaml",
                         space={"lr0": tune.uniform(1e-5, 1e-1)},
                         epochs=50,
                         use_ray=True)
```

In the code snippet above, we create a YOLO model with the "yolov8n.pt" pretrained weights. Then, we call the `tune()` method, specifying the dataset configuration with "coco128.yaml". We provide a custom search space for the initial learning rate `lr0` using a dictionary with the key "lr0" and the value `tune.uniform(1e-5, 1e-1)`. Finally, we pass additional training arguments, such as the number of epochs directly to the tune method as `epochs=50`.

## Processing Ray Tune Results

After running a hyperparameter tuning experiment with Ray Tune, you might want to perform various analyses on the obtained results. This guide will take you through common workflows for processing and analyzing these results.

### Loading Tune Experiment Results from a Directory

After running the tuning experiment with `tuner.fit()`, you can load the results from a directory. This is useful, especially if you're performing the analysis after the initial training script has exited.

```python
experiment_path = f"{storage_path}/{exp_name}"
print(f"Loading results from {experiment_path}...")

restored_tuner = tune.Tuner.restore(experiment_path, trainable=train_mnist)
result_grid = restored_tuner.get_results()
```

### Basic Experiment-Level Analysis

Get an overview of how trials performed. You can quickly check if there were any errors during the trials.

```python
if result_grid.errors:
    print("One or more trials failed!")
else:
    print("No errors!")
```

### Basic Trial-Level Analysis

Access individual trial hyperparameter configurations and the last reported metrics.

```python
for i, result in enumerate(result_grid):
    print(f"Trial #{i}: Configuration: {result.config}, Last Reported Metrics: {result.metrics}")
```

### Plotting the Entire History of Reported Metrics for a Trial

You can plot the history of reported metrics for each trial to see how the metrics evolved over time.



```python
import matplotlib.pyplot as plt

for result in result_grid:
    plt.plot(result.metrics_dataframe["training_iteration"], result.metrics_dataframe["mean_accuracy"], label=f"Trial {i}")

plt.xlabel('Training Iterations')
plt.ylabel('Mean Accuracy')
plt.legend()
plt.show()
```

# yolo special commands

## yolo help

```sh
> yolo help

    Arguments received: ['yolo', 'help']. Ultralytics 'yolo' commands use the following syntax:

        yolo TASK MODE ARGS

        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose')
                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')
                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.
                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'

    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01
        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01

    2. Predict a YouTube video using a pretrained segmentation model at image size 320:
        yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320

    3. Val a pretrained detection model at batch-size 1 and image size 640:
        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640

    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)
        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128

    5. Run special commands:
        yolo help
        yolo checks
        yolo version
        yolo settings
        yolo copy-cfg
        yolo cfg

    Docs: https://docs.ultralytics.com
    Community: https://community.ultralytics.com
    GitHub: https://github.com/ultralytics/ultralytics
```

## yolo checks

```sh
> yolo checks
Ultralytics YOLOv8.0.195  Python-3.11.4 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11264MiB)
Setup complete  (16 CPUs, 31.9 GB RAM, 152.1/200.0 GB disk)

OS                  Windows-10-10.0.19044-SP0
Environment         Windows
Python              3.11.4
Install             git
RAM                 31.91 GB
CPU                 Intel Core(TM) i7-10700 2.90GHz
CUDA                12.1

matplotlib           3.8.0>=3.3.0
numpy                1.26.0>=1.22.2
opencv-python        4.8.1.78>=4.6.0
pillow               10.0.1>=7.1.2
pyyaml               6.0.1>=5.3.1
requests             2.31.0>=2.23.0
scipy                1.10.1>=1.4.1
torch                2.1.0+cu121>=1.8.0
torchvision          0.16.0+cu121>=0.9.0
tqdm                 4.66.1>=4.64.0
pandas               2.1.1>=1.1.4
seaborn              0.13.0>=0.11.0
psutil               5.9.5
py-cpuinfo           9.0.0
thop                 0.1.1-2209072238>=0.1.1
```

## yolo version

```sh
> yolo version
8.0.195
```

## yolo settings

```sh
> yolo settings
 Learn about settings at https://docs.ultralytics.com/quickstart/#ultralytics-settings
Printing 'C:\Users\Administrator\AppData\Roaming\Ultralytics\settings.yaml'

settings_version: 0.0.4
datasets_dir: D:\ml\code\datasets
weights_dir: d:\ml\code\yolov8-ultralytics\weights
runs_dir: d:\ml\code\yolov8-ultralytics\runs
uuid: 062fa24c9a04873db7e870e2df7f4297a2745f5a740d9e7bd868b5884cf0b91a
sync: true
api_key: ''
clearml: true
comet: true
dvc: true
hub: true
mlflow: true
neptune: true
raytune: true
tensorboard: true
wandb: true
```

## yolo copy-cfg

```sh
> yolo copy-cfg
D:\ml\code\yolov8-ultralytics\ultralytics\cfg\default.yaml copied to D:\ml\code\yolov8-ultralytics\default_copy.yaml
Example YOLO command with this new custom cfg:
    yolo cfg='D:\ml\code\yolov8-ultralytics\default_copy.yaml' imgsz=320 batch=8
```

## yolo cfg

```sh
> yolo cfg
Printing 'D:\ml\code\yolov8-ultralytics\ultralytics\cfg\default.yaml'

task: detect
mode: train
model: null
data: null
epochs: 100
patience: 50
batch: 16
imgsz: 640
save: true
save_period: -1
cache: false
device: null
workers: 8
project: null
name: null
exist_ok: false
pretrained: true
optimizer: auto
verbose: true
seed: 0
deterministic: true
single_cls: false
rect: false
cos_lr: false
close_mosaic: 10
resume: false
amp: true
fraction: 1.0
profile: false
freeze: None
overlap_mask: true
mask_ratio: 4
dropout: 0.0
val: true
split: val
save_json: false
save_hybrid: false
conf: null
iou: 0.7
max_det: 300
half: false
dnn: false
plots: true
source: null
show: false
save_txt: false
save_conf: false
save_crop: false
show_labels: true
show_conf: true
vid_stride: 1
stream_buffer: false
line_width: null
visualize: false
augment: false
agnostic_nms: false
classes: null
retina_masks: false
boxes: true
format: torchscript
keras: false
optimize: false
int8: false
dynamic: false
simplify: false
opset: null
workspace: 4
nms: false
lr0: 0.01
lrf: 0.01
momentum: 0.937
weight_decay: 0.0005
warmup_epochs: 3.0
warmup_momentum: 0.8
warmup_bias_lr: 0.1
box: 7.5
cls: 0.5
dfl: 1.5
pose: 12.0
kobj: 1.0
label_smoothing: 0.0
nbs: 64
hsv_h: 0.015
hsv_s: 0.7
hsv_v: 0.4
degrees: 0.0
translate: 0.1
scale: 0.5
shear: 0.0
perspective: 0.0
flipud: 0.0
fliplr: 0.5
mosaic: 1.0
mixup: 0.0
copy_paste: 0.0
cfg: null
tracker: botsort.yaml
```

