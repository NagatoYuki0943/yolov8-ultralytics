

# æœ¬åœ°å®‰è£…

```sh
git clone https://github.com/ultralytics/ultralytics
cd ultralytics
pip install -v -e .
# "-v" æŒ‡è¯¦ç»†è¯´æ˜ï¼Œæˆ–æ›´å¤šçš„è¾“å‡º
# "-e" è¡¨ç¤ºåœ¨å¯ç¼–è¾‘æ¨¡å¼ä¸‹å®‰è£…é¡¹ç›®ï¼Œå› æ­¤å¯¹ä»£ç æ‰€åšçš„ä»»ä½•æœ¬åœ°ä¿®æ”¹éƒ½ä¼šç”Ÿæ•ˆï¼Œä»è€Œæ— éœ€é‡æ–°å®‰è£…ã€‚
```

# [CLI](https://docs.ultralytics.com/usage/cli/)

The YOLO Command Line Interface (CLI) is the easiest way to get started training, validating, predicting and exporting YOLOv8 models.

The `yolo` command is used for all actions:

```sh
yolo TASK MODE ARGS

Where   TASK (optional) is one of [detect, segment, classify]
        MODE (required) is one of [train, val, predict, export, track]
        ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.
```

Where:

- `TASK` (optional) is one of `[detect, segment, classify]`. If it is not passed explicitly YOLOv8 will try to guess the `TASK` from the model type.
- `MODE` (required) is one of `[train, val, predict, export, track]`
- `ARGS` (optional) are any number of custom `arg=value` pairs like `imgsz=320` that override defaults. For a full list of available `ARGS` see the [Configuration](https://docs.ultralytics.com/usage/cfg/) page and `defaults.yaml` GitHub [source](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/default.yaml).

Arguments must be passed as `arg=val` pairs, split by an equals `=` sign and delimited by spaces between pairs. Do not use `--` argument prefixes or commas `,` between arguments.

- `yolo predict model=yolov8n.pt imgsz=640 conf=0.25`  âœ…
- `yolo predict model yolov8n.pt imgsz 640 conf 0.25`  âŒ
- `yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25`  âŒ

## cli æ–‡ä»¶

> `ultralytics/cfg/__init__.py`

```python
# Ultralytics YOLO ğŸš€, AGPL-3.0 license

import contextlib
import re
import shutil
import sys
from difflib import get_close_matches
from pathlib import Path
from types import SimpleNamespace
from typing import Dict, List, Union

from ultralytics.utils import (DEFAULT_CFG, DEFAULT_CFG_DICT, DEFAULT_CFG_PATH, LOGGER, ROOT, SETTINGS, SETTINGS_YAML,
                               IterableSimpleNamespace, __version__, checks, colorstr, deprecation_warn, yaml_load,
                               yaml_print)

# Define valid tasks and modes
MODES = 'train', 'val', 'predict', 'export', 'track', 'benchmark'
TASKS = 'detect', 'segment', 'classify', 'pose'
TASK2DATA = {'detect': 'coco8.yaml', 'segment': 'coco8-seg.yaml', 'classify': 'imagenet100', 'pose': 'coco8-pose.yaml'}
TASK2MODEL = {
    'detect': 'yolov8n.pt',
    'segment': 'yolov8n-seg.pt',
    'classify': 'yolov8n-cls.pt',
    'pose': 'yolov8n-pose.pt'}
TASK2METRIC = {
    'detect': 'metrics/mAP50-95(B)',
    'segment': 'metrics/mAP50-95(M)',
    'classify': 'metrics/accuracy_top1',
    'pose': 'metrics/mAP50-95(P)'}

CLI_HELP_MSG = \
    f"""
    Arguments received: {str(['yolo'] + sys.argv[1:])}. Ultralytics 'yolo' commands use the following syntax:

        yolo TASK MODE ARGS

        Where   TASK (optional) is one of {TASKS}
                MODE (required) is one of {MODES}
                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.
                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'

    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01
        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01

    2. Predict a YouTube video using a pretrained segmentation model at image size 320:
        yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320

    3. Val a pretrained detection model at batch-size 1 and image size 640:
        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640

    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)
        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128

    5. Run special commands:
        yolo help
        yolo checks
        yolo version
        yolo settings
        yolo copy-cfg
        yolo cfg

    Docs: https://docs.ultralytics.com
    Community: https://community.ultralytics.com
    GitHub: https://github.com/ultralytics/ultralytics
    """

# Define keys for arg type checks
CFG_FLOAT_KEYS = 'warmup_epochs', 'box', 'cls', 'dfl', 'degrees', 'shear'
CFG_FRACTION_KEYS = ('dropout', 'iou', 'lr0', 'lrf', 'momentum', 'weight_decay', 'warmup_momentum', 'warmup_bias_lr',
                     'label_smoothing', 'hsv_h', 'hsv_s', 'hsv_v', 'translate', 'scale', 'perspective', 'flipud',
                     'fliplr', 'mosaic', 'mixup', 'copy_paste', 'conf', 'iou', 'fraction')  # fraction floats 0.0 - 1.0
CFG_INT_KEYS = ('epochs', 'patience', 'batch', 'workers', 'seed', 'close_mosaic', 'mask_ratio', 'max_det', 'vid_stride',
                'line_width', 'workspace', 'nbs', 'save_period')
CFG_BOOL_KEYS = ('save', 'exist_ok', 'verbose', 'deterministic', 'single_cls', 'rect', 'cos_lr', 'overlap_mask', 'val',
                 'save_json', 'save_hybrid', 'half', 'dnn', 'plots', 'show', 'save_txt', 'save_conf', 'save_crop',
                 'show_labels', 'show_conf', 'visualize', 'augment', 'agnostic_nms', 'retina_masks', 'boxes', 'keras',
                 'optimize', 'int8', 'dynamic', 'simplify', 'nms', 'profile')


def cfg2dict(cfg):
    """
    Convert a configuration object to a dictionary, whether it is a file path, a string, or a SimpleNamespace object.

    Args:
        cfg (str | Path | SimpleNamespace): Configuration object to be converted to a dictionary.

    Returns:
        cfg (dict): Configuration object in dictionary format.
    """
    if isinstance(cfg, (str, Path)):
        cfg = yaml_load(cfg)  # load dict
    elif isinstance(cfg, SimpleNamespace):
        cfg = vars(cfg)  # convert to dict
    return cfg


def get_cfg(cfg: Union[str, Path, Dict, SimpleNamespace] = DEFAULT_CFG_DICT, overrides: Dict = None):
    """
    Load and merge configuration data from a file or dictionary.

    Args:
        cfg (str | Path | Dict | SimpleNamespace): Configuration data.
        overrides (str | Dict | optional): Overrides in the form of a file name or a dictionary. Default is None.

    Returns:
        (SimpleNamespace): Training arguments namespace.
    """
    cfg = cfg2dict(cfg)

    # Merge overrides
    if overrides:
        overrides = cfg2dict(overrides)
        check_dict_alignment(cfg, overrides)
        cfg = {**cfg, **overrides}  # merge cfg and overrides dicts (prefer overrides)

    # Special handling for numeric project/name
    for k in 'project', 'name':
        if k in cfg and isinstance(cfg[k], (int, float)):
            cfg[k] = str(cfg[k])
    if cfg.get('name') == 'model':  # assign model to 'name' arg
        cfg['name'] = cfg.get('model', '').split('.')[0]
        LOGGER.warning(f"WARNING âš ï¸ 'name=model' automatically updated to 'name={cfg['name']}'.")

    # Type and Value checks
    for k, v in cfg.items():
        if v is not None:  # None values may be from optional args
            if k in CFG_FLOAT_KEYS and not isinstance(v, (int, float)):
                raise TypeError(f"'{k}={v}' is of invalid type {type(v).__name__}. "
                                f"Valid '{k}' types are int (i.e. '{k}=0') or float (i.e. '{k}=0.5')")
            elif k in CFG_FRACTION_KEYS:
                if not isinstance(v, (int, float)):
                    raise TypeError(f"'{k}={v}' is of invalid type {type(v).__name__}. "
                                    f"Valid '{k}' types are int (i.e. '{k}=0') or float (i.e. '{k}=0.5')")
                if not (0.0 <= v <= 1.0):
                    raise ValueError(f"'{k}={v}' is an invalid value. "
                                     f"Valid '{k}' values are between 0.0 and 1.0.")
            elif k in CFG_INT_KEYS and not isinstance(v, int):
                raise TypeError(f"'{k}={v}' is of invalid type {type(v).__name__}. "
                                f"'{k}' must be an int (i.e. '{k}=8')")
            elif k in CFG_BOOL_KEYS and not isinstance(v, bool):
                raise TypeError(f"'{k}={v}' is of invalid type {type(v).__name__}. "
                                f"'{k}' must be a bool (i.e. '{k}=True' or '{k}=False')")

    # Return instance
    return IterableSimpleNamespace(**cfg)


def _handle_deprecation(custom):
    """Hardcoded function to handle deprecated config keys"""

    for key in custom.copy().keys():
        if key == 'hide_labels':
            deprecation_warn(key, 'show_labels')
            custom['show_labels'] = custom.pop('hide_labels') == 'False'
        if key == 'hide_conf':
            deprecation_warn(key, 'show_conf')
            custom['show_conf'] = custom.pop('hide_conf') == 'False'
        if key == 'line_thickness':
            deprecation_warn(key, 'line_width')
            custom['line_width'] = custom.pop('line_thickness')

    return custom


def check_dict_alignment(base: Dict, custom: Dict, e=None):
    """
    This function checks for any mismatched keys between a custom configuration list and a base configuration list.
    If any mismatched keys are found, the function prints out similar keys from the base list and exits the program.

    Args:
        custom (dict): a dictionary of custom configuration options
        base (dict): a dictionary of base configuration options
    """
    custom = _handle_deprecation(custom)
    base_keys, custom_keys = (set(x.keys()) for x in (base, custom))
    mismatched = [k for k in custom_keys if k not in base_keys]
    if mismatched:
        string = ''
        for x in mismatched:
            matches = get_close_matches(x, base_keys)  # key list
            matches = [f'{k}={base[k]}' if base.get(k) is not None else k for k in matches]
            match_str = f'Similar arguments are i.e. {matches}.' if matches else ''
            string += f"'{colorstr('red', 'bold', x)}' is not a valid YOLO argument. {match_str}\n"
        raise SyntaxError(string + CLI_HELP_MSG) from e


def merge_equals_args(args: List[str]) -> List[str]:
    """
    Merges arguments around isolated '=' args in a list of strings.
    The function considers cases where the first argument ends with '=' or the second starts with '=',
    as well as when the middle one is an equals sign.

    Args:
        args (List[str]): A list of strings where each element is an argument.

    Returns:
        List[str]: A list of strings where the arguments around isolated '=' are merged.
    """
    new_args = []
    for i, arg in enumerate(args):
        if arg == '=' and 0 < i < len(args) - 1:  # merge ['arg', '=', 'val']
            new_args[-1] += f'={args[i + 1]}'
            del args[i + 1]
        elif arg.endswith('=') and i < len(args) - 1 and '=' not in args[i + 1]:  # merge ['arg=', 'val']
            new_args.append(f'{arg}{args[i + 1]}')
            del args[i + 1]
        elif arg.startswith('=') and i > 0:  # merge ['arg', '=val']
            new_args[-1] += arg
        else:
            new_args.append(arg)
    return new_args


def handle_yolo_hub(args: List[str]) -> None:
    """
    Handle Ultralytics HUB command-line interface (CLI) commands.

    This function processes Ultralytics HUB CLI commands such as login and logout.
    It should be called when executing a script with arguments related to HUB authentication.

    Args:
        args (List[str]): A list of command line arguments

    Example:
        ```bash
        python my_script.py hub login your_api_key
```
## Overriding default config file

You can override the `default.yaml` config file entirely by passing a new file with the `cfg` arguments, i.e. `cfg=custom.yaml`.

To do this first create a copy of `default.yaml` in your current working dir with the `yolo copy-cfg` command.

This will create `default_copy.yaml`, which you can then pass as `cfg=default_copy.yaml` along with any additional args, like `imgsz=320` in this example:

```sh
yolo copy-cfg
yolo cfg=default_copy.yaml imgsz=320
```

## é»˜è®¤é…ç½®æ–‡ä»¶

> `ultralytics/cfg/default.yaml`

```yaml
# Ultralytics YOLO ğŸš€, AGPL-3.0 license
# Default training settings and hyperparameters for medium-augmentation COCO training

task: detect  # (str) YOLO task, i.e. detect, segment, classify, pose
mode: train  # (str) YOLO mode, i.e. train, val, predict, export, track, benchmark

# Train settings -------------------------------------------------------------------------------------------------------
model:  # (str, optional) path to model file, i.e. yolov8n.pt, yolov8n.yaml
data:  # (str, optional) path to data file, i.e. coco128.yaml
epochs: 100  # (int) number of epochs to train for
patience: 50  # (int) epochs to wait for no observable improvement for early stopping of training
batch: 16  # (int) number of images per batch (-1 for AutoBatch)
imgsz: 640  # (int | list) input images size as int for train and val modes, or list[w,h] for predict and export modes
save: True  # (bool) save train checkpoints and predict results
save_period: -1 # (int) Save checkpoint every x epochs (disabled if < 1)
cache: False  # (bool) True/ram, disk or False. Use cache for data loading
device:  # (int | str | list, optional) device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu
workers: 8  # (int) number of worker threads for data loading (per RANK if DDP)
project:  # (str, optional) project name
name:  # (str, optional) experiment name, results saved to 'project/name' directory
exist_ok: False  # (bool) whether to overwrite existing experiment
pretrained: True  # (bool | str) whether to use a pretrained model (bool) or a model to load weights from (str)
optimizer: auto  # (str) optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]
verbose: True  # (bool) whether to print verbose output
seed: 0  # (int) random seed for reproducibility
deterministic: True  # (bool) whether to enable deterministic mode
single_cls: False  # (bool) train multi-class data as single-class
rect: False  # (bool) rectangular training if mode='train' or rectangular validation if mode='val'
cos_lr: False  # (bool) use cosine learning rate scheduler
close_mosaic: 10  # (int) disable mosaic augmentation for final epochs (0 to disable)
resume: False  # (bool) resume training from last checkpoint
amp: True  # (bool) Automatic Mixed Precision (AMP) training, choices=[True, False], True runs AMP check
fraction: 1.0  # (float) dataset fraction to train on (default is 1.0, all images in train set)
profile: False  # (bool) profile ONNX and TensorRT speeds during training for loggers
freeze: None  # (int | list, optional) freeze first n layers, or freeze list of layer indices during training
# Segmentation
overlap_mask: True  # (bool) masks should overlap during training (segment train only)
mask_ratio: 4  # (int) mask downsample ratio (segment train only)
# Classification
dropout: 0.0  # (float) use dropout regularization (classify train only)

# Val/Test settings ----------------------------------------------------------------------------------------------------
val: True  # (bool) validate/test during training
split: val  # (str) dataset split to use for validation, i.e. 'val', 'test' or 'train'
save_json: False  # (bool) save results to JSON file
save_hybrid: False  # (bool) save hybrid version of labels (labels + additional predictions)
conf:  # (float, optional) object confidence threshold for detection (default 0.25 predict, 0.001 val)
iou: 0.7  # (float) intersection over union (IoU) threshold for NMS
max_det: 300  # (int) maximum number of detections per image
half: False  # (bool) use half precision (FP16)
dnn: False  # (bool) use OpenCV DNN for ONNX inference
plots: True  # (bool) save plots during train/val

# Prediction settings --------------------------------------------------------------------------------------------------
source:  # (str, optional) source directory for images or videos
show: False  # (bool) show results if possible
save_txt: False  # (bool) save results as .txt file
save_conf: False  # (bool) save results with confidence scores
save_crop: False  # (bool) save cropped images with results
show_labels: True  # (bool) show object labels in plots
show_conf: True  # (bool) show object confidence scores in plots
vid_stride: 1  # (int) video frame-rate stride
stream_buffer: False  # (bool) buffer all streaming frames (True) or return the most recent frame (False)
line_width:   # (int, optional) line width of the bounding boxes, auto if missing
visualize: False  # (bool) visualize model features
augment: False  # (bool) apply image augmentation to prediction sources
agnostic_nms: False  # (bool) class-agnostic NMS
classes:  # (int | list[int], optional) filter results by class, i.e. classes=0, or classes=[0,2,3]
retina_masks: False  # (bool) use high-resolution segmentation masks
boxes: True  # (bool) Show boxes in segmentation predictions

# Export settings ------------------------------------------------------------------------------------------------------
format: torchscript  # (str) format to export to, choices at https://docs.ultralytics.com/modes/export/#export-formats
keras: False  # (bool) use Kera=s
optimize: False  # (bool) TorchScript: optimize for mobile
int8: False  # (bool) CoreML/TF INT8 quantization
dynamic: False  # (bool) ONNX/TF/TensorRT: dynamic axes
simplify: False  # (bool) ONNX: simplify model
opset:  # (int, optional) ONNX: opset version
workspace: 4  # (int) TensorRT: workspace size (GB)
nms: False  # (bool) CoreML: add NMS

# Hyperparameters ------------------------------------------------------------------------------------------------------
lr0: 0.01  # (float) initial learning rate (i.e. SGD=1E-2, Adam=1E-3)
lrf: 0.01  # (float) final learning rate (lr0 * lrf)
momentum: 0.937  # (float) SGD momentum/Adam beta1
weight_decay: 0.0005  # (float) optimizer weight decay 5e-4
warmup_epochs: 3.0  # (float) warmup epochs (fractions ok)
warmup_momentum: 0.8  # (float) warmup initial momentum
warmup_bias_lr: 0.1  # (float) warmup initial bias lr
box: 7.5  # (float) box loss gain
cls: 0.5  # (float) cls loss gain (scale with pixels)
dfl: 1.5  # (float) dfl loss gain
pose: 12.0  # (float) pose loss gain
kobj: 1.0  # (float) keypoint obj loss gain
label_smoothing: 0.0  # (float) label smoothing (fraction)
nbs: 64  # (int) nominal batch size
hsv_h: 0.015  # (float) image HSV-Hue augmentation (fraction)
hsv_s: 0.7  # (float) image HSV-Saturation augmentation (fraction)
hsv_v: 0.4  # (float) image HSV-Value augmentation (fraction)
degrees: 0.0  # (float) image rotation (+/- deg)
translate: 0.1  # (float) image translation (+/- fraction)
scale: 0.5  # (float) image scale (+/- gain)
shear: 0.0  # (float) image shear (+/- deg)
perspective: 0.0  # (float) image perspective (+/- fraction), range 0-0.001
flipud: 0.0  # (float) image flip up-down (probability)
fliplr: 0.5  # (float) image flip left-right (probability)
mosaic: 1.0  # (float) image mosaic (probability)
mixup: 0.0  # (float) image mixup (probability)
copy_paste: 0.0  # (float) segment copy-paste (probability)

# Custom config.yaml ---------------------------------------------------------------------------------------------------
cfg:  # (str, optional) for overriding defaults.yaml

# Tracker settings ------------------------------------------------------------------------------------------------------
tracker: botsort.yaml  # (str) tracker type, choices=[botsort.yaml, bytetrack.yaml]
```

# [Configuration](https://docs.ultralytics.com/usage/cfg/)

YOLO settings and hyperparameters play a critical role in the model's performance, speed, and accuracy. These settings and hyperparameters can affect the model's behavior at various stages of the model development process, including training, validation, and prediction.

YOLOv8 'yolo' CLI commands use the following syntax:

```sh
yolo TASK MODE ARGS
yolo task=detect    mode=train    model=yolov8n.pt        args...
          classify       predict        yolov8n-cls.yaml  args...
          segment        val            yolov8n-seg.yaml  args...
                         export         yolov8n.pt        format=onnx  args...

# example    åé¢å¿…é¡»ä½¿ç”¨ =
yolo predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'
yolo val detect data=coco.yaml device=0
yolo val detect data=coco128.yaml batch=1 device=0|cpu
```

Where:

- `TASK` (optional) is one of `[detect, segment, classify, pose]`. If it is not passed explicitly YOLOv8 will try to guess the `TASK` from the model type.
- `MODE` (required) is one of `[train, val, predict, export, track, benchmark]`
- `ARGS` (optional) are any number of custom `arg=value` pairs like `imgsz=320` that override defaults. For a full list of available `ARGS` see the [Configuration](https://docs.ultralytics.com/usage/cfg/) page and `defaults.yaml` GitHub [source](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/default.yaml).

YOLO models can be used for a variety of tasks, including detection, segmentation, classification and pose. These tasks differ in the type of output they produce and the specific problem they are designed to solve.

- **Detect**: For identifying and localizing objects or regions of interest in an image or video.
- **Segment**: For dividing an image or video into regions or pixels that correspond to different objects or classes.
- **Classify**: For predicting the class label of an input image.
- **Pose**: For identifying objects and estimating their keypoints in an image or video.

| Key    | Value      | Description                                     |
| :----- | :--------- | :---------------------------------------------- |
| `task` | `'detect'` | YOLO task, i.e. detect, segment, classify, pose |

## Modes

YOLO models can be used in different modes depending on the specific problem you are trying to solve. These modes include:

- **Train**: For training a YOLOv8 model on a custom dataset.

- **Val**: For validating a YOLOv8 model after it has been trained.

- **Predict**: For making predictions using a trained YOLOv8 model on new images or videos.

- **Export**: For exporting a YOLOv8 model to a format that can be used for deployment.

- **Track**: For tracking objects in real-time using a YOLOv8 model.

- **Benchmark**: For benchmarking YOLOv8 exports (ONNX, TensorRT, etc.) speed and accuracy.

| Key    | Value     | Description                                                  |
| :----- | :-------- | :----------------------------------------------------------- |
| `mode` | `'train'` | YOLO mode, i.e. train, val, predict, export, track, benchmark |

# æ•°æ®é›†

> å…ˆè¦æŠŠæ•°æ®é›†æ”¾å…¥datasetä¸­ï¼Œä¿®æ”¹data/ç›®å½•ä¸‹çš„yamlï¼Œè°ƒæ•´ä¸ºè‡ªå·±çš„æ•°æ®é›†ï¼Œéœ€è¦è°ƒæ•´è·¯å¾„ï¼Œåˆ†ç±»æ•°ï¼Œæ ‡ç­¾å

> yoloæ•°æ®é›†æ ¼å¼(yolov5çš„coco128å’Œéœ¹é›³å§å•¦Wzçš„yolo3ä¸ºä¾‹)
>
> txtå†…å®¹ï¼Œæ¯ä¸€è¡Œéƒ½æ˜¯ `3 0.933536 0.486124 0.030408 0.154487`
>
> æ˜¯ label ä¸­å¿ƒæ¨ªåæ ‡ä¸å›¾åƒå®½åº¦æ¯”å€¼ ä¸­å¿ƒçºµåæ ‡ä¸å›¾åƒé«˜åº¦æ¯”å€¼ bboxå®½åº¦ä¸å›¾åƒå®½åº¦æ¯”å€¼ bboxé«˜åº¦ä¸å›¾åƒå®½é«˜æ¯”å€¼

```
#-------------------------------------------#
# 	yolov5çš„coco128æ ¼å¼
# 	éœ€è¦åœ¨~data/coco128.yamlä¸­ä¿®æ”¹å¦‚ä¸‹ä¿¡æ¯
# 	nc: 10  # åˆ†ç±»æ•°è¦å’Œdatasetä¸­ä¸€è‡´
# 	names: ["aeroplane", "bicycle", "bird", "boat", "bottle": 5] # åˆ†ç±»åç§°
#-------------------------------------------#
datasets
â”œâ”€â”€ coco128
	â”œâ”€â”€ images
	â”‚	â”œâ”€â”€ train2017	è®­ç»ƒå›¾ç‰‡
	â”‚	â””â”€â”€ val2017		éªŒè¯å›¾ç‰‡
	â””â”€â”€ labels
    	â”œâ”€â”€ train2017	è®­ç»ƒæ ‡ç­¾txt
    	â””â”€â”€ val2017		éªŒè¯æ ‡ç­¾txt


#-------------------------------------------#
#	éœ¹é›³å§å•¦Wzçš„yolo3
#-------------------------------------------#
data
â”œâ”€â”€ pascal_voc_classes.json		å­˜æ”¾ç±»åˆ«ä¿¡æ¯ {"aeroplane": 1, "bicycle": 2, "bird": 3, "boat": 4, "bottle": 5}
â”œâ”€â”€ train
â”‚	â”œâ”€â”€ images		è®­ç»ƒå›¾ç‰‡
â”‚	â””â”€â”€ labels		è®­ç»ƒæ ‡ç­¾txt
â””â”€â”€ val
	â”œâ”€â”€ images		éªŒè¯å›¾ç‰‡
	â””â”€â”€ labels		éªŒè¯å›¾ç‰‡txt
```

> `ultralytics/datasets/class20.yaml`

```yaml
# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
# COCO128 dataset https://www.kaggle.com/ultralytics/coco128 (first 128 images from COCO train2017) by Ultralytics
# Example usage: python train.py --data coco128.yaml
# parent
# â”œâ”€â”€ ultralytics
# |   â””â”€â”€ ultralytics
# â””â”€â”€ datasets
#     â””â”€â”€ yourname
#         â””â”€â”€ images/
#             â””â”€â”€ train2017/  å­˜æ”¾è®­ç»ƒå›¾ç‰‡
#             â””â”€â”€ val2017/    å­˜æ”¾éªŒè¯å›¾ç‰‡
#         â””â”€â”€ labels/
#             â””â”€â”€ train2017/  å­˜æ”¾è®­ç»ƒæ ‡ç­¾  class x_center y_center width height
#             â””â”€â”€ val2017/    å­˜æ”¾éªŒè¯æ ‡ç­¾


# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: ../datasets/classes20  # dataset root dir
train: images/train2017  # train images (relative to 'path') 128 images
val: images/val2017  # val images (relative to 'path') 128 images
test:  # test images (optional)

# Classes
names:
  0: aeroplane
  1: bicycle
  2: bird
  3: boat
  4: bottle
  5: bus
  6: car
  7: cat
  8: chair
  9: cow
  10: diningtable
  11: dog
  12: horse
  13: motorbike
  14: person
  15: pottedplant
  16: sheep
  17: sofa
  18: train
  19: tvmonitor
```

# ä¸‹è½½æƒé‡

> å°†ä¸‹è½½å¥½çš„æƒé‡æ”¾åˆ°`weights/`æ–‡ä»¶ä¸‹ä¸‹

## æ¨¡å‹

æ‰€æœ‰çš„ YOLOv8 é¢„è®­ç»ƒæ¨¡å‹éƒ½å¯ä»¥åœ¨æ­¤æ‰¾åˆ°ã€‚æ£€æµ‹ã€åˆ†å‰²å’Œå§¿æ€æ¨¡å‹åœ¨ [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/coco.yaml) æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè€Œåˆ†ç±»æ¨¡å‹åœ¨ [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/ImageNet.yaml) æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚

åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶ï¼Œ[æ¨¡å‹](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models) ä¼šè‡ªåŠ¨ä»æœ€æ–°çš„ Ultralytics [å‘å¸ƒç‰ˆæœ¬](https://github.com/ultralytics/assets/releases)ä¸­ä¸‹è½½ã€‚

| æ¨¡å‹                                                         | å°ºå¯¸ ï¼ˆåƒç´ ï¼‰ | mAPval 50-95 | æ¨ç†é€Ÿåº¦ CPU ONNX (ms) | æ¨ç†é€Ÿåº¦ A100 TensorRT (ms) | å‚æ•°é‡ (M) | FLOPs (B) |
| ------------------------------------------------------------ | ------------- | ------------ | ---------------------- | --------------------------- | ---------- | --------- |
| [YOLOv8n](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt) | 640           | 37.3         | 80.4                   | 0.99                        | 3.2        | 8.7       |
| [yolov8n](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt) | 640           | 44.9         | 128.4                  | 1.20                        | 11.2       | 28.6      |
| [YOLOv8m](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt) | 640           | 50.2         | 234.7                  | 1.83                        | 25.9       | 78.9      |
| [YOLOv8l](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt) | 640           | 52.9         | 375.2                  | 2.39                        | 43.7       | 165.2     |
| [YOLOv8x](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt) | 640           | 53.9         | 479.1                  | 3.53                        | 68.2       | 257.8     |

- **mAPval** ç»“æœéƒ½åœ¨ [COCO val2017](http://cocodataset.org/) æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨å•æ¨¡å‹å•å°ºåº¦æµ‹è¯•å¾—åˆ°ã€‚
  å¤ç°å‘½ä»¤ `yolo val detect data=coco.yaml device=0`
- **æ¨ç†é€Ÿåº¦**ä½¿ç”¨ COCO éªŒè¯é›†å›¾ç‰‡æ¨ç†æ—¶é—´è¿›è¡Œå¹³å‡å¾—åˆ°ï¼Œæµ‹è¯•ç¯å¢ƒä½¿ç”¨ [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) å®ä¾‹ã€‚
  å¤ç°å‘½ä»¤ `yolo val detect data=coco128.yaml batch=1 device=0|cpu`

# [Train](https://docs.ultralytics.com/modes/train/)

**Train mode** is used for training a YOLOv8 model on a custom dataset. In this mode, the model is trained using the specified dataset and hyperparameters. The training process involves optimizing the model's parameters so that it can accurately predict the classes and locations of objects in an image.

- YOLOv8 datasets like COCO, VOC, ImageNet and many others automatically download on first use, i.e. `yolo train data=coco.yaml`

## Python

### Single-GPU and CPU Training Example

Device is determined automatically. If a GPU is available then it will be used, otherwise training will start on CPU.

```python
from ultralytics import YOLO

# Load a model
model = YOLO('yolov8n.yaml')  # build a new model from YAML
model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)
model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights

# Train the model
results = model.train(data='coco128.yaml', epochs=100, imgsz=640)
```

### Multi-GPU Training

The training device can be specified using the `device` argument. If no argument is passed GPU `device=0` will be used if available, otherwise `device=cpu` will be used.

```python
from ultralytics import YOLO

# Load a model
model = YOLO('yolov8n.yaml')  # build a new model from YAML
model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)
model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights

# Train the model with 2 GPUs
results = model.train(data='coco128.yaml', epochs=100, imgsz=640, device=[0, 1])
```

### Resuming Interrupted Trainings

Resuming training from a previously saved state is a crucial feature when working with deep learning models. This can come in handy in various scenarios, like when the training process has been unexpectedly interrupted, or when you wish to continue training a model with new data or for more epochs.

When training is resumed, Ultralytics YOLO loads the weights from the last saved model and also restores the optimizer state, learning rate scheduler, and the epoch number. This allows you to continue the training process seamlessly from where it was left off.

You can easily resume training in Ultralytics YOLO by setting the `resume` argument to `True` when calling the `train` method, and specifying the path to the `.pt` file containing the partially trained model weights.

```python
from ultralytics import YOLO

# Load a model
model = YOLO('path/to/last.pt')  # load a partially trained model

# Resume training
results = model.train(resume=True)
```

By setting `resume=True`, the `train` function will continue training from where it left off, using the state stored in the 'path/to/last.pt' file. If the `resume` argument is omitted or set to `False`, the `train` function will start a new training session.

Remember that checkpoints are saved at the end of every epoch by default, or at fixed interval using the `save_period` argument, so you must complete at least 1 epoch to resume a training run.

## Arguments

Training settings for YOLO models refer to the various hyperparameters and configurations used to train the model on a dataset. These settings can affect the model's performance, speed, and accuracy. Some common YOLO training settings include the batch size, learning rate, momentum, and weight decay. Other factors that may affect the training process include the choice of optimizer, the choice of loss function, and the size and composition of the training dataset. It is important to carefully tune and experiment with these settings to achieve the best possible performance for a given task.

| Key               | Value    | Description                                                  |
| :---------------- | :------- | :----------------------------------------------------------- |
| `model`           | `None`   | path to model file, i.e. yolov8n.pt, yolov8n.yaml            |
| `data`            | `None`   | path to data file, i.e. coco128.yaml                         |
| `epochs`          | `100`    | number of epochs to train for                                |
| `patience`        | `50`     | epochs to wait for no observable improvement for early stopping of training |
| `batch`           | `16`     | number of images per batch (-1 for AutoBatch)                |
| `imgsz`           | `640`    | size of input images as integer or w,h                       |
| `save`            | `True`   | save train checkpoints and predict results                   |
| `save_period`     | `-1`     | Save checkpoint every x epochs (disabled if < 1)             |
| `cache`           | `False`  | True/ram, disk or False. Use cache for data loading          |
| `device`          | `None`   | device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu |
| `workers`         | `8`      | number of worker threads for data loading (per RANK if DDP)  |
| `project`         | `None`   | project name                                                 |
| `name`            | `None`   | experiment name                                              |
| `exist_ok`        | `False`  | whether to overwrite existing experiment                     |
| `pretrained`      | `False`  | whether to use a pretrained model                            |
| `optimizer`       | `'auto'` | optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto] |
| `verbose`         | `False`  | whether to print verbose output                              |
| `seed`            | `0`      | random seed for reproducibility                              |
| `deterministic`   | `True`   | whether to enable deterministic mode                         |
| `single_cls`      | `False`  | train multi-class data as single-class                       |
| `rect`            | `False`  | rectangular training with each batch collated for minimum padding |
| `cos_lr`          | `False`  | use cosine learning rate scheduler                           |
| `close_mosaic`    | `10`     | (int) disable mosaic augmentation for final epochs (0 to disable) |
| `resume`          | `False`  | resume training from last checkpoint                         |
| `amp`             | `True`   | Automatic Mixed Precision (AMP) training, choices=[True, False] |
| `fraction`        | `1.0`    | dataset fraction to train on (default is 1.0, all images in train set) |
| `profile`         | `False`  | profile ONNX and TensorRT speeds during training for loggers |
| `freeze`          | `None`   | (int or list, optional) freeze first n layers, or freeze list of layer indices during training |
| `lr0`             | `0.01`   | initial learning rate (i.e. SGD=1E-2, Adam=1E-3)             |
| `lrf`             | `0.01`   | final learning rate (lr0 * lrf)                              |
| `momentum`        | `0.937`  | SGD momentum/Adam beta1                                      |
| `weight_decay`    | `0.0005` | optimizer weight decay 5e-4                                  |
| `warmup_epochs`   | `3.0`    | warmup epochs (fractions ok)                                 |
| `warmup_momentum` | `0.8`    | warmup initial momentum                                      |
| `warmup_bias_lr`  | `0.1`    | warmup initial bias lr                                       |
| `box`             | `7.5`    | box loss gain                                                |
| `cls`             | `0.5`    | cls loss gain (scale with pixels)                            |
| `dfl`             | `1.5`    | dfl loss gain                                                |
| `pose`            | `12.0`   | pose loss gain (pose-only)                                   |
| `kobj`            | `2.0`    | keypoint obj loss gain (pose-only)                           |
| `label_smoothing` | `0.0`    | label smoothing (fraction)                                   |
| `nbs`             | `64`     | nominal batch size                                           |
| `overlap_mask`    | `True`   | masks should overlap during training (segment train only)    |
| `mask_ratio`      | `4`      | mask downsample ratio (segment train only)                   |
| `dropout`         | `0.0`    | use dropout regularization (classify train only)             |
| `val`             | `True`   | validate/test during training                                |

## example



```sh
# Build a new model from YAML and start training from scratch
yolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640

# Start training from a pretrained *.pt model
yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640

# Build a new model from YAML, transfer pretrained weights to it and start training
yolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640
```

### # Multi-GPU Training

```sh
# Start training from a pretrained *.pt model using GPUs 0 and 1
yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640 device=0,1
```

> `auto optimizer`

```sh
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=ultralytics/cfg/models/v8/yolov8n.yaml pretrained=weights/yolov8n.pt data=ultralytics/datasets/coco128.yaml

#                                                                                                                               			   modelå¯ä»¥ç›´æ¥è®¾ç½®ä¸ºpt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml

#														   rtdetr è®­ç»ƒè½®æ•°æ›´å°‘
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=100 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=weights/rtdetr-x.pt data=ultralytics/cfg/datasets/coco128.yaml
```

> `SGD`

```sh
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 optimizer=SGD lr0=0.01 cos_lr=True device=0 model=ultralytics/cfg/models/v8/yolov8n.yaml pretrained=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml

#                                                                                                                               			   modelå¯ä»¥ç›´æ¥è®¾ç½®ä¸ºpt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 optimizer=SGD lr0=0.01 cos_lr=True device=0 model=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml
```

> `Adam`

```sh
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 optimizer=AdamW lr0=0.001 cos_lr=True device=0 model=ultralytics/cfg/models/v8/yolov8n.yaml pretrained=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml

#                                                                                                                                  				  modelå¯ä»¥ç›´æ¥è®¾ç½®ä¸ºpt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 optimizer=AdamW lr0=0.001 cos_lr=True device=0 model=weights/yolov8n.pt data=ultralytics/cfg/datasets/coco128.yaml
```

> `resume`

```sh
#																													 	 	        model=æœ€åçš„pt
yolo task=detect mode=train imgsz=640 batch=-1 workers=8 epochs=300 patience=0 close_mosaic=10 fraction=1.0 cos_lr=True device=0 model=weights/last.pt data=ultralytics/cfg/datasets/coco128.yaml resume=True exist_ok=True
```



## **ä¸éœ€è¦åœ¨æ¨¡å‹é…ç½®ä¸­æ˜¾ç¤ºæ›´æ”¹ç±»åˆ«æ•°**

> ä¼šè‡ªåŠ¨å°†ncè°ƒæ•´ä¸ºæ•°æ®é›†çš„ç±»åˆ«æ•°é‡

```sh
> yolo task=detect mode=train imgsz=640 batch=-1 epochs=300 optimizer=SGD lr0=0.01 cos_lr=True device=0 pretrained=weights/yolov8n.pt model=ultralytics/models/v8/yolov8n.yaml data=ultralytics/datasets/classes20.yaml

                   from  n    params  module                                       arguments
  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]
  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]
  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]
  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]
  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]
  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]
  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]
  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]
  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]
  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]
 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]
 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]
 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]
 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]
 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]
 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]
 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]
 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]
 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]
 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]
 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.Detect                [80, [64, 128, 256]]
yolov8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs

Transferred 355/355 items from pretrained weights
Ultralytics YOLOv8.0.58  Python-3.10.9 torch-2.0.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11264MiB)
yolo\engine\trainer: task=detect, mode=train, model=ultralytics/models/v8/yolov8n.yaml, data=ultralytics/datasets/classes20.yaml, epochs=300, patience=50, batch=-1, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=None, exist_ok=False, pretrained=weights/yolov8n.pt, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=d:\code\ultralytics\runs\detect\train2
Overriding model.yaml nc=80 with nc=20		# è¿™é‡Œè‡ªåŠ¨è¦†ç›–äº†æ—§çš„ç±»åˆ«æ•°

                   from  n    params  module                                       arguments
  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]
  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]
  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]
  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]
  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]
  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]
  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]
  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]
  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]
  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]
 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]
 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]
 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]
 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]
 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]
 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]
 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]
 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]
 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]
 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]
 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.Detect                [20, [64, 128, 256]]
yolov8n summary: 225 layers, 3014748 parameters, 3014732 gradients, 8.2 GFLOPs

Transferred 319/355 items from pretrained weights
TensorBoard: Start with 'tensorboard --logdir d:\code\ultralytics\runs\detect\train', view at http://localhost:6006/
AMP: running Automatic Mixed Precision (AMP) checks with yolov8n...
AMP: checks passed
AutoBatch: Computing optimal batch size for imgsz=640
AutoBatch: CUDA:0 (NVIDIA GeForce GTX 1080 Ti) 11.00G total, 0.10G reserved, 0.07G allocated, 10.83G free
      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output
     3014748       8.215         0.210         28.59         17.95        (1, 3, 640, 640)                    list
     3014748       16.43         0.296         13.96         21.27        (2, 3, 640, 640)                    list
     3014748       32.86         0.581         12.96         20.99        (4, 3, 640, 640)                    list
     3014748       65.72         1.065         20.27          28.6        (8, 3, 640, 640)                    list
     3014748       131.4         2.334         34.56         48.56       (16, 3, 640, 640)                    list
AutoBatch: Using batch-size 50 for CUDA:0 7.30G/11.00G (66%)
optimizer: SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.000390625), 63 bias
train: Scanning D:\code\datasets\classes20\labels\train.cache... 5266 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
val: Scanning D:\code\datasets\classes20\labels\val.cache... 586 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586
Plotting labels to d:\code\ultralytics\runs\detect\train\labels.jpg...
Image sizes 640 train, 640 val
Using 8 dataloader workers
Logging results to d:\code\ultralytics\runs\detect\train
Starting training for 300 epochs...
```

> è‡ªåŠ¨è°ƒæ•´ `nc` çš„ä»£ç åœ¨ `ultralytics/nn/task.py`

```python
        ch = self.yaml['ch'] = self.yaml.get('ch', ch)  # input channels
        if nc and nc != self.yaml['nc']:    # ä½¿ç”¨data configä¸­çš„namesé•¿åº¦è¦†ç›–æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­çš„ç±»åˆ«
            LOGGER.info(f"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}")
            self.yaml['nc'] = nc  # override yaml value
```

## è®­ç»ƒæ—¶å‡ºç°çš„é—®é¢˜

### è®­ç»ƒ `obj_loss` å¢å¤§ | reduce FPs | è§£å†³ç‰¹æ®Šåœºæ™¯æ¨¡å‹æ‹æ‘„æ—¥å¸¸ç›®æ ‡çš„FPæ•°é‡è¿‡å¤š

> [how to use Background images in training? Â· Issue #2844 Â· ultralytics/yolov5 (github.com)](https://github.com/ultralytics/yolov5/issues/2844)
>
> åœ¨å›¾ç‰‡è®­ç»ƒæ–‡ä»¶å¤¹ `images/train` ä¸­æ·»åŠ èƒŒæ™¯å›¾ç‰‡æ–‡ä»¶ï¼Œæ¯”å¦‚cocoæˆ–è€…vocæ•°æ®é›†çš„ä¸€äº›ç…§ç‰‡
>
> ä¸éœ€è¦æ·»åŠ ç©ºç™½label txtæ–‡ä»¶ï¼Œæ·»åŠ äº†ä¹Ÿä¸ä¼šå‡ºé”™
>
> `(if no objects in image, no `*.txt` file is required).`
>
> [ç›®æ ‡æ£€æµ‹ï¼ˆé™ä½è¯¯æ£€æµ‹ç‡åŠå°ç›®æ ‡æ£€æµ‹ç³»åˆ—ç¬”è®°ï¼‰](https://blog.csdn.net/weixin_44836143/article/details/105952819)

```sh
train: Scanning D:\code\datasets\classes20\labels\train... 5266 images, 1000 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|
train: New cache created: D:\code\datasets\classes20\labels\train.cache
val: Scanning D:\code\datasets\classes20\labels\val... 586 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|
val: New cache created: D:\code\datasets\classes20\labels\val.cache
```

# [Export](https://docs.ultralytics.com/modes/export/)

**Export mode** is used for exporting a YOLOv8 model to a format that can be used for deployment. In this mode, the model is converted to a format that can be used by other software applications or hardware devices. This mode is useful when deploying the model to production environments.

- Export to ONNX or OpenVINO for up to 3x CPU speedup.
- Export to TensorRT for up to 5x GPU speedup.

## Python

Export a YOLOv8n model to a different format like ONNX or TensorRT. See Arguments section below for a full list of export arguments.

```python
from ultralytics import YOLO

# Load a model
model = YOLO('yolov8n.pt')  # load an official model
model = YOLO('path/to/best.pt')  # load a custom trained

# Export the model
model.export(format='onnx')
```

## Arguments

Export settings for YOLO models refer to the various configurations and options used to save or export the model for use in other environments or platforms. These settings can affect the model's performance, size, and compatibility with different systems. Some common YOLO export settings include the format of the exported model file (e.g. ONNX, TensorFlow SavedModel), the device on which the model will be run (e.g. CPU, GPU), and the presence of additional features such as masks or multiple labels per box. Other factors that may affect the export process include the specific task the model is being used for and the requirements or constraints of the target environment or platform. It is important to carefully consider and configure these settings to ensure that the exported model is optimized for the intended use case and can be used effectively in the target environment.

| Key         | Value           | Description                                          |
| :---------- | :-------------- | :--------------------------------------------------- |
| `format`    | `'torchscript'` | format to export to                                  |
| `imgsz`     | `640`           | image size as scalar or (h, w) list, i.e. (640, 480) |
| `keras`     | `False`         | use Keras for TF SavedModel export                   |
| `optimize`  | `False`         | TorchScript: optimize for mobile                     |
| `half`      | `False`         | FP16 quantization                                    |
| `int8`      | `False`         | INT8 quantization                                    |
| `dynamic`   | `False`         | ONNX/TensorRT: dynamic axes                          |
| `simplify`  | `False`         | ONNX/TensorRT: simplify model                        |
| `opset`     | `None`          | ONNX: opset version (optional, defaults to latest)   |
| `workspace` | `4`             | TensorRT: workspace size (GB)                        |
| `nms`       | `False`         | CoreML: add NMS                                      |

## Export Formats

Available YOLOv8 export formats are in the table below. You can export to any format using the `format` argument, i.e. `format='onnx'` or `format='engine'`.

| Format                                                       | `format` Argument | Model                     | Metadata | Arguments                                           |
| :----------------------------------------------------------- | :---------------- | :------------------------ | :------- | :-------------------------------------------------- |
| [PyTorch](https://pytorch.org/)                              | -                 | `yolov8n.pt`              | âœ…        | -                                                   |
| [TorchScript](https://pytorch.org/docs/stable/jit.html)      | `torchscript`     | `yolov8n.torchscript`     | âœ…        | `imgsz`, `optimize`                                 |
| [ONNX](https://onnx.ai/)                                     | `onnx`            | `yolov8n.onnx`            | âœ…        | `imgsz`, `half`, `dynamic`, `simplify`, `opset`     |
| [OpenVINO](https://docs.openvino.ai/latest/index.html)       | `openvino`        | `yolov8n_openvino_model/` | âœ…        | `imgsz`, `half`, `int8`                             |
| [TensorRT](https://developer.nvidia.com/tensorrt)            | `engine`          | `yolov8n.engine`          | âœ…        | `imgsz`, `half`, `dynamic`, `simplify`, `workspace` |
| [CoreML](https://github.com/apple/coremltools)               | `coreml`          | `yolov8n.mlpackage`       | âœ…        | `imgsz`, `half`, `int8`, `nms`                      |
| [TF SavedModel](https://www.tensorflow.org/guide/saved_model) | `saved_model`     | `yolov8n_saved_model/`    | âœ…        | `imgsz`, `keras`                                    |
| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n.pb`              | âŒ        | `imgsz`                                             |
| [TF Lite](https://www.tensorflow.org/lite)                   | `tflite`          | `yolov8n.tflite`          | âœ…        | `imgsz`, `half`, `int8`                             |
| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)   | `edgetpu`         | `yolov8n_edgetpu.tflite`  | âœ…        | `imgsz`                                             |
| [TF.js](https://www.tensorflow.org/js)                       | `tfjs`            | `yolov8n_web_model/`      | âœ…        | `imgsz`                                             |
| [PaddlePaddle](https://github.com/PaddlePaddle)              | `paddle`          | `yolov8n_paddle_model/`   | âœ…        | `imgsz`                                             |
| [ncnn](https://github.com/Tencent/ncnn)                      | `ncnn`            | `yolov8n_ncnn_model/`     | âœ…        | `imgsz`, `half`                                     |

> example

```sh
yolo export model=yolov8n.pt format=onnx  # export official model
yolo export model=path/to/best.pt format=onnx  # export custom trained model
```

## torchscript

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=torchscript device=0
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=torchscript device=cpu optimize=True # optimize not compatible with cuda devices, i.e. use device=cpu
```

## onnx

> æ³¨æ„:
>
> `onnxruntime` å’Œ `onnxruntime-gpu` ä¸è¦åŒæ—¶å®‰è£…ï¼Œå¦åˆ™ä½¿ç”¨ `gpu` æ¨ç†æ—¶é€Ÿåº¦ä¼šå¾ˆæ…¢ï¼Œå¦‚æœåŒæ—¶å®‰è£…äº†2ä¸ªåŒ…ï¼Œè¦å…¨éƒ¨å¸è½½ï¼Œå†å®‰è£…`onnxruntime-gpu` æ‰èƒ½ä½¿ç”¨gpuæ¨ç†ï¼Œå¦åˆ™gpué€Ÿåº¦ä¼šå¾ˆæ…¢

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=0

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=0 half=True                # half=True only compatible with GPU export, i.e. use device=0

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=cpu dynamic=True

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=cpu half=True dynamic=True  # å¯¼å‡ºå¤±è´¥ half=True not compatible with dynamic=True, i.e. use only one.
```

### opencvä½¿ç”¨çš„onnx

> https://github.com/ultralytics/ultralytics/tree/main/examples/YOLOv8-OpenCV-ONNX-Python

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=0 opset=12             # opsetå¿…é¡»ä¸º12

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=onnx simplify=True device=0 half=True opset=12   # opsetå¿…é¡»ä¸º12

# opencvä¸æ”¯æŒdynamic
```

## openvino

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=openvino simplify=True device=cpu # å¯ä»¥ç”¨simplifyçš„onnx

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=openvino simplify=True device=cpu half=True

yolo task =detect mode=export imgsz=640 model=weights/yolov8n.pt format=openvino simplify=True device=cpu int8=True data=ultralytics/cfg/datasets/coco128.yaml # INT8 export requires a data argument for calibration
```

### é€šè¿‡openvinoçš„`mo`å‘½ä»¤å°†onnxè½¬æ¢ä¸ºopenvinoæ ¼å¼(æ”¯æŒ**fp16**)

> https://docs.openvino.ai/latest/notebooks/102-pytorch-onnx-to-openvino-with-output.html

```sh
mo --input_model "onnx_path" --output_dir "output_path" --compress_to_fp16

mo --input_model "onnx_path" --output_dir "output_path" --compress_to_fp16
```

#### ä»£ç æ–¹å¼

```python
from openvino.tools import mo
from openvino.runtime import serialize

onnx_path = "onnx_path"

# fp32 IR model
fp32_path = "fp32_path"
output_path = fp32_path + ".xml"
print(f"Export ONNX to OpenVINO FP32 IR to: {output_path}")
model = mo.convert_model(onnx_path)
serialize(model, output_path)

# fp16 IR model
fp16_path = "fp16_path"
output_path = fp16_path + ".xml"

print(f"Export ONNX to OpenVINO FP16 IR to: {output_path}")
model = mo.convert_model(onnx_path, compress_to_fp16=True)
serialize(model, output_path)
```

### export failure  0.9s: DLL load failed while importing ie_api

> https://blog.csdn.net/qq_26815239/article/details/123047840
>
> å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œå¹¶ä¸”æ˜¯åœ¨Windowsç³»ç»Ÿä¸‹é€šè¿‡pipå®‰è£…çš„openvinoï¼Œé‚£ä¹ˆè¯¥é”™è¯¯çš„è§£å†³æ–¹æ¡ˆå¦‚ä¸‹ï¼š

1. è¿›å…¥ç›®å½• `your\env\site-packages\openvino\inference_engine`
2. æ‰“å¼€æ–‡ä»¶ `__init__.py`
3. 26è¡Œä¸‹æ·»åŠ ä¸€è¡Œ

```python
        if os.path.isdir(lib_path):
            # On Windows, with Python >= 3.8, DLLs are no longer imported from the PATH.
            if (3, 8) <= sys.version_info:
                os.add_dll_directory(os.path.abspath(lib_path))
                os.environ['PATH'] = os.path.abspath(lib_path) + ';' + os.environ['PATH']	# æ·»åŠ è¿™ä¸€è¡Œ
```

## tensorrt

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=engine simplify=True device=0 # å¯ä»¥ç”¨simplifyçš„onnx

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=engine simplify=True device=0 half=True
```

## ncnn

```sh
yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=ncnn simplify=True device=0 # å¯ä»¥ç”¨simplifyçš„onnx

yolo task=detect mode=export imgsz=640 model=weights/yolov8n.pt format=ncnn simplify=True device=0 half=True
```

## onnx openvino tensorrt

> ç›®å‰ä¸æ”¯æŒåŒæ—¶å¯¼å‡ºå¤šç§æ ¼å¼ï¼Œæ¯ç§æ ¼å¼éƒ½è¦å•ç‹¬å¯¼å‡º

# [Predict](https://docs.ultralytics.com/modes/predict/)

The prediction settings for YOLO models encompass a range of hyperparameters and configurations that influence the model's performance, speed, and accuracy during inference on new data. Careful tuning and experimentation with these settings are essential to achieve optimal performance for a specific task. Key settings include the confidence threshold, Non-Maximum Suppression (NMS) threshold, and the number of classes considered. Additional factors affecting the prediction process are input data size and format, the presence of supplementary features such as masks or multiple labels per box, and the particular task the model is employed for.

## Python

YOLOv8 **predict mode** can generate predictions for various tasks, returning either a list of `Results` objects or a memory-efficient generator of `Results` objects when using the streaming mode. Enable streaming mode by passing `stream=True` in the predictor's call method.

> Return a list with `stream=False`

```python
from ultralytics import YOLO

# Load a model
model = YOLO('yolov8n.pt')  # pretrained YOLOv8n model

# Run batched inference on a list of images
results = model(['im1.jpg', 'im2.jpg'])  # return a list of Results objects

# Process results list
for result in results:
    boxes = result.boxes  # Boxes object for bbox outputs
    masks = result.masks  # Masks object for segmentation masks outputs
    keypoints = result.keypoints  # Keypoints object for pose outputs
    probs = result.probs  # Probs object for classification outputs
```

> **Return a generator with** `stream=True`

```python
from ultralytics import YOLO

# Load a model
model = YOLO('yolov8n.pt')  # pretrained YOLOv8n model

# Run batched inference on a list of images
results = model(['im1.jpg', 'im2.jpg'], stream=True)  # return a generator of Results objects

# Process results generator
for result in results:
    boxes = result.boxes  # Boxes object for bbox outputs
    masks = result.masks  # Masks object for segmentation masks outputs
    keypoints = result.keypoints  # Keypoints object for pose outputs
    probs = result.probs  # Probs object for classification outputs
```

## Inference Sources

YOLOv8 can process different types of input sources for inference, as shown in the table below. The sources include static images, video streams, and various data formats. The table also indicates whether each source can be used in streaming mode with the argument `stream=True` âœ…. Streaming mode is beneficial for processing videos or live streams as it creates a generator of results instead of loading all frames into memory.

> Use `stream=True` for processing long videos or large datasets to efficiently manage memory. When `stream=False`, the results for all frames or data points are stored in memory, which can quickly add up and cause out-of-memory errors for large inputs. In contrast, `stream=True` utilizes a generator, which only keeps the results of the current frame or data point in memory, significantly reducing memory consumption and preventing out-of-memory issues.

| Source         | Argument                                   | Type            | Notes                                                        |
| :------------- | :----------------------------------------- | :-------------- | :----------------------------------------------------------- |
| image          | `'image.jpg'`                              | `str` or `Path` | Single image file.                                           |
| URL            | `'https://ultralytics.com/images/bus.jpg'` | `str`           | URL to an image.                                             |
| screenshot     | `'screen'`                                 | `str`           | Capture a screenshot.                                        |
| PIL            | `Image.open('im.jpg')`                     | `PIL.Image`     | HWC format with RGB channels.                                |
| OpenCV         | `cv2.imread('im.jpg')`                     | `np.ndarray`    | HWC format with BGR channels `uint8 (0-255)`.                |
| numpy          | `np.zeros((640,1280,3))`                   | `np.ndarray`    | HWC format with BGR channels `uint8 (0-255)`.                |
| torch          | `torch.zeros(16,3,320,640)`                | `torch.Tensor`  | BCHW format with RGB channels `float32 (0.0-1.0)`.           |
| CSV            | `'sources.csv'`                            | `str` or `Path` | CSV file containing paths to images, videos, or directories. |
| video âœ…        | `'video.mp4'`                              | `str` or `Path` | Video file in formats like MP4, AVI, etc.                    |
| directory âœ…    | `'path/'`                                  | `str` or `Path` | Path to a directory containing images or videos.             |
| glob âœ…         | `'path/*.jpg'`                             | `str`           | Glob pattern to match multiple files. Use the `*` character as a wildcard. |
| YouTube âœ…      | `'https://youtu.be/Zgi9g1ksQHc'`           | `str`           | URL to a YouTube video.                                      |
| stream âœ…       | `'rtsp://example.com/media.mp4'`           | `str`           | URL for streaming protocols such as RTSP, RTMP, or an IP address. |
| multi-stream âœ… | `'list.streams'`                           | `str` or `Path` | `*.streams` text file with one stream URL per row, i.e. 8 streams will run at batch-size 8. |

## Arguments

All supported arguments:

| Name           | Type           | Default                | Description                                                  |
| :------------- | :------------- | :--------------------- | :----------------------------------------------------------- |
| `source`       | `str`          | `'ultralytics/assets'` | source directory for images or videos                        |
| `conf`         | `float`        | `0.25`                 | object confidence threshold for detection                    |
| `iou`          | `float`        | `0.7`                  | intersection over union (IoU) threshold for NMS              |
| `imgsz`        | `int or tuple` | `640`                  | image size as scalar or (h, w) list, i.e. (640, 480)         |
| `half`         | `bool`         | `False`                | use half precision (FP16)                                    |
| `device`       | `None or str`  | `None`                 | device to run on, i.e. cuda device=0/1/2/3 or device=cpu     |
| `show`         | `bool`         | `False`                | show results if possible                                     |
| `save`         | `bool`         | `False`                | save images with results                                     |
| `save_txt`     | `bool`         | `False`                | save results as .txt file                                    |
| `save_conf`    | `bool`         | `False`                | save results with confidence scores                          |
| `save_crop`    | `bool`         | `False`                | save cropped images with results                             |
| `hide_labels`  | `bool`         | `False`                | hide labels                                                  |
| `hide_conf`    | `bool`         | `False`                | hide confidence scores                                       |
| `max_det`      | `int`          | `300`                  | maximum number of detections per image                       |
| `vid_stride`   | `bool`         | `False`                | video frame-rate stride                                      |
| `line_width`   | `None or int`  | `None`                 | The line width of the bounding boxes. If None, it is scaled to the image size. |
| `visualize`    | `bool`         | `False`                | visualize model features                                     |
| `augment`      | `bool`         | `False`                | apply image augmentation to prediction sources               |
| `agnostic_nms` | `bool`         | `False`                | class-agnostic NMS                                           |
| `retina_masks` | `bool`         | `False`                | use high-resolution segmentation masks                       |
| `classes`      | `None or list` | `None`                 | filter results by class, i.e. class=0, or class=[0,2,3]      |
| `boxes`        | `bool`         | `True`                 | Show boxes in segmentation predictions                       |

## torch

```sh
yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.pt source=ultralytics/assets/bus.jpg device=0

yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.pt source=../datasets/coco128/images/train2017 device=0
```

## torchscript

```sh
yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.torchscript source=ultralytics/assets/bus.jpg device=0

yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.torchscript source=../datasets/coco128/images/train2017 device=0
```

## onnx

> æ³¨æ„:
>
> `onnxruntime` å’Œ `onnxruntime-gpu` ä¸è¦åŒæ—¶å®‰è£…ï¼Œå¦åˆ™ä½¿ç”¨ `gpu` æ¨ç†æ—¶é€Ÿåº¦ä¼šå¾ˆæ…¢ï¼Œå¦‚æœåŒæ—¶å®‰è£…äº†2ä¸ªåŒ…ï¼Œè¦å…¨éƒ¨å¸è½½ï¼Œå†å®‰è£… `onnxruntime-gpu` æ‰èƒ½ä½¿ç”¨gpuæ¨ç†ï¼Œå¦åˆ™gpué€Ÿåº¦ä¼šå¾ˆæ…¢

```sh
yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.onnx source=ultralytics/assets/bus.jpg device=0
yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.onnx source=../datasets/coco128/images/train2017 device=0

yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp16.onnx half=True source=ultralytics/assets/bus.jpg device=0           # fp16æ¨¡å‹éœ€è¦ half=True
yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp16.onnx half=True source=../datasets/coco128/images/train2017 device=0

yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.cpu.dynamic.onnx source=ultralytics/assets/bus.jpg device=0              # ä½¿ç”¨cpuå¯¼å‡ºçš„dynamicæ¨¡å‹å¯ä»¥ç”¨gpuæ¨ç†
yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.cpu.dynamic.onnx source=../datasets/coco128/images/train2017 device=0
```

## openvino

> æ³¨æ„ï¼šopenvinoæ²¡æ³•ä½¿ç”¨cudaï¼Œä½†æ˜¯ä½¿ç”¨ `device=0` ä¼šæé«˜æ¨ç†é€Ÿåº¦

```sh
yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n_openvino_model source=ultralytics/assets/bus.jpg device=cpu

yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n_openvino_model source=../datasets/coco128/images/train2017 device=cpu
```

## tensorrt

```sh
yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.engine half=True source=ultralytics/assets/bus.jpg device=0                          # fp32æ¨¡å‹ä¹Ÿèƒ½ç”¨ --half æ¨ç†
yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.engine half=True source=../datasets/coco128/images/train2017 device=0

yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp16.engine half=True source=ultralytics/assets/bus.jpg device=0
yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp16.engine half=True source=../datasets/coco128/images/train2017 device=0

yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp32.dynamic.engine half=True source=ultralytics/assets/bus.jpg device=0             # fp32æ¨¡å‹ä¹Ÿèƒ½ç”¨ --half æ¨ç†
yolo task=detect mode=predict imgsz=640 save=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.fp32.dynamic.engine half=True source=../datasets/coco128/images/train2017 device=0
```

# [Val](https://docs.ultralytics.com/modes/val/)

Validate trained YOLOv8n model accuracy on the COCO128 dataset. No argument need to passed as the `model` retains it's training `data` and arguments as model attributes. See Arguments section below for a full list of export arguments.

- YOLOv8 models automatically remember their training settings, so you can validate a model at the same image size and on the original dataset easily with just `yolo val model=yolov8n.pt` or `model('yolov8n.pt').val()`

## Python

Validate trained YOLOv8n model accuracy on the COCO128 dataset. No argument need to passed as the `model` retains it's training `data` and arguments as model attributes. See Arguments section below for a full list of export arguments.

```python
from ultralytics import YOLO

# Load a model
model = YOLO('yolov8n.pt')  # load an official model
model = YOLO('path/to/best.pt')  # load a custom model

# Validate the model
metrics = model.val()  # no arguments needed, dataset and settings remembered
metrics.box.map    # map50-95
metrics.box.map50  # map50
metrics.box.map75  # map75
metrics.box.maps   # a list contains map50-95 of each category
```

## Arguments

Validation settings for YOLO models refer to the various hyperparameters and configurations used to evaluate the model's performance on a validation dataset. These settings can affect the model's performance, speed, and accuracy. Some common YOLO validation settings include the batch size, the frequency with which validation is performed during training, and the metrics used to evaluate the model's performance. Other factors that may affect the validation process include the size and composition of the validation dataset and the specific task the model is being used for. It is important to carefully tune and experiment with these settings to ensure that the model is performing well on the validation dataset and to detect and prevent overfitting.

| Key           | Value   | Description                                                  |
| :------------ | :------ | :----------------------------------------------------------- |
| `data`        | `None`  | path to data file, i.e. coco128.yaml                         |
| `imgsz`       | `640`   | image size as scalar or (h, w) list, i.e. (640, 480)         |
| `batch`       | `16`    | number of images per batch (-1 for AutoBatch)                |
| `save_json`   | `False` | save results to JSON file                                    |
| `save_hybrid` | `False` | save hybrid version of labels (labels + additional predictions) |
| `conf`        | `0.001` | object confidence threshold for detection                    |
| `iou`         | `0.7`   | intersection over union (IoU) threshold for NMS              |
| `max_det`     | `300`   | maximum number of detections per image                       |
| `half`        | `True`  | use half precision (FP16)                                    |
| `device`      | `None`  | device to run on, i.e. cuda device=0/1/2/3 or device=cpu     |
| `dnn`         | `False` | use OpenCV DNN for ONNX inference                            |
| `plots`       | `False` | show plots during training                                   |
| `rect`        | `False` | rectangular val with each batch collated for minimum padding |
| `split`       | `val`   | dataset split to use for validation, i.e. 'val', 'test' or 'train' |

> example

```sh
yolo detect val model=yolov8n.pt  # val official model
yolo detect val model=path/to/best.pt  # val custom model
```

> `--save-hybrid` ä¼šåˆå¹¶å·²çŸ¥çš„labelsï¼Œå¯¼è‡´å¾—åˆ†å¾ˆé«˜

## torch

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.pt device=0
```

## torchscript

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.torchscript device=0
```

## onnx

> æ³¨æ„:
>
> `onnxruntime` å’Œ `onnxruntime-gpu` ä¸è¦åŒæ—¶å®‰è£…ï¼Œå¦åˆ™ä½¿ç”¨ `gpu` æ¨ç†æ—¶é€Ÿåº¦ä¼šå¾ˆæ…¢ï¼Œå¦‚æœåŒæ—¶å®‰è£…äº†2ä¸ªåŒ…ï¼Œè¦å…¨éƒ¨å¸è½½ï¼Œå†å®‰è£… `onnxruntime-gpu` æ‰èƒ½ä½¿ç”¨gpuæ¨ç†ï¼Œå¦åˆ™gpué€Ÿåº¦ä¼šå¾ˆæ…¢

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.onnx device=0
```

## openvino

> æ³¨æ„ï¼šopenvinoæ²¡æ³•ä½¿ç”¨cudaï¼Œä½†æ˜¯ä½¿ç”¨ --device 0 ä¼šæé«˜æ¨ç†é€Ÿåº¦

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n_openvnio_model device=cpu
```

## tensorrt

```sh
yolo task=detect mode=val imgsz=640 save_json=True save_txt=True save_conf=True conf=0.25 iou=0.6 data=ultralytics/cfg/datasets/coco128.yaml model=weights/yolov8n.onnx device=0 half=True
```

